{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 영화 리뷰 감성 분석 (자연어 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os, pathlib, shutil, random\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 다운로드 및 압축 해제\n",
    "\n",
    "Stanford 대학의 AI 연구실에서 제공하는 IMDB 영화 리뷰 데이터셋을 다운로드하고 압축을 해제합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드 함수\n",
    "def download():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "    response = requests.get(url, stream=True)  # 스트리밍 방식으로 다운로드\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):  # 8KB씩 다운로드\n",
    "            file.write(chunk)\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# 압축풀기 함수 (tar 프로그램 필요)\n",
    "def release():\n",
    "    # tar.gz => linux에서는 파일을 여러개를 한번에 압축을 못함 tar라는 형식으로 압축할 모든 파일을 하나로 묶어서 패키지로 만든다음에 \n",
    "    #           압축을 한다.  tar , gz가동  그래서 압축풀고 다시 패키지도 풀어야 한다. \n",
    "    #           tar  -xvzf 파일명   형태임\n",
    "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True) #tar 프로그램 가동하기 \n",
    "    print(\"압축풀기 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download() # 최초 한번만 실행\n",
    "# release()  # 최초 한번만 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 준비: 훈련/검증 데이터 분리\n",
    "\n",
    "원본 훈련 데이터셋(train)의 20%를 검증 데이터셋(validation)으로 분리합니다. `unsup` 폴더는 라벨이 없으므로 사용하지 않습니다 (미리 수동으로 삭제했다고 가정)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 및 데이터 분리 함수\n",
    "def labeling(): \n",
    "    base_dir = pathlib.Path(\"aclImdb\") \n",
    "    val_dir = base_dir/\"val\"   # pathlib 객체에  / \"디렉토리\" => 결과가 문자열이 아니다 \n",
    "    train_dir = base_dir/\"train\"\n",
    "\n",
    "    # val 디렉토리 생성\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        if not os.path.exists(val_dir/category):\n",
    "             os.makedirs(val_dir/category)  #디렉토리를 만들고 \n",
    "        \n",
    "        files = os.listdir(train_dir/category) #해당 카테고리의 파일 목록을 모두 가져온다 \n",
    "        random.Random(1337).shuffle(files) #파일을 랜덤하게 섞어서 복사하려고 파일 목록을 모두 섞는다 \n",
    "        num_val_samples = int(0.2 * len(files)) \n",
    "        val_files = files[-num_val_samples:] #20%만 val폴더로 이동한다 \n",
    "        for fname in val_files:\n",
    "            shutil.move(train_dir/category/fname, val_dir/category/fname )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labeling() # 최초 한번만 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 로드\n",
    "\n",
    "`keras.utils.text_dataset_from_directory`를 사용하여 디렉토리에서 텍스트 파일을 `tf.data.Dataset` 객체로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 #한번에 읽어올 양 \n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 구조 확인\n",
    "\n",
    "로드된 데이터셋의 형태(shape)와 데이터 타입(dtype)을 확인합니다. 데이터셋은 (텍스트, 라벨) 쌍으로 구성됩니다. 라벨은 폴더 이름(neg, pos)에 따라 자동으로 0과 1로 인코딩됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds: #실제 읽어오는 데이터 확인 \n",
    "    print(\"inputs.shape\", inputs.shape)\n",
    "    print(\"inputs.dtype\", inputs.dtype)\n",
    "    print(\"targets.shape\", targets.shape)\n",
    "    print(\"targets.dtype\", targets.dtype)\n",
    "    print(\"inputs[0]\", inputs[:3])\n",
    "    print(\"targets[0]\", targets[:3])\n",
    "    break #하나만 출력해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 텍스트 벡터화 (Text Vectorization)\n",
    "\n",
    "텍스트 데이터를 모델이 처리할 수 있는 정수 시퀀스로 변환합니다.\n",
    "\n",
    "- `max_tokens`: 어휘 사전에 포함할 최대 단어 수 (빈도수 기준)\n",
    "- `output_sequence_length`: 모든 시퀀스를 동일한 길이로 맞추기 위한 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 600  #한 평론에서 사용하는 단어는 최대 길이를 600개라고 보자  \n",
    "max_tokens = 20000 #자주 사용하는 단어 20000 개만 쓰겠다 \n",
    "\n",
    "text_vectorization = TextVectorization( \n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = \"int\", #임베딩 층을 사용하려면 반드시 int여야 한다\n",
    "    output_sequence_length = max_length  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어휘 사전 생성\n",
    "\n",
    "훈련 데이터셋의 텍스트만 사용하여 `TextVectorization` 레이어의 어휘 사전을 생성합니다 (`adapt` 메서드 사용)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터만 있는 데이터셋을 만듭니다.\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "# 어휘 사전을 만듭니다.\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋에 벡터화 적용\n",
    "\n",
    "생성된 어휘 사전을 사용하여 훈련, 검증, 테스트 데이터셋의 텍스트를 모두 정수 시퀀스로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_train_ds = train_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=4 )\n",
    "int_val_ds = val_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=4 )\n",
    "int_test_ds = test_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변환된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in int_train_ds:\n",
    "    print(item)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 구축 (RNN)\n",
    "\n",
    "이전 예제에서는 정수 시퀀스를 직접 원-핫 인코딩하여 모델에 입력했지만, 이는 메모리 비효율을 야기할 수 있습니다. 이번에는 **임베딩 레이어**를 사용하는 대신, 원-핫 인코딩을 모델의 일부로 포함시키는 `Lambda` 레이어를 사용해봅니다. (실제로는 임베딩 레이어를 사용하는 것이 일반적입니다.)\n",
    "\n",
    "- `Input`: 정수 시퀀스를 입력으로 받습니다.\n",
    "- `Lambda`: 입력된 정수 시퀀스를 `tf.one_hot`을 사용하여 원-핫 벡터 시퀀스로 변환합니다.\n",
    "- `Bidirectional(LSTM)`: 양방향 LSTM을 사용하여 시퀀스의 문맥을 양방향으로 학습합니다.\n",
    "- `Dropout`: 과적합을 방지하기 위해 일부 뉴런을 비활성화합니다.\n",
    "- `Dense`: 최종 출력을 생성하여 긍정(1) 또는 부정(0)을 예측합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 원-핫 인코딩을 모델의 한 레이어로 추가\n",
    "# 시퀀스 => 원-핫 인코딩으로 변환하는 Lambda 레이어\n",
    "embedded = layers.Lambda(\n",
    "    lambda x: tf.reshape(tf.one_hot(x, depth=max_tokens), (-1, tf.shape(x)[1], max_tokens)),  \n",
    "    output_shape=(None, max_tokens) \n",
    ")(inputs) \n",
    "\n",
    "# 양방향 RNN 모델 구성\n",
    "x = layers.Bidirectional( layers.LSTM(32))(embedded) \n",
    "x = layers.Dropout(0.5)(x) \n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs) \n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"테스트셋 평가 결과 \", model.evaluate(int_test_ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}