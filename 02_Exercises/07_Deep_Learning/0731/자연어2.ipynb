{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 벡터화: 사용자 정의 함수 활용\n",
    "\n",
    "`TextVectorization` 레이어는 텍스트를 정수 시퀀스로 변환하는 편리한 도구입니다. 기본적으로 구두점 제거, 소문자 변환 등의 표준화와 공백 기준의 토큰화를 수행하지만, 사용자가 직접 함수를 정의하여 이 과정을 커스터마이징할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 사용자 정의 함수 정의\n",
    "\n",
    "- **표준화(Standardization) 함수**: 텍스트를 전처리하는 단계입니다. 여기서는 텍스트를 소문자로 변환하고 정규식을 사용하여 모든 구두점을 제거합니다.\n",
    "- **토큰화(Splitting) 함수**: 표준화된 텍스트를 단어(토큰) 단위로 분리하는 단계입니다. 여기서는 공백을 기준으로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정의 표준화 함수\n",
    "def custom_standardization_fn(text):\n",
    "    # 텍스트를 소문자로 변환\n",
    "    lower_text = tf.strings.lower(text)\n",
    "    # 정규식을 사용하여 구두점 제거\n",
    "    return tf.strings.regex_replace(lower_text, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "# 사용자 정의 토큰 분리 함수\n",
    "def custom_split_fn(text):\n",
    "    # 공백을 기준으로 텍스트를 분리하여 토큰 리스트를 반환\n",
    "    return tf.strings.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TextVectorization 레이어 초기화\n",
    "\n",
    "`TextVectorization` 객체를 생성할 때 `standardize`와 `split` 매개변수에 위에서 정의한 사용자 정의 함수를 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    output_mode=\"int\",                   # 출력 형식을 정수 시퀀스로 지정\n",
    "    standardize=custom_standardization_fn, # 표준화에 사용할 함수 전달\n",
    "    split=custom_split_fn                  # 토큰화에 사용할 함수 전달\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 어휘 사전(Vocabulary) 생성\n",
    "\n",
    "샘플 데이터셋을 `adapt` 메서드에 전달하여 텍스트 데이터에 있는 모든 고유한 단어(토큰)로 구성된 어휘 사전을 생성합니다. 이 사전은 단어를 정수 인덱스에 매핑하는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용할 샘플 데이터셋\n",
    "dataset = [\n",
    "    \"I write, erase, rewrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\",\n",
    "    \"Dog is pretty\"\n",
    "]\n",
    "\n",
    "# 데이터셋을 사용하여 어휘 사전을 생성\n",
    "text_vectorization.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 생성된 어휘 사전 확인\n",
    "\n",
    "`get_vocabulary()` 메서드를 사용하여 생성된 사전을 확인할 수 있습니다. 단어들은 빈도수 순으로 정렬되며, 인덱스 0은 패딩(padding), 인덱스 1은 사전에 없는 단어(OOV, Out-of-Vocabulary)를 위해 예약되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "print(\"어휘 사전:\", vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 인코딩 (텍스트 -> 정수 시퀀스)\n",
    "\n",
    "어휘 사전이 생성되면, `TextVectorization` 레이어를 함수처럼 호출하여 새로운 텍스트를 정수 시퀀스로 변환(인코딩)할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩할 텍스트\n",
    "text = \"I write, rewrite, and still rewrite again\"\n",
    "\n",
    "# 텍스트를 정수 시퀀스로 변환\n",
    "encoded = text_vectorization(text)\n",
    "print(\"인코딩된 시퀀:\", encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 디코딩 (정수 시퀀스 -> 텍스트)\n",
    "\n",
    "인코딩된 정수 시퀀스를 다시 사람이 읽을 수 있는 텍스트로 되돌리려면, 어휘 사전을 사용하여 인덱스를 단어로 매핑하는 딕셔너리를 만들어야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 단어로 매핑하는 딕셔너리 생성\n",
    "decoded_vocab = dict(enumerate(vocabulary))\n",
    "print(\"인덱스-단어 맵:\", decoded_vocab)\n",
    "\n",
    "# 정수 시퀀스를 텍스트로 변환\n",
    "decoded_sentence = \" \".join(decoded_vocab[int(i)] for i in encoded)\n",
    "print(\"\\n디코딩된 문장:\", decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}