{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 붓꽃(Iris) 품종 분류: 다양한 머신러닝 모델 비교\n",
    "\n",
    "이 노트북은 머신러닝에서 가장 널리 사용되는 예제 데이터셋 중 하나인 **붓꽃(Iris) 데이터셋**을 사용하여 붓꽃의 품종을 분류하는 과정을 다룹니다. 다양한 분류 알고리즘(K-최근접 이웃, 로지스틱 회귀, 의사결정나무, 랜덤 포레스트)을 적용하고, 각 모델의 성능을 비교 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 준비 및 탐색\n",
    "\n",
    "`scikit-learn`에 내장된 붓꽃 데이터셋을 불러와 데이터의 구조를 확인합니다.\n",
    "\n",
    "**붓꽃 데이터셋**: 붓꽃의 꽃받침 길이(sepal length), 꽃받침 너비(sepal width), 꽃잎 길이(petal length), 꽃잎 너비(petal width) 4가지 특성을 사용하여 3가지 품종(setosa, versicolor, virginica) 중 하나로 분류하는 문제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "# 데이터셋의 키 확인\n",
    "print(\"데이터셋 키:\", iris.keys())\n",
    "\n",
    "# 타겟(품종) 이름 확인\n",
    "print(\"타겟 이름:\", iris['target_names'])\n",
    "\n",
    "# 데이터셋 설명 (필요시 주석 해제)\n",
    "# print(\"데이터셋 설명:\")\n",
    "# print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성(X)과 타겟(y) 데이터 분리\n",
    "X = iris.data \n",
    "y = iris.target \n",
    "\n",
    "print(f\"특성 데이터 형태: {X.shape}\")\n",
    "print(f\"타겟 데이터 형태: {y.shape}\")\n",
    "\n",
    "print(\"\n특성 데이터 샘플 (상위 10개):\n\", X[:10])\n",
    "print(\"\n타겟 데이터 샘플:\", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터셋 분할\n",
    "\n",
    "모델의 일반화 성능을 평가하기 위해, 전체 데이터를 훈련(train) 세트와 테스트(test) 세트로 분할합니다. `random_state`를 설정하여 재현 가능하도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1234, test_size=0.3)\n",
    "\n",
    "print(f\"훈련 데이터 형태: {X_train.shape}\")\n",
    "print(f\"테스트 데이터 형태: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 다양한 분류 모델 학습 및 평가\n",
    "\n",
    "이제 준비된 데이터로 여러 분류 모델을 학습시키고, 각 모델의 성능을 비교해 보겠습니다. 모델의 성능은 `score()` 함수가 반환하는 **정확도(Accuracy)**를 기준으로 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. K-최근접 이웃 (K-Nearest Neighbors, KNN)\n",
    "\n",
    "KNN은 새로운 데이터 포인트가 주어졌을 때, 가장 가까운 k개의 훈련 데이터 포인트를 찾아 그들의 클래스를 기반으로 예측하는 알고리즘입니다. `n_neighbors`는 고려할 이웃의 수를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn = KNeighborsClassifier(n_neighbors=2) # 이웃의 수를 2로 설정\n",
    "model_knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = model_knn.predict(X_test)\n",
    "\n",
    "print(\"--- KNN 모델 ---\")\n",
    "print(f\"훈련 세트 정확도: {model_knn.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 세트 정확도: {model_knn.score(X_test, y_test):.4f}\")\n",
    "print(\"\n테스트 데이터 예측 (일부):\", y_pred_knn[:10])\n",
    "print(\"실제 테스트 데이터 (일부):\", y_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. 로지스틱 회귀 (Logistic Regression)\n",
    "\n",
    "로지스틱 회귀는 선형 모델이지만 분류 문제에 사용됩니다. 각 클래스에 속할 확률을 예측하며, 이진 분류뿐만 아니라 다중 분류에도 확장하여 사용될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(max_iter=1000) # 수렴을 위해 max_iter 증가\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- 로지스틱 회귀 모델 ---\")\n",
    "print(f\"훈련 세트 정확도: {model_lr.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 세트 정확도: {model_lr.score(X_test, y_test):.4f}\")\n",
    "print(\"계수 (Coefficients):\n\", model_lr.coef_)\n",
    "print(\"절편 (Intercept):\", model_lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. 의사결정나무 (Decision Tree)\n",
    "\n",
    "의사결정나무는 데이터를 특정 기준에 따라 분할하여 예측을 수행하는 트리 구조의 모델입니다. 직관적이고 해석하기 쉽지만, 과대적합(overfitting)되기 쉽습니다. `feature_importances_` 속성을 통해 각 특성의 중요도를 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = DecisionTreeClassifier(random_state=1) # 재현성을 위해 random_state 설정\n",
    "model_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- 의사결정나무 모델 ---\")\n",
    "print(f\"훈련 세트 정확도: {model_dt.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 세트 정확도: {model_dt.score(X_test, y_test):.4f}\")\n",
    "print(\"특성 중요도:\", model_dt.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 특성 중요도 시각화\n",
    "\n",
    "의사결정나무가 어떤 특성을 중요하게 판단했는지 막대 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, feature_names):\n",
    "    n_features = len(feature_names)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), feature_names)\n",
    "    plt.xlabel(\"특성 중요도\")\n",
    "    plt.ylabel(\"특성\")\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.title(\"의사결정나무 특성 중요도\")\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importances(model_dt, iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. 랜덤 포레스트 (Random Forest)\n",
    "\n",
    "랜덤 포레스트는 여러 개의 의사결정나무를 만들어 그 예측을 종합하여 최종 예측을 수행하는 **앙상블(Ensemble)** 모델입니다. 의사결정나무의 과대적합 문제를 완화하고 더 안정적인 성능을 제공합니다.\n",
    "\n",
    "- `n_estimators`: 생성할 트리의 개수\n",
    "- `max_depth`: 각 트리의 최대 깊이 (과대적합 방지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestClassifier(random_state=0, max_depth=3, n_estimators=1000) # n_estimators를 1000으로 설정\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"--- 랜덤 포레스트 모델 ---\")\n",
    "print(f\"훈련 세트 정확도: {model_rf.score(X_train, y_train):.4f}\")\n",
    "print(f\"테스트 세트 정확도: {model_rf.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 결론\n",
    "\n",
    "붓꽃 데이터셋은 비교적 간단한 분류 문제이므로 대부분의 모델이 높은 정확도를 보입니다. 하지만 실제 복잡한 데이터에서는 각 모델의 특성과 하이퍼파라미터 튜닝이 모델 성능에 큰 영향을 미칩니다. 이 노트북을 통해 다양한 분류 모델의 기본 작동 방식과 성능 평가 방법을 이해하는 데 도움이 되었기를 바랍니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}