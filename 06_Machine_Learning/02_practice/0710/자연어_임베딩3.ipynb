{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전학습된 임베딩 계층을 이용한 자연어 처리\n",
    "\n",
    "이 노트북은 사전 학습된 Word2Vec (GoogleNews-vectors-negative300.bin) 임베딩을 사용하여 텍스트 분류 모델을 구축하는 과정을 보여줍니다. IMDB 영화 리뷰 데이터셋을 사용하여 긍정/부정 감성 분석을 수행합니다.\n",
    "\n",
    "## 1. 라이브러리 설치 및 데이터 준비\n",
    "\n",
    "### 1.1. 필요한 라이브러리 설치\n",
    "\n",
    "gensim 라이브러리가 필요합니다. 다음 명령어를 사용하여 설치할 수 있습니다:\n",
    "```bash\n",
    "conda install gensim\n",
    "```\n",
    "\n",
    "### 1.2. Word2Vec 모델 다운로드\n",
    "\n",
    "GoogleNews-vectors-negative300.bin.gz 파일을 다운로드해야 합니다. 다음 링크 중 하나를 사용할 수 있습니다:\n",
    "\n",
    "- **Direct Link (S3 AWS):** https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "- **Hugging Face:** NathaNn1111/word2vec-google-news-negative-300-bin와 같은 저장소에서 다운로드 링크를 제공합니다.\n",
    "- **Kaggle:** https://www.kaggle.com/datasets/leadbest/googlenewsvectorsnegative300\n",
    "\n",
    "다운로드한 파일은 `./data/` 디렉토리에 `GoogleNews-vectors-negative300.bin`으로 저장해야 합니다.\n",
    "\n",
    "### 1.3. IMDB 데이터셋 다운로드 및 전처리\n",
    "\n",
    "IMDB 영화 리뷰 데이터셋을 다운로드하고 압축을 해제한 후, 훈련 및 검증 세트로 분할합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "import os, pathlib, shutil, random\n",
    "import keras\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 다운로드\n",
    "def download():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "    response = requests.get(url, stream=True)  # 스트리밍 방식으로 다운로드\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):  # 8KB씩 다운로드\n",
    "            file.write(chunk)\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "# 압축풀기\n",
    "def release():\n",
    "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"])\n",
    "    print(\"압축풀기 완료\")\n",
    "\n",
    "# 라벨링 (train/val 분리)\n",
    "def labeling():\n",
    "    base_dir = pathlib.Path(\"aclImdb\")\n",
    "    val_dir = base_dir/\"val\"\n",
    "    train_dir = base_dir/\"train\"\n",
    "\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        os.makedirs(val_dir/category, exist_ok=True)\n",
    "        files = os.listdir(train_dir/category)\n",
    "        random.Random(1337).shuffle(files)\n",
    "        num_val_samples = int(0.2 * len(files))\n",
    "        val_files = files[-num_val_samples:]\n",
    "        for fname in val_files:\n",
    "            shutil.move(train_dir/category/fname, val_dir/category/fname)\n",
    "\n",
    "# 데이터 다운로드 및 전처리 실행 (필요시 주석 해제)\n",
    "# download()\n",
    "# release()\n",
    "# labeling()\n",
    "\n",
    "print(\"데이터 다운로드 및 전처리 함수 정의 완료. 필요시 주석을 해제하여 실행하세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 로드 및 전처리\n",
    "\n",
    "IMDB 데이터셋을 Keras `text_dataset_from_directory`를 사용하여 로드하고, `TextVectorization` 레이어를 사용하여 텍스트를 정수 시퀀스로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\",\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "print(\"\n",
    "데이터셋 샘플 확인:\")\n",
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[:3])\n",
    "    print(\"targets[0]:\", targets[:3])\n",
    "    break\n",
    "\n",
    "# 텍스트 벡터화\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = max_length\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds) # 어휘 사전 구축\n",
    "\n",
    "int_train_ds = train_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "int_val_ds = val_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "int_test_ds = test_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE )\n",
    "\n",
    "print(\"\n",
    "정수 인코딩된 데이터셋 샘플 확인:\")\n",
    "for item in int_train_ds:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec 임베딩 로드 및 임베딩 행렬 생성\n",
    "\n",
    "사전 학습된 Word2Vec 모델을 로드하고, 이를 기반으로 Keras `Embedding` 레이어에 사용할 임베딩 행렬을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 파일 불러오기\n",
    "filename = \"./data/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "try:\n",
    "    word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "    embedding_dim = word2vec_model.vector_size\n",
    "    print(\"Word2Vec 파일 로딩 성공. 임베딩 차원:\", embedding_dim)\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: {filename}을(를) 찾을 수 없습니다. 파일을 `./data/` 디렉토리에 올바르게 배치했는지 확인하세요.\")\n",
    "    exit()\n",
    "except Exception as e :\n",
    "    print(f\"에러 발생: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        try:\n",
    "            embedding_vector = word2vec_model[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            pass  # Word2Vec 모델에 없는 단어는 건너뜀\n",
    "\n",
    "print(\"\n",
    "생성된 임베딩 행렬의 일부 (상위 10개 단어):\")\n",
    "print(embedding_matrix[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 구축 및 훈련\n",
    "\n",
    "사전 학습된 임베딩 레이어를 사용하여 양방향 LSTM 모델을 구축하고 훈련합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구축\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim = max_tokens,\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,  # 사전 학습된 임베딩은 훈련 중 업데이트하지 않음\n",
    "    mask_zero=True\n",
    ")(\n",
    "    inputs\n",
    ")\n",
    "\n",
    "print(\"임베딩 레이어 출력 형태:\", embedded.shape)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"\n",
    "모델 요약:\")\n",
    "model.summary()\n",
    "\n",
    "# 모델 훈련\n",
    "print(\"\n",
    "모델 훈련 시작...\")\n",
    "history = model.fit(int_train_ds, validation_data=int_val_ds, epochs=15)\n",
    "\n",
    "print(\"\n",
    "테스트셋 평가:\")\n",
    "print(\"테스트셋 결과:\", model.evaluate(int_test_ds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
