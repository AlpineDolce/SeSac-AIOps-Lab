# 📊 머신러닝 심화: 군집 분석, 차원 축소 (PCA) 및 교차 검증

> 본 문서는 비지도 학습의 핵심 기법인 군집 분석(K-Means)과 차원 축소(PCA)의 원리 및 적용 사례를 다룹니다. 또한, 모델의 일반화 성능을 신뢰성 있게 평가하기 위한 교차 검증(K-Fold, Stratified K-Fold)의 중요성과 활용 방법을 상세히 설명합니다. 각 기법의 실습 결과 분석과 함께 실무 적용을 위한 가이드라인을 제시하여 머신러닝 프로젝트의 효율성과 신뢰성을 높이는 데 기여합니다.

---

## 목차 (Table of Contents)

1.  [군집분석(클러스터링)](#1-군집분석클러스터링)
    *   [1.1. 군집분석이란?](#군집분석이란)
    *   [1.2. K-Means 클러스터링](#k-means-클러스터링)
    *   [1.3. 가우시안(정규) 분포](#가우시안정규-분포)
    *   [1.4. 실습 결과](#실습-결과)
    *   [1.5. 군집 개수 결정 방법](#군집-개수-결정-방법)
2.  [주성분분석(PCA)](#2-주성분분석pca)
    *   [2.1. PCA의 개념](#pca의-개념)
    *   [2.2. PCA의 필요성](#pca의-필요성)
    *   [2.3. PCA 과정](#pca-과정)
    *   [2.4. 실습 결과 분석](#실습-결과-분석)
    *   [2.5. PCA의 장단점](#pca의-장단점)
3.  [K-Fold 교차검증](#3-k-fold-교차검증)
    *   [3.1. 교차검증의 목적](#교차검증의-목적)
    *   [3.2. 교차검증 방법들](#교차검증-방법들)
    *   [3.3. K-Fold 동작 원리 (K=5 예시)](#k-fold-동작-원리-k5-예시)
    *   [3.4. 실습 결과 비교](#실습-결과-비교)
    *   [3.5. K-Fold vs Stratified K-Fold 차이점](#k-fold-vs-stratified-k-fold-차이점)
4.  [성능 비교 및 분석](#4-성능-비교-및-분석)
    *   [4.1. 비지도 학습 (PCA)의 효과 분석](#41-비지도-학습-pca의-효과-분석)
    *   [4.2. 모델별 특성 분석 및 성능 비교](#42-모델별-특성-분석-및-성능-비교)
5.  [실무 적용 가이드](#5-실무-적용-가이드)
    *   [5.1. 군집분석 활용 가이드](#군집분석-활용-가이드)
    *   [5.2. PCA 적용 가이드](#pca-적용-가이드)
    *   [5.3. 교차검증 적용 가이드](#교차검증-적용-가이드)
    *   [5.4. 종합 워크플로우](#종합-워크플로우)
    *   [5.5. 핵심 권장사항](#핵심-권장사항)
    *   [5.6. 주의사항](#주의사항)
    *   [5.7. 결론](#결론)

---

## 1. 군집분석(클러스터링)

### 군집분석이란?

군집분석(Clustering)은 **비지도 학습(Unsupervised Learning)**의 대표적인 방법 중 하나입니다. 데이터에 미리 정의된 정답(레이블)이 없는 상태에서, 데이터 포인트들 간의 유사성(거리)을 기반으로 자연스러운 그룹(군집, Cluster)을 찾아 데이터를 분류하는 기법입니다.

-   **목표**: 데이터 내에 숨겨진 패턴이나 구조를 발견하고, 유사한 특성을 가진 데이터들을 하나의 그룹으로 묶는 것입니다.
-   **특징**: 
    -   **비지도 학습**: 모델 학습 시 정답(레이블)을 사용하지 않고, 오직 입력 데이터의 특성만을 활용합니다.
    -   **패턴 발견**: 데이터의 내재된 구조를 파악하여 새로운 인사이트를 얻거나, 데이터 탐색(EDA)의 초기 단계에서 활용됩니다.
    -   **사전 군집 개수 지정**: 대부분의 군집 알고리즘(예: K-Means)은 사용자가 사전에 군집의 개수(K)를 지정해야 합니다. 적절한 K값을 찾는 것이 중요합니다.
-   **활용 분야**: 고객 세분화, 이미지 분할, 문서 요약, 이상 탐지 등 다양한 분야에서 활용됩니다.

### K-Means 클러스터링

K-Means는 가장 널리 사용되는 군집 알고리즘 중 하나로, 주어진 데이터를 K개의 군집으로 나누는 것을 목표로 합니다. 각 군집은 해당 군집의 중심점(Centroid)에 가장 가까운 데이터 포인트들로 구성됩니다.

#### 기본 원리

1.  **초기 중심점 설정**: 사용자가 지정한 K개의 중심점을 무작위로 선택하거나 특정 전략에 따라 초기화합니다.
2.  **데이터 할당**: 각 데이터 포인트를 가장 가까운 중심점을 가진 군집에 할당합니다. (유클리드 거리 등 거리 측정 사용)
3.  **중심점 업데이트**: 각 군집에 할당된 데이터 포인트들의 평균 위치를 계산하여 새로운 중심점으로 업데이트합니다.
4.  **반복**: 데이터 할당과 중심점 업데이트 과정을 중심점의 위치가 더 이상 변하지 않거나, 미리 설정한 반복 횟수에 도달할 때까지 반복합니다.

#### 주요 매개변수

```python
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs # 가상 데이터 생성을 위해 임포트

# 가상 데이터 생성 (3개의 군집을 가진 데이터)
X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)

# KMeans 모델 생성
kmeans = KMeans(
    n_clusters=3,      # 생성할 군집의 개수 (K값)
    init='k-means++',  # 중심점 초기화 방법. 'k-means++'는 초기 중심점을 더 똑똑하게 선택하여 수렴 속도를 높입니다.
    max_iter=300,      # 최대 반복 횟수. 이 횟수만큼 반복해도 수렴하지 않으면 중단합니다.
    n_init=10,         # 다른 중심점 초기화 시도를 n_init번 수행하여 가장 좋은 결과를 선택합니다.
                       # (KMeans++ 초기화 방식이 여러 번 실행됨)
    random_state=42    # 재현 가능한 결과를 위한 난수 시드
)

# 모델 학습
kmeans.fit(X)

# 각 데이터 포인트가 속한 군집 레이블
labels = kmeans.labels_

# 군집 중심점
centroids = kmeans.cluster_centers_

# 시각화
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.7, marker='X', label='Centroids')
plt.title('K-Means Clustering Result')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()
```

| 매개변수 | 설명 | 기본값 | 중요성 |
|:---|:---|:---|:---|
| `n_clusters` | 생성할 군집의 개수(K). 이 값을 사전에 지정해야 합니다. | 8 | 군집 분석의 핵심 매개변수로, 데이터의 특성과 분석 목표에 따라 적절한 값을 찾아야 합니다. | 
| `init` | 중심점 초기화 방법. `'k-means++'`는 초기 중심점을 더 똑똑하게 선택하여 수렴 속도를 높입니다. `'random'`은 무작위로 선택합니다. | `'k-means++'` | 초기 중심점의 위치에 따라 최종 군집 결과가 달라질 수 있으므로, 안정적인 초기화 방법이 중요합니다. | 
| `n_init` | 다른 중심점 초기화 시도를 `n_init`번 수행하여 가장 좋은 결과를 선택합니다. (KMeans++ 초기화 방식이 여러 번 실행됨) | 10 | 여러 번의 초기화를 통해 지역 최적해(Local Optima)에 빠지는 것을 방지하고, 더 안정적인 군집 결과를 얻을 수 있습니다. | 
| `max_iter` | 한 번의 초기화 시도에서 K-Means 알고리즘의 최대 반복 횟수. 이 횟수만큼 반복해도 수렴하지 않으면 중단합니다. | 300 | 알고리즘이 무한히 반복되는 것을 방지하고, 계산 시간을 제어합니다. | 
| `random_state` | 재현 가능한 결과를 위한 난수 시드. 동일한 `random_state` 값을 사용하면 항상 동일한 초기 중심점 선택 및 군집 결과를 얻을 수 있습니다. | `None` | 실험의 재현성을 보장하기 위해 중요합니다. |

### 가우시안(정규) 분포
- `np.random.normal(평균, 표준편차, 형태)` 사용
- 예: `np.random.normal(173, 10, 100)` - 평균 173, 표준편차 10인 100개 데이터
- 인공 데이터 생성을 통한 알고리즘 검증

### 실습 결과

#### 1. 인공 데이터 군집분석

평균과 표준편차를 다르게 설정한 3개의 가우시안(정규) 분포에서 데이터를 생성하여 K-Means 클러스터링을 적용한 결과입니다. 각 군집의 중심점이 명확히 분리되어 정확한 분류가 성공적으로 이루어졌음을 확인할 수 있습니다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

# 3개의 가우시안 분포에서 인공 데이터 생성
np.random.seed(42)
cluster1 = np.random.normal(loc=[0, 0], scale=[0.5, 0.5], size=(50, 2)) # 평균 (0,0), 표준편차 (0.5,0.5)
cluster2 = np.random.normal(loc=[3, 3], scale=[0.5, 0.5], size=(50, 2)) # 평균 (3,3), 표준편차 (0.5,0.5)
cluster3 = np.random.normal(loc=[0, 3], scale=[0.5, 0.5], size=(50, 2)) # 평균 (0,3), 표준편차 (0.5,0.5)

X_artificial = np.vstack([cluster1, cluster2, cluster3])

# K-Means 모델 학습 (K=3)
kmeans_artificial = KMeans(n_clusters=3, random_state=42, n_init=10)
kmeans_artificial.fit(X_artificial)

# 군집 결과 시각화
plt.figure(figsize=(8, 6))
plt.scatter(X_artificial[:, 0], X_artificial[:, 1], c=kmeans_artificial.labels_, cmap='viridis', s=50)
plt.scatter(kmeans_artificial.cluster_centers_[:, 0], kmeans_artificial.cluster_centers_[:, 1],
            c='red', marker='X', s=200, label='Centroids')
plt.title('K-Means Clustering on Artificial Data')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.grid(True)
plt.show()
```

#### 2. Iris 데이터셋 군집분석

Iris 데이터셋은 실제로는 3개의 클래스(종)를 가지고 있지만, K-Means 모델에 5개의 군집으로 강제 분할하도록 설정했을 때의 결과입니다. 원래 군집 개수보다 많이 설정하면 데이터가 **강제로 세분화**되어 불필요한 군집이 생성될 수 있으며, 적게 설정하면 **정보 손실**이 발생할 수 있습니다.

```python
from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

# Iris 데이터셋 로드
iris = load_iris()
X_iris = iris.data
y_iris = iris.target # 실제 레이블 (군집 결과와 비교용)

# 데이터 스케일링 (K-Means는 거리 기반이므로 스케일링이 중요)
scaler_iris = StandardScaler()
X_iris_scaled = scaler_iris.fit_transform(X_iris)

# K-Means 모델 학습 (K=5로 강제 설정)
kmeans_iris = KMeans(n_clusters=5, random_state=42, n_init=10)
kmeans_iris.fit(X_iris_scaled)

# 군집 결과 시각화 (첫 두 개의 특성 사용)
plt.figure(figsize=(10, 7))
plt.scatter(X_iris_scaled[:, 0], X_iris_scaled[:, 1], c=kmeans_iris.labels_, cmap='viridis', s=50)
plt.scatter(kmeans_iris.cluster_centers_[:, 0], kmeans_iris.cluster_centers_[:, 1],
            c='red', marker='X', s=200, alpha=0.7, label='Centroids')
plt.title('K-Means Clustering on Iris Dataset (K=5)')
plt.xlabel(iris.feature_names[0])
plt.ylabel(iris.feature_names[1])
plt.legend()
plt.grid(True)
plt.show()

# 실제 레이블과 군집 레이블 비교 (참고용)
# from sklearn.metrics import confusion_matrix
# print("\nConfusion Matrix (Actual vs Clustered Labels):\n", confusion_matrix(y_iris, kmeans_iris.labels_))
```

### 군집 개수 결정 방법

K-Means와 같은 군집 알고리즘에서는 군집의 개수(K)를 사전에 지정해야 합니다. 적절한 K값을 찾는 것은 군집 분석의 성능과 해석에 매우 중요하며, 주로 다음과 같은 방법들이 사용됩니다.

#### 1. 엘보우 방법 (Elbow Method)

엘보우 방법은 각 군집 개수(K)에 따른 **군집 내 분산(Within-Cluster Sum of Squares, WCSS)**의 변화를 시각화하여 최적의 K값을 찾는 방법입니다. WCSS는 각 데이터 포인트와 해당 군집의 중심점 간의 거리 제곱합을 의미하며, K값이 증가할수록 WCSS는 감소합니다. WCSS 감소율이 급격히 줄어드는 지점, 즉 그래프가 팔꿈치(Elbow)처럼 꺾이는 지점을 최적의 K값으로 판단합니다.

```python
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# 가상 데이터 생성
X_elbow, y_elbow = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

wcss = []
# K를 1부터 10까지 변화시키면서 WCSS 계산
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_elbow)
    wcss.append(kmeans.inertia_) # inertia_ 속성이 WCSS를 나타냄

# 엘보우 그래프 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), wcss, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('WCSS')
plt.xticks(range(1, 11))
plt.grid(True)
plt.show()
```

#### 2. 실루엣 방법 (Silhouette Analysis)

실루엣 방법은 각 데이터 포인트가 해당 군집에 얼마나 잘 속해 있는지, 그리고 다른 군집과는 얼마나 잘 분리되어 있는지를 측정하는 방법입니다. 실루엣 계수(Silhouette Coefficient)는 -1에서 1 사이의 값을 가지며, 1에 가까울수록 군집이 잘 되어 있음을 의미합니다.

-   **실루엣 계수**: `(b - a) / max(a, b)`
    -   `a`: 해당 데이터 포인트와 같은 군집 내 다른 데이터 포인트들 간의 평균 거리 (군집 내 응집도)
    -   `b`: 해당 데이터 포인트와 가장 가까운 다른 군집 내 모든 데이터 포인트들 간의 평균 거리 (군집 간 분리도)

```python
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs

# 가상 데이터 생성
X_silhouette, y_silhouette = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)

silhouette_scores = []
# K를 2부터 10까지 변화시키면서 실루엣 계수 계산
for i in range(2, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_silhouette)
    score = silhouette_score(X_silhouette, kmeans.labels_)
    silhouette_scores.append(score)

# 실루엣 계수 그래프 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='--')
plt.title('Silhouette Scores for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.xticks(range(2, 11))
plt.grid(True)
plt.show()
```

#### 3. 전문가적 견해 (Domain Knowledge)

데이터에 대한 도메인 지식을 활용하여 군집의 개수를 결정하는 방법입니다. 예를 들어, 고객 세분화 시 마케팅 전략에 필요한 고객 그룹의 수가 정해져 있거나, 특정 비즈니스 목표에 따라 군집의 개수를 설정할 수 있습니다. 이는 통계적 방법과 함께 사용될 때 가장 효과적입니다.

---

## 2. 주성분분석(PCA)

### PCA의 개념

주성분 분석(Principal Component Analysis, PCA)은 **고차원 데이터를 저차원으로 축소**하는 대표적인 **비지도 학습** 기법입니다. 데이터의 분산을 가장 잘 설명하는 새로운 직교 좌표계(주성분, Principal Components)를 찾아 데이터를 이 새로운 좌표계에 투영함으로써 차원을 줄입니다. 이 과정에서 데이터의 가장 중요한 정보(분산)를 최대한 보존하면서 불필요한 노이즈를 제거하는 효과를 얻을 수 있습니다.

-   **목표**: 데이터의 차원을 줄이면서 원본 데이터의 정보 손실을 최소화합니다.
-   **주성분**: 데이터의 분산이 가장 큰 방향을 첫 번째 주성분으로, 첫 번째 주성분과 직교하면서 남은 분산을 가장 잘 설명하는 방향을 두 번째 주성분으로 설정하는 방식으로 주성분들을 찾아냅니다.
-   **비지도 학습**: 데이터의 레이블(정답) 없이도 데이터 자체의 패턴을 분석하여 차원을 축소합니다.

### PCA의 필요성

현실 세계의 데이터는 매우 많은 특성(Feature)을 가질 수 있으며, 이러한 고차원 데이터는 여러 가지 문제를 야기합니다.

#### 1. 차원의 저주 (Curse of Dimensionality) 해결

특성의 수가 증가할수록 데이터 공간의 부피가 기하급수적으로 커지면서 데이터가 희소(sparse)해지는 현상을 '차원의 저주'라고 합니다. 이는 다음과 같은 문제를 발생시킵니다.

-   **계산 복잡도 증가**: 모델 학습 시간 및 메모리 사용량이 급격히 증가합니다. (예: 80x80x3 컬러 이미지의 경우 19,200개의 특성을 가집니다.)
-   **과적합 위험 증가**: 특성 수가 샘플 수보다 많아지면 모델이 훈련 데이터에 과도하게 맞춰져 일반화 성능이 저하될 위험이 커집니다.
-   **데이터 희소성**: 데이터 포인트 간의 거리가 의미 없어지면서 거리 기반 알고리즘(KNN 등)의 성능이 저하됩니다.

#### 2. 다중공선성 (Multicollinearity) 문제 완화

독립 변수들 간에 강한 상관관계가 존재할 때 발생하는 문제입니다. 다중공선성은 회귀 모델의 안정성을 저해하고, 각 특성의 개별적인 영향력을 해석하기 어렵게 만듭니다. PCA는 상관관계가 높은 특성들을 하나의 주성분으로 통합함으로써 이 문제를 완화할 수 있습니다.

-   **예시 (유방암 데이터셋)**:
    -   `mean radius`와 `mean perimeter` 간의 상관계수: 0.998
    -   `mean radius`와 `mean area` 간의 상관계수: 0.987
    이처럼 매우 높은 상관관계를 가진 특성들이 존재할 때 PCA는 이들을 효과적으로 처리합니다.

#### 3. 데이터 시각화 및 노이즈 제거

-   **시각화**: 고차원 데이터를 2차원 또는 3차원으로 축소하여 시각적으로 표현함으로써 데이터의 패턴이나 군집 구조를 쉽게 파악할 수 있습니다.
-   **노이즈 제거**: 중요하지 않거나 중복되는 특성들을 제거하여 데이터의 본질적인 패턴을 더 명확하게 드러낼 수 있습니다.

### PCA 과정

PCA는 일반적으로 다음과 같은 단계로 진행됩니다.

#### 1단계: 데이터 표준화 (Standardization)

PCA는 특성들의 스케일에 민감하게 반응하므로, PCA를 적용하기 전에 데이터를 표준화(평균 0, 표준편차 1)하는 것이 필수적입니다. `StandardScaler`를 사용하여 이를 수행합니다.

```python
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer

# 유방암 데이터셋 로드 (예시)
cancer = load_breast_cancer()
X = cancer.data

# StandardScaler 객체 생성
scaler = StandardScaler()

# 데이터 표준화
X_scaled = scaler.fit_transform(X)

print("원본 데이터 형태:", X.shape)
print("표준화된 데이터 형태:", X_scaled.shape)
print("표준화된 데이터 (일부):
", X_scaled[:3, :5]) # 첫 3개 샘플, 5개 특성만 출력
```

#### 2단계: PCA 적용 (Dimensionality Reduction)

표준화된 데이터에 PCA를 적용하여 주성분을 추출하고 차원을 축소합니다. `n_components` 매개변수를 통해 유지할 주성분의 개수를 지정할 수 있습니다. (예: 10개 주성분 유지)

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

# 데이터 로드 및 표준화 (1단계와 동일)
cancer = load_breast_cancer()
X = cancer.data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA 객체 생성 (10개의 주성분 유지)
pca = PCA(n_components=10)

# PCA 적용하여 차원 축소
X_pca = pca.fit_transform(X_scaled)

print("원본 데이터 차원:", X_scaled.shape)
print("PCA 후 데이터 차원:", X_pca.shape)
print("PCA 후 데이터 (일부):
", X_pca[:3, :5]) # 첫 3개 샘플, 5개 주성분만 출력
```

#### 3단계: 설명 분산 확인 (Explained Variance)

각 주성분이 원본 데이터의 분산을 얼마나 설명하는지 확인하여, 축소된 차원이 원본 데이터의 정보를 얼마나 잘 보존하고 있는지 평가합니다. `explained_variance_ratio_` 속성을 사용합니다.

```python
from sklearn.decomposition import PCA
from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np

# 데이터 로드 및 표준화 (1단계와 동일)
cancer = load_breast_cancer()
X = cancer.data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA 객체 생성 (모든 주성분 유지하여 설명 분산 확인)
pca_full = PCA()
pca_full.fit(X_scaled)

# 각 주성분의 설명 분산 비율
explained_variance_ratio = pca_full.explained_variance_ratio_

# 누적 설명 분산 계산
cumulative_explained_variance = np.cumsum(explained_variance_ratio)

print("각 주성분의 설명 분산 비율 (상위 5개):
", explained_variance_ratio[:5])
print(f"누적 설명 분산 (10개 주성분): {cumulative_explained_variance[9]:.2%}")

# 설명 분산 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), cumulative_explained_variance, marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.title('Cumulative Explained Variance by Principal Components')
plt.grid(True)
plt.show()
```

### 실습 결과 분석

#### 1. 손글씨 데이터셋 (Digits) 분석

Scikit-learn의 `load_digits` 데이터셋은 8x8 픽셀의 손글씨 숫자 이미지로 구성되어 있으며, 각 이미지는 64개의 특성(픽셀 값)을 가집니다. 이 고차원 데이터를 PCA를 통해 10차원으로 축소했을 때의 설명 분산과 성능 변화를 분석합니다.

-   **원본**: 64차원 (8×8 픽셀)
-   **PCA 후**: 10차원 (84% 차원 축소)
-   **설명 분산**: 58.87% (10개의 주성분이 원본 데이터 분산의 약 58.87%를 설명)

```python
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 손글씨 데이터셋 로드
digits = load_digits()
X_digits = digits.data
y_digits = digits.target

# 데이터 표준화
scaler_digits = StandardScaler()
X_digits_scaled = scaler_digits.fit_transform(X_digits)

# PCA 적용 (10개의 주성분 유지)
pca_digits = PCA(n_components=10)
X_digits_pca = pca_digits.fit_transform(X_digits_scaled)

print(f"원본 손글씨 데이터 차원: {X_digits.shape}")
print(f"PCA 후 손글씨 데이터 차원: {X_digits_pca.shape}")
print(f"PCA 후 설명 분산 (10개 주성분): {pca_digits.explained_variance_ratio_.sum():.2%}")
```

#### 2. 성능 비교 (로지스틱 회귀)

원본 데이터, 표준화된 데이터, 그리고 PCA를 통해 차원 축소된 데이터에 대해 로지스틱 회귀 모델을 적용했을 때의 훈련 및 테스트 정확도를 비교합니다. 이를 통해 PCA가 차원을 크게 줄이면서도 어느 정도의 성능을 유지하는지 확인할 수 있습니다.

| 방법 | 차원 | 훈련 정확도 | 테스트 정확도 |
|:---|:---|:---|:---|
| 원본 데이터 | 64 | 1.0000 | 0.9733 |
| 표준화 | 64 | 0.9985 | 0.9711 |
| PCA | 10 | 0.9005 | 0.8933 |

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 데이터 분할 (원본 데이터)
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(X_digits, y_digits, random_state=42)

# 데이터 분할 (표준화된 데이터)
X_train_scaled_lr, X_test_scaled_lr, y_train_scaled_lr, y_test_scaled_lr = train_test_split(X_digits_scaled, y_digits, random_state=42)

# 데이터 분할 (PCA 데이터)
X_train_pca_lr, X_test_pca_lr, y_train_pca_lr, y_test_pca_lr = train_test_split(X_digits_pca, y_digits, random_state=42)

# 1. 원본 데이터로 로지스틱 회귀 학습 및 평가
lr_orig = LogisticRegression(max_iter=10000, random_state=42)
lr_orig.fit(X_train_orig, y_train_orig)
print(f"
원본 데이터 - 훈련 정확도: {lr_orig.score(X_train_orig, y_train_orig):.4f}")
print(f"원본 데이터 - 테스트 정확도: {lr_orig.score(X_test_orig, y_test_orig):.4f}")

# 2. 표준화된 데이터로 로지스틱 회귀 학습 및 평가
lr_scaled = LogisticRegression(max_iter=10000, random_state=42)
lr_scaled.fit(X_train_scaled_lr, y_train_scaled_lr)
print(f"표준화 데이터 - 훈련 정확도: {lr_scaled.score(X_train_scaled_lr, y_train_scaled_lr):.4f}")
print(f"표준화 데이터 - 테스트 정확도: {lr_scaled.score(X_test_scaled_lr, y_test_scaled_lr):.4f}")

# 3. PCA 데이터로 로지스틱 회귀 학습 및 평가
lr_pca = LogisticRegression(max_iter=10000, random_state=42)
lr_pca.fit(X_train_pca_lr, y_train_pca_lr)
print(f"PCA 데이터 - 훈련 정확도: {lr_pca.score(X_train_pca_lr, y_train_pca_lr):.4f}")
print(f"PCA 데이터 - 테스트 정확도: {lr_pca.score(X_test_pca_lr, y_test_pca_lr):.4f}")
```

#### 3. PCA 시각화

PCA를 통해 고차원 데이터를 2차원 또는 3차원으로 축소하여 시각화함으로써 데이터의 군집 패턴을 직관적으로 파악할 수 있습니다. 각 점은 손글씨 숫자를 나타내며, 색상은 실제 숫자(클래스)를 의미합니다.

##### 2D 시각화 (PC1 + PC2)

가장 중요한 두 개의 주성분(PC1, PC2)만을 사용하여 데이터를 2차원 평면에 투영하여 시각화합니다. 이 두 주성분이 설명하는 누적 분산은 약 21.59%입니다.

```python
import matplotlib.pyplot as plt

# PCA 적용 (2개의 주성분 유지)
pca_2d = PCA(n_components=2)
X_digits_pca_2d = pca_2d.fit_transform(X_digits_scaled)

plt.figure(figsize=(10, 8))
plt.scatter(X_digits_pca_2d[:, 0], X_digits_pca_2d[:, 1], c=y_digits, cmap='viridis', s=50, alpha=0.8)
plt.xlabel(f'Principal Component 1 ({pca_2d.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'Principal Component 2 ({pca_2d.explained_variance_ratio_[1]:.2%})')
plt.title('Digits Dataset PCA 2D Visualization')
plt.colorbar(label='Digit Class')
plt.grid(True)
plt.show()

print(f"2D PCA 누적 설명 분산: {pca_2d.explained_variance_ratio_.sum():.2%}")
```

##### 3D 시각화 (PC1 + PC2 + PC3)

세 번째 주성분(PC3)을 추가하여 데이터를 3차원 공간에 투영하여 시각화합니다. 2D 시각화보다 클래스 간의 분리가 더 명확해지는 것을 확인할 수 있으며, 이 세 주성분이 설명하는 누적 분산은 약 30.04%입니다.

```python
from mpl_toolkits.mplot3d import Axes3D

# PCA 적용 (3개의 주성분 유지)
pca_3d = PCA(n_components=3)
X_digits_pca_3d = pca_3d.fit_transform(X_digits_scaled)

fig = plt.figure(figsize=(12, 10))
ax = fig.add_subplot(111, projection='3d')

scatter = ax.scatter(X_digits_pca_3d[:, 0], X_digits_pca_3d[:, 1], X_digits_pca_3d[:, 2], 
                     c=y_digits, cmap='viridis', s=50, alpha=0.8)

ax.set_xlabel(f'Principal Component 1 ({pca_3d.explained_variance_ratio_[0]:.2%})')
ax.set_ylabel(f'Principal Component 2 ({pca_3d.explained_variance_ratio_[1]:.2%})')
ax.set_zlabel(f'Principal Component 3 ({pca_3d.explained_variance_ratio_[2]:.2%})')
ax.set_title('Digits Dataset PCA 3D Visualization')
plt.colorbar(scatter, label='Digit Class')
plt.grid(True)
plt.show()

print(f"3D PCA 누적 설명 분산: {pca_3d.explained_variance_ratio_.sum():.2%}")
```

### PCA의 장단점

#### 장점
- **차원 축소**: 저장 공간 절약, 계산 속도 향상
- **노이즈 제거**: 중요하지 않은 특성 제거
- **시각화**: 고차원 데이터를 2D/3D로 표현
- **다중공선성 해결**: 상관관계 높은 특성들 처리
- **과적합 방지**: 특성 수 감소로 일반화 성능 향상

#### 단점
- **해석 어려움**: 새로운 특성(주성분)의 의미 파악 어려움
- **정보 손실**: 일부 분산 정보 손실
- **선형 변환 한계**: 비선형 관계 포착 제한
- **스케일링 필수**: 사전 데이터 표준화 필요

---

## 3. K-Fold 교차검증

### 교차검증의 목적

교차 검증(Cross-Validation)은 모델의 성능을 더욱 신뢰성 있고 객관적으로 평가하기 위한 통계적 방법입니다. 단일 훈련/테스트 분할의 한계를 극복하고, 모델이 특정 데이터셋에 과적합되는 것을 방지하며, 새로운 데이터에 대한 일반화 성능을 더 정확하게 측정하는 데 목적이 있습니다.

-   **과적합 방지**: 모델이 훈련 데이터의 특정 패턴이나 노이즈에만 과도하게 맞춰지는 것을 방지하여, 보지 못한 데이터에 대한 예측 성능이 떨어지는 현상을 줄입니다.
-   **일반화 성능 측정**: 데이터셋을 여러 번 나누어 모델을 학습하고 평가함으로써, 모델이 다양한 데이터에 대해 얼마나 일관된 성능을 보이는지 측정합니다. 이는 모델의 실제 환경에서의 성능을 더 잘 예측할 수 있게 합니다.
-   **안정적인 성능 측정**: 여러 번의 평가 결과를 평균하여 최종 성능으로 제시함으로써, 특정 데이터 분할에 따른 성능 변동성을 줄이고 더 안정적인 성능 추정치를 얻을 수 있습니다.
-   **하이퍼파라미터 튜닝**: 교차 검증은 `GridSearchCV`나 `RandomizedSearchCV`와 같은 하이퍼파라미터 튜닝 기법의 핵심 구성 요소로 사용되어, 최적의 모델 파라미터를 찾는 데 기여합니다.

### 교차검증 방법들

#### 1. K-Fold 교차검증

K-Fold 교차검증은 데이터셋을 K개의 동일한 크기의 폴드(Fold)로 나눈 후, 각 폴드를 한 번씩 테스트 세트로 사용하고 나머지 K-1개의 폴드를 훈련 세트로 사용하여 모델을 K번 학습하고 평가하는 방법입니다. 

-   **특징**: 
    -   데이터를 K개 그룹으로 나누어 검증합니다.
    -   각 그룹을 한 번씩 테스트셋으로 사용하고 나머지를 훈련셋으로 사용합니다.
    -   **클래스 분포를 고려하지 않습니다.** 따라서 클래스 불균형이 있는 데이터셋에서는 편향된 평가를 초래할 수 있습니다.

```python
from sklearn.model_selection import KFold
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Iris 데이터셋 로드
iris = load_iris()
X, y = iris.data, iris.target

# KFold 객체 생성 (n_splits=5: 5개의 폴드로 분할, shuffle=True: 데이터 섞기)
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# 모델 정의
model = LogisticRegression(max_iter=200, random_state=42)

# 각 폴드별 정확도를 저장할 리스트
kfold_accuracies = []

# K-Fold 교차 검증 수행
for train_index, test_index in kfold.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    kfold_accuracies.append(accuracy)

print(f"K-Fold 각 폴드의 정확도: {kfold_accuracies}")
print(f"K-Fold 평균 정확도: {np.mean(kfold_accuracies):.4f}")
print(f"K-Fold 정확도 표준편차: {np.std(kfold_accuracies):.4f}")
```

#### 2. Stratified K-Fold

Stratified K-Fold는 K-Fold 교차검증의 단점을 보완한 방법으로, 특히 **분류 문제**에서 유용합니다. 데이터셋을 K개의 폴드로 나눌 때, 각 폴드에 원본 데이터셋의 **클래스 비율을 동일하게 유지**하도록 분할합니다. 이는 클래스 불균형이 심한 데이터셋에서 모델 성능을 더 신뢰성 있게 평가할 수 있게 합니다.

-   **특징**: 
    -   클래스 분포를 고려하여 각 폴드에서 클래스 비율을 균등하게 유지합니다.
    -   **분류 문제에서 권장**되는 교차검증 방법입니다.

```python
from sklearn.model_selection import StratifiedKFold
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Iris 데이터셋 로드
iris = load_iris()
X, y = iris.data, iris.target

# StratifiedKFold 객체 생성 (n_splits=5, shuffle=True: 데이터 섞기)
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 모델 정의
model = LogisticRegression(max_iter=200, random_state=42)

# 각 폴드별 정확도를 저장할 리스트
stratified_accuracies = []

# Stratified K-Fold 교차 검증 수행
for train_index, test_index in stratified_kfold.split(X, y): # y를 split에 전달하여 클래스 비율 유지
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    stratified_accuracies.append(accuracy)

print(f"Stratified K-Fold 각 폴드의 정확도: {stratified_accuracies}")
print(f"Stratified K-Fold 평균 정확도: {np.mean(stratified_accuracies):.4f}")
print(f"Stratified K-Fold 정확도 표준편차: {np.std(stratified_accuracies):.4f}")
```

#### 3. `cross_val_score` 함수

`cross_val_score` 함수는 Scikit-learn에서 제공하는 편리한 함수로, 교차검증 과정을 한 줄로 간결하게 수행할 수 있게 해줍니다. 내부적으로 K-Fold 또는 Stratified K-Fold를 사용하여 모델을 평가하고, 각 폴드의 점수를 반환합니다.

-   **특징**: 
    -   **한 줄로 교차검증 수행**: 복잡한 반복문 없이 간단하게 처리할 수 있습니다.
    -   **다양한 평가 지표**: `scoring` 매개변수를 통해 `accuracy`, `precision`, `recall`, `f1` 등 다양한 평가 지표를 지정할 수 있습니다.
    -   **자동 평균 계산**: 각 폴드의 결과를 자동으로 반환하며, 사용자가 직접 평균을 계산할 수 있습니다.

```python
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression

# Iris 데이터셋 로드
iris = load_iris()
X, y = iris.data, iris.target

# 모델 정의
model = LogisticRegression(max_iter=200, random_state=42)

# cross_val_score 함수를 사용하여 교차 검증 수행
# cv=5: 5-Fold 교차 검증 (분류 문제이므로 Stratified K-Fold가 자동으로 적용됨)
# scoring="accuracy": 평가 지표로 정확도 사용
scores = cross_val_score(model, X, y, scoring="accuracy", cv=5)

print(f"cross_val_score 각 폴드의 정확도: {scores}")
print(f"cross_val_score 평균 정확도: {scores.mean():.4f}")
print(f"cross_val_score 정확도 표준편차: {scores.std():.4f}")
```

### K-Fold 동작 원리 (K=5 예시)

K-Fold 교차검증은 데이터셋을 K개의 부분집합(폴드)으로 나눈 후, 각 폴드를 한 번씩 테스트 세트로 사용하고 나머지 폴드를 훈련 세트로 사용하는 방식입니다. K=5일 때의 동작 원리는 다음과 같습니다.

```
전체 데이터: [Fold 1] [Fold 2] [Fold 3] [Fold 4] [Fold 5]

1회차: 훈련셋 [Fold 2, 3, 4, 5] → 테스트셋 [Fold 1]
2회차: 훈련셋 [Fold 1, 3, 4, 5] → 테스트셋 [Fold 2]
3회차: 훈련셋 [Fold 1, 2, 4, 5] → 테스트셋 [Fold 3]
4회차: 훈련셋 [Fold 1, 2, 3, 5] → 테스트셋 [Fold 4]
5회차: 훈련셋 [Fold 1, 2, 3, 4] → 테스트셋 [Fold 5]

최종 성능: 5회차 점수들의 평균
```

### 실습 결과 비교

#### 1. Iris 데이터셋 성능 (의사결정나무)

Iris 데이터셋에 대해 의사결정나무 모델을 사용하여 K-Fold와 Stratified K-Fold 교차 검증을 수행한 결과입니다. Stratified K-Fold가 더 안정적인 성능을 보여줍니다.

| 방법 | 평균 정확도 | 표준편차 | 최고 정확도 | 최저 정확도 |
|:---|:---|:---|:---|:---|
| K-Fold | 0.9133 | 0.0833 | 1.0 | 0.8 |
| Stratified K-Fold | 0.9533 | 0.0340 | 1.0 | 0.9 |
| cross_val_score | 0.9533 | 0.0340 | 1.0 | 0.9 |

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score
from sklearn.datasets import load_iris
import numpy as np

# Iris 데이터셋 로드
iris = load_iris()
X, y = iris.data, iris.target

# 모델 정의
dtree_model = DecisionTreeClassifier(random_state=42)

# K-Fold 교차 검증
kfold_cv = KFold(n_splits=5, shuffle=True, random_state=42)
kfold_scores = cross_val_score(dtree_model, X, y, cv=kfold_cv, scoring='accuracy')
print(f"K-Fold (Decision Tree) - 평균 정확도: {kfold_scores.mean():.4f}, 표준편차: {kfold_scores.std():.4f}")

# Stratified K-Fold 교차 검증
stratified_kfold_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
stratified_scores = cross_val_score(dtree_model, X, y, cv=stratified_kfold_cv, scoring='accuracy')
print(f"Stratified K-Fold (Decision Tree) - 평균 정확도: {stratified_scores.mean():.4f}, 표준편차: {stratified_scores.std():.4f}")

# cross_val_score (자동 Stratified K-Fold 적용)
cross_val_scores = cross_val_score(dtree_model, X, y, cv=5, scoring='accuracy')
print(f"cross_val_score (Decision Tree) - 평균 정확도: {cross_val_scores.mean():.4f}, 표준편차: {cross_val_scores.std():.4f}")
```

#### 2. 다양한 모델 성능 비교

Iris 데이터셋에 대해 다양한 분류 모델을 Stratified K-Fold 교차 검증으로 평가한 결과입니다. K-NN 모델이 가장 높은 평균 정확도를 보였습니다.

| 모델 | 평균 정확도 | 표준편차 |
|:---|:---|:---|
| Decision Tree | 0.9533 | 0.0340 |
| Random Forest | 0.9667 | 0.0211 |
| SVM | 0.9667 | 0.0211 |
| K-NN | 0.9733 | 0.0249 |

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

# 모델 정의
rf_model = RandomForestClassifier(random_state=42)
svm_model = SVC(random_state=42)
knn_model = KNeighborsClassifier()

models = {
    'Decision Tree': dtree_model,
    'Random Forest': rf_model,
    'SVM': svm_model,
    'K-NN': knn_model
}

print("\n--- 다양한 모델의 교차 검증 성능 ---")
for name, model in models.items():
    # 분류 문제이므로 Stratified K-Fold가 자동으로 적용됨
    scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
    print(f"{name}: 평균 정확도 {scores.mean():.4f}, 표준편차 {scores.std():.4f}")
```

### K-Fold vs Stratified K-Fold 차이점

#### 일반 K-Fold 문제점
- **클래스 분포 불균등**: 순서대로 분할하여 특정 폴드에 특정 클래스만 포함
- **성능 변동 큰**: 표준편차 0.0833으로 불안정
- **편향된 평가**: 일부 폴드에서 극단적인 결과

#### Stratified K-Fold 장점
- **클래스 분포 균등**: 각 폴드에서 클래스 비율을 원본과 동일하게 유지
- **안정적 성능**: 표준편차 0.0340으로 일관된 결과
- **공정한 평가**: 모든 폴드에서 균등한 조건

---

## 4. 성능 비교 및 분석

이 섹션에서는 앞서 다룬 비지도 학습(군집 분석, PCA)과 교차 검증 기법들이 모델의 성능에 미치는 영향을 종합적으로 분석합니다.

### 4.1. 비지도 학습 (PCA)의 효과 분석

#### PCA 차원축소 효과

손글씨 숫자 데이터셋(Digits)에 PCA를 적용하여 차원을 축소했을 때의 성능 변화를 통해 PCA의 효과를 분석합니다.

-   **차원 축소율**: 원본 64차원 데이터를 10차원으로 축소하여 **약 84%의 차원 축소**를 달성했습니다.
-   **성능 유지**: 차원 축소에도 불구하고 로지스틱 회귀 모델의 테스트 정확도는 97.33%에서 89.33%로 **약 8% 감소**하는 데 그쳤습니다. 이는 PCA가 데이터의 중요한 정보를 효과적으로 보존했음을 의미합니다.
-   **효율성 개선**: 차원 축소를 통해 모델 학습 시간 단축 및 메모리 사용량 감소 등 **계산 효율성**을 크게 개선할 수 있습니다. 특히 고차원 데이터셋에서 이러한 이점은 더욱 두드러집니다.

#### 교차검증 안정성

교차 검증 방법 중 Stratified K-Fold가 일반 K-Fold에 비해 얼마나 안정적인 성능 측정치를 제공하는지 분석합니다.

-   **Stratified K-Fold**: 분류 문제에서 클래스 불균형을 고려하여 각 폴드의 클래스 비율을 원본과 동일하게 유지함으로써, 가장 안정적이고 신뢰할 수 있는 성능 평가 결과를 제공합니다.
-   **표준편차 개선**: Iris 데이터셋에 대한 의사결정나무 모델의 교차 검증 결과, 일반 K-Fold의 표준편차(0.0833)가 Stratified K-Fold의 표준편차(0.0340)로 **약 59% 향상**되었습니다. 이는 Stratified K-Fold가 성능 변동성을 크게 줄여 더 일관된 평가를 가능하게 함을 보여줍니다.
-   **실무 권장**: 분류 문제, 특히 클래스 불균형이 존재하는 데이터셋에서는 Stratified K-Fold를 필수적으로 사용하여 모델의 일반화 성능을 공정하게 평가하는 것이 중요합니다.

### 4.2. 모델별 특성 분석 및 성능 비교

Iris 데이터셋에 대한 다양한 분류 모델들의 교차 검증 결과를 통해 각 모델의 특성과 성능을 비교 분석합니다.

#### 앙상블 모델의 우수성

-   **Random Forest**: 여러 개의 의사결정나무를 앙상블하여 과적합 위험을 줄이고 안정적인 성능을 제공합니다. 개별 트리의 단점을 보완하여 높은 예측 정확도를 달성합니다.
-   **SVM (Support Vector Machine)**: 복잡한 결정 경계를 학습할 수 있는 강력한 분류 모델입니다. 특히 고차원 공간에서 효과적이며, 마진을 최대화하여 분류 성능을 높입니다. (단, 스케일링에 매우 민감)
-   **K-NN (K-Nearest Neighbors)**: 단순하고 직관적인 거리 기반 모델이지만, 데이터의 지역적 패턴을 효과적으로 학습하여 높은 성능을 보일 수 있습니다. 특히 데이터의 양이 충분하고 특성 간의 거리가 의미 있는 경우에 강점을 가집니다.

#### 단일 모델의 한계

-   **Decision Tree**: 직관적이고 해석하기 쉽지만, 단일 트리로는 복잡한 패턴을 학습하는 데 한계가 있으며, 훈련 데이터에 과적합될 위험이 높습니다. 작은 데이터 변화에도 민감하게 반응하여 불안정한 성능을 보일 수 있습니다.
-   **개선 방안**: 단일 의사결정나무의 한계를 극복하기 위해 Random Forest와 같은 **앙상블 기법**을 적용하거나, 트리의 깊이 제한(`max_depth`) 및 최소 샘플 수(`min_samples_split`) 설정과 같은 **정규화 기법**을 적용하여 과적합을 방지할 수 있습니다.

---

## 5. 실무 적용 가이드

### 군집분석 활용 가이드

군집분석은 데이터 내의 숨겨진 패턴을 발견하고 고객 세분화, 이상 탐지 등 다양한 비즈니스 문제 해결에 활용될 수 있습니다.

#### 적용 분야
-   **고객 세분화**: 고객의 구매 패턴, 행동 특성 등을 기반으로 유사한 고객 그룹을 나누어 맞춤형 마케팅 전략을 수립합니다.
-   **이미지 세그멘테이션**: 이미지 내에서 유사한 픽셀들을 그룹화하여 객체를 분리하거나 특정 영역을 식별하는 컴퓨터 비전 분야에 활용됩니다.
-   **유전자 분석**: 유전자 발현 패턴을 기반으로 유사한 유전자 그룹을 식별하거나 질병과의 연관성을 분석하는 바이오인포매틱스 분야에 사용됩니다.
-   **시장 분석**: 경쟁 그룹 식별, 제품 포지셔닝, 새로운 시장 기회 탐색 등에 활용됩니다.

#### 주의사항

군집분석을 효과적으로 활용하기 위해서는 다음과 같은 사항들을 고려해야 합니다.

1.  **데이터 스케일링 필수**: K-Means와 같은 거리 기반 군집 알고리즘은 특성들의 스케일에 매우 민감합니다. 스케일이 큰 특성이 군집 결과에 지배적인 영향을 미칠 수 있으므로, 군집분석 전에 반드시 `StandardScaler` 등을 사용하여 데이터를 표준화해야 합니다.
    ```python
    from sklearn.preprocessing import StandardScaler
    # X는 군집분석을 수행할 특성 데이터
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    print("스케일링 후 데이터 형태:", X_scaled.shape)
    ```
2.  **적절한 K값 선택**: 군집의 개수(K)는 군집분석의 핵심 하이퍼파라미터입니다. 엘보우 방법, 실루엣 방법, 또는 도메인 지식을 활용하여 최적의 K값을 찾아야 합니다.
    ```python
    from sklearn.cluster import KMeans
    # 최적의 K값을 3으로 가정
    kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
    kmeans.fit(X_scaled) # 스케일링된 데이터로 학습
    print("군집 중심점:", kmeans.cluster_centers_)
    print("각 데이터 포인트의 군집 레이블 (일부):", kmeans.labels_[:10])
    ```
3.  **결과 검증 및 해석**: 군집분석 결과는 정답이 없으므로, 실루엣 점수와 같은 통계적 지표뿐만 아니라 도메인 전문가의 검토를 통해 군집 결과의 유의미성을 확인하고 해석해야 합니다.
    ```python
    from sklearn.metrics import silhouette_score
    # labels는 KMeans.labels_ 속성에서 얻은 군집 레이블
    score = silhouette_score(X_scaled, kmeans.labels_)
    print(f"실루엣 점수: {score:.4f}")
    # 실루엣 점수는 -1과 1 사이의 값. 1에 가까울수록 좋은 군집.
    ```

### PCA 적용 가이드

#### 사용 시점
1. **고차원 데이터**: 특성 수가 샘플 수보다 많을 때
2. **상관관계 높음**: 다중공선성 문제 발생 시
3. **시각화 필요**: 고차원 데이터를 2D/3D로 표현
4. **계산 효율성**: 학습 시간 단축 필요 시

#### 실무 템플릿
```python
# 1. 데이터 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 2. PCA 적용
pca = PCA(n_components=0.95)  # 95% 분산 유지
X_pca = pca.fit_transform(X_scaled)

# 3. 결과 확인
print(f"원본 차원: {X.shape[1]}")
print(f"축소 차원: {X_pca.shape[1]}")
print(f"설명 분산: {pca.explained_variance_ratio_.sum():.2%}")
```

### 교차검증 적용 가이드

#### 상황별 교차검증 선택
```python
# 분류 문제 (클래스 불균형 고려)
from sklearn.model_selection import StratifiedKFold
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# 회귀 문제 (연속값 예측)
from sklearn.model_selection import KFold
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# 시계열 데이터
from sklearn.model_selection import TimeSeriesSplit
cv = TimeSeriesSplit(n_splits=5)

# 그룹 데이터
from sklearn.model_selection import GroupKFold
cv = GroupKFold(n_splits=5)
```

#### 적절한 K값 선택
- **K=5 또는 K=10**: 일반적으로 권장
- **작은 데이터셋**: K=5 또는 Leave-One-Out
- **큰 데이터셋**: K=3 또는 Hold-out 검증

### 종합 워크플로우

#### 1단계: 탐색적 데이터 분석
```python
# 데이터 기본 정보
print(f"데이터 크기: {X.shape}")
print(f"결측치: {pd.DataFrame(X).isnull().sum().sum()}")

# 상관관계 분석 (PCA 필요성 판단)
corr_matrix = pd.DataFrame(X).corr()
high_corr = (corr_matrix.abs() > 0.8).sum().sum()
```

#### 2단계: 전처리 및 차원축소
```python
# 스케일링
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# PCA (필요시)
if X.shape[1] > 50:  # 고차원인 경우
    pca = PCA(n_components=0.95)
    X_final = pca.fit_transform(X_scaled)
else:
    X_final = X_scaled
```

#### 3단계: 모델 평가
```python
# 교차검증을 통한 모델 비교
models = {
    'RandomForest': RandomForestClassifier(random_state=42),
    'SVM': SVC(random_state=42),
    'KNN': KNeighborsClassifier()
}

for name, model in models.items():
    scores = cross_val_score(model, X_final, y, cv=5, scoring='accuracy')
    print(f"{name}: {scores.mean():.4f} (±{scores.std():.4f})")
```

### 핵심 권장사항

1. **항상 Stratified K-Fold 사용** (분류 문제)
2. **PCA 전 반드시 스케일링 적용**
3. **군집 개수는 도메인 지식과 통계적 방법 병행**
4. **교차검증으로 모델 선택 및 성능 평가**
5. **재현 가능한 결과를 위해 random_state 설정**

### 주의사항

1. **데이터 누수 방지**: 테스트 데이터로 전처리하지 않기
2. **과적합 모니터링**: 훈련-검증 성능 차이 확인
3. **계산 비용 고려**: 대용량 데이터에서는 샘플링 고려
4. **해석 가능성**: PCA 후 특성 의미 파악 어려움 인지

### 결론

- **군집분석**: 패턴 발견을 위한 비지도학습
- **주성분분석**: 효율적인 차원축소와 시각화
- **교차검증**: 신뢰할 수 있는 모델 평가

이 기법들은 실무에서 **데이터 전처리, 탐색적 분석, 모델 검증**에 필수적으로 사용되는 도구들입니다.

---

[⏮️ 이전 문서](./0709_ML정리.md) | [다음 문서 ⏭️](./0714_ML정리.md)