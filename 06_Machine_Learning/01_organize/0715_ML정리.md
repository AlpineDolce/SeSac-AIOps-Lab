# π“ λ¨Έμ‹ λ¬λ‹ λ¨λΈ μµμ ν™”: ν•μ΄νΌνλΌλ―Έν„° νλ‹κ³Ό Pipeline ν™μ© μ „λµ

> λ³Έ λ¬Έμ„λ” λ¨Έμ‹ λ¬λ‹ λ¨λΈμ μ„±λ¥μ„ κ·Ήλ€ν™”ν•κΈ° μ„ν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ²•λ“¤μ„ μ‹¬μΈµμ μΌλ΅ λ‹¤λ£Ήλ‹λ‹¤. μ „ν†µμ μΈ Grid Searchλ¶€ν„° ν¨μ¨μ μΈ Bayesian Optimization(Optuna)κΉμ§€ λ‹¤μ–‘ν• λ°©λ²•λ΅ μ„ λΉ„κµ λ¶„μ„ν•κ³ , Scikit-learn Pipelineμ„ ν™μ©ν•μ—¬ μ „μ²λ¦¬ λ° λ¨λΈλ§ κ³Όμ •μ„ ν†µν•©ν•λ” μ‹¤μ „ μ „λµμ„ μ μ‹ν•©λ‹λ‹¤. κ° κΈ°λ²•μ μ¥λ‹¨μ , μ‹¤μµ μμ , κ·Έλ¦¬κ³  μ‹¤λ¬΄ μ μ© κ°€μ΄λ“λ¥Ό ν†µν•΄ λ¨λΈ κ°λ°μ ν¨μ¨μ„±κ³Ό μ‹ λΆ°μ„±μ„ λ†’μ΄λ” λ° κΈ°μ—¬ν•©λ‹λ‹¤.

---

## λ©μ°¨

1.  [ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κ°λ΅ ](#1-ν•μ΄νΌνλΌλ―Έν„°-μµμ ν™”-κ°λ΅ )
    *   [1.1 ν•μ΄νΌνλΌλ―Έν„°λ€?](#11-ν•μ΄νΌνλΌλ―Έν„°λ€)
    *   [1.2 ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ μ¤‘μ”μ„±](#12-ν•μ΄νΌνλΌλ―Έν„°-μµμ ν™”μ-μ¤‘μ”μ„±)
    *   [1.3 μ£Όμ” μµμ ν™” λ°©λ²•λ΅ ](#13-μ£Όμ”-μµμ ν™”-λ°©λ²•λ΅ )
2.  [Optuna λ² μ΄μ§€μ• μµμ ν™”](#2-optuna-λ² μ΄μ§€μ•-μµμ ν™”)
    *   [2.1 Optuna κ°μ”](#21-optuna-κ°μ”)
    *   [2.2 μ‹¤μµ: μ λ°©μ•” λ°μ΄ν„° λ¶„λ¥ μµμ ν™” (RandomForest)](#22-μ‹¤μµ-μ λ°©μ•”-λ°μ΄ν„°-λ¶„λ¥-μµμ ν™”-randomforest)
3.  [Scikit-learn Pipeline ν™μ©](#3-scikit-learn-pipeline-ν™μ©)
    *   [3.1 Pipeline κΈ°μ΄ κ°λ…](#31-pipeline-κΈ°μ΄-κ°λ…)
    *   [3.2 μ‹¤μµ: Iris λ‹¤μ¤‘λ¶„λ¥ Pipeline](#32-μ‹¤μµ-iris-λ‹¤μ¤‘λ¶„λ¥-pipeline)
    *   [3.3 Pipeline + GridSearchCV ν†µν•©](#33-pipeline--gridsearchcv-ν†µν•©)
    *   [3.4 Pipeline + Optuna ν†µν•©](#34-pipeline--optuna-ν†µν•©)
4.  [GridSearchCV vs Optuna λΉ„κµ](#4-gridsearchcv-vs-optuna-λΉ„κµ)
    *   [4.1 μΆ…ν•© μ„±λ¥ λΉ„κµ](#41-μΆ…ν•©-μ„±λ¥-λΉ„κµ)
    *   [4.2 λ°©λ²•λ΅ λ³„ μ¥λ‹¨μ  λ¶„μ„](#42-λ°©λ²•λ΅ λ³„-μ¥λ‹¨μ -λ¶„μ„)
    *   [4.3 μ‹¤λ¬΄ μ μ© κ°€μ΄λ“](#43-μ‹¤λ¬΄-μ μ©-κ°€μ΄λ“)
5.  [μ‹¤λ¬΄ μ μ© μ „λµ](#5-μ‹¤λ¬΄-μ μ©-μ „λµ)
    *   [5.1 ν”„λ΅μ νΈ λ‹¨κ³„λ³„ κ°€μ΄λ“](#51-ν”„λ΅μ νΈ-λ‹¨κ³„λ³„-κ°€μ΄λ“)
    *   [5.2 ν•μ΄νΌνλΌλ―Έν„° μ„ νƒ μ „λµ](#52-ν•μ΄νΌνλΌλ―Έν„°-μ„ νƒ-μ „λµ)
    *   [5.3 μ„±λ¥ ν‰κ°€ λ° ν•΄μ„](#53-μ„±λ¥-ν‰κ°€-λ°-ν•΄μ„)
6.  [μΆ…ν•© μ •λ¦¬ λ° κ²°λ΅ ](#6-μΆ…ν•©-μ •λ¦¬-λ°-κ²°λ΅ )
    *   [6.1 ν•µμ‹¬ ν•™μµ λ‚΄μ© μ”μ•½](#61-ν•µμ‹¬-ν•™μµ-λ‚΄μ©-μ”μ•½)
    *   [6.2 μ‹¤λ¬΄ μ μ© Best Practices](#62-μ‹¤λ¬΄-μ μ©-best-practices)
    *   [6.3 ν–¥ν›„ ν•™μµ λ°©ν–¥](#63-ν–¥ν›„-ν•™μµ-λ°©ν–¥)

---

## 1. ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κ°λ΅ 

### 1.1 ν•μ΄νΌνλΌλ―Έν„°λ€?

> **ν•μ΄νΌνλΌλ―Έν„°(Hyperparameter)**: λ¨Έμ‹ λ¬λ‹ λ¨λΈμ ν•™μµ κ³Όμ • μ „μ— μ‚¬μ©μκ°€ μ§μ ‘ μ„¤μ •ν•΄μ•Ό ν•λ” λ§¤κ°λ³€μμ…λ‹λ‹¤. λ¨λΈμ΄ λ°μ΄ν„°λ΅λ¶€ν„° μλ™μΌλ΅ ν•™μµν•λ” νλΌλ―Έν„°(μ: μ„ ν• νκ·€μ κ°€μ¤‘μΉ, μ‹ κ²½λ§μ κ°€μ¤‘μΉ)μ™€λ” κµ¬λ³„λ©λ‹λ‹¤.

#### λ¨λΈ νλΌλ―Έν„° vs ν•μ΄νΌνλΌλ―Έν„°

| κµ¬λ¶„ | λ¨λΈ νλΌλ―Έν„° | ν•μ΄νΌνλΌλ―Έν„° |
|:---|:---|:---|
| **μ •μ** | λ°μ΄ν„°λ΅λ¶€ν„° ν•™μµλλ” κ°’ (μ: μ„ ν• νκ·€μ κ³„μ, μ‹ κ²½λ§μ κ°€μ¤‘μΉ λ° νΈν–¥) | λ¨λΈ ν•™μµ μ „μ— μ‚¬μ©μκ°€ μ„¤μ •ν•λ” κ°’ (μ: ν•™μµλ¥ , νΈλ¦¬μ κΉμ΄, μ •κ·ν™” κ°•λ„, ν΄λ¬μ¤ν„° κ°μ) |
| **κ²°μ • λ°©λ²•** | μ•κ³ λ¦¬μ¦μ΄ ν›λ ¨ λ°μ΄ν„°λ¥Ό ν†µν•΄ μλ™ ν•™μµ | μ‚¬λμ΄ μ§μ ‘ μ„¤μ •ν•κ±°λ‚, ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” κΈ°λ²•μ„ ν†µν•΄ νƒμƒ‰ν•μ—¬ κ²°μ • |
| **μν–¥** | λ¨λΈμ μμΈ΅ μ„±λ¥μ— μ§μ ‘μ μΈ μν–¥ | ν•™μµ κ³Όμ •κ³Ό μµμΆ… λ¨λΈμ μ„±λ¥ λ° μΌλ°ν™” λ¥λ ¥μ— ν° μν–¥ |

### 1.2 ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ μ¤‘μ”μ„±

ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ” λ¨Έμ‹ λ¬λ‹ λ¨λΈ κ°λ° κ³Όμ •μ—μ„ λ§¤μ° μ¤‘μ”ν• λ‹¨κ³„μ…λ‹λ‹¤. μ μ ν• ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •μ€ λ¨λΈμ μ„±λ¥μ„ κ·Ήλ€ν™”ν•κ³ , μ‹¤μ  ν™κ²½μ—μ„μ μ μ© κ°€λ¥μ„±μ„ λ†’μ΄λ” λ° κ²°μ •μ μΈ μ—­ν• μ„ ν•©λ‹λ‹¤.

#### ν•µμ‹¬ μµμ ν™” λ©ν‘

-   **μ„±λ¥ ν–¥μƒ**: λ¨λΈμ μμΈ΅ μ •ν™•λ„, μ •λ°€λ„, μ¬ν„μ¨ λ“± ν•µμ‹¬ μ„±λ¥ μ§€ν‘λ¥Ό κ°μ„ ν•μ—¬ λ¬Έμ  ν•΄κ²° λ¥λ ¥μ„ λ†’μ…λ‹λ‹¤.
-   **κ³Όμ ν•© λ°©μ§€**: λ¨λΈμ΄ ν›λ ¨ λ°μ΄ν„°μ—λ§ κ³Όλ„ν•κ² λ§μ¶°μ§€λ” κ³Όμ ν•©(Overfitting)μ„ λ°©μ§€ν•κ³ , μƒλ΅μ΄ λ°μ΄ν„°μ— λ€ν• μΌλ°ν™” μ„±λ¥(Generalization Performance)μ„ ν™•λ³΄ν•©λ‹λ‹¤.
-   **ν¨μ¨μ„± μ¦λ€**: λ¶ν•„μ”ν• κ³„μ‚° λ³µμ΅λ„λ¥Ό μ¤„μ΄κ³ , λ¨λΈ ν•™μµ μ‹κ°„κ³Ό μμ›(CPU/GPU, λ©”λ¨λ¦¬) μ‚¬μ©μ„ μµμ ν™”ν•μ—¬ κ°λ° λ° μ΄μ ν¨μ¨μ„±μ„ λ†’μ…λ‹λ‹¤.

#### μ£Όμ: μµμ ν™” μ‹¤ν¨ μ‹ λ¬Έμ μ 

-   **κ³Όμ ν•©(Overfitting)**: λ¨λΈμ΄ ν›λ ¨ λ°μ΄ν„°μ λ…Έμ΄μ¦κΉμ§€ ν•™μµν•μ—¬ ν›λ ¨ μ„±λ¥μ€ λ†’μ§€λ§, μ‹¤μ  λ°μ΄ν„°μ— λ€ν• μμΈ΅ μ„±λ¥μ΄ ν„μ €ν λ–¨μ–΄μ§‘λ‹λ‹¤.
-   **κ³Όμ†μ ν•©(Underfitting)**: λ¨λΈμ΄ λ°μ΄ν„°λ¥Ό μ¶©λ¶„ν ν•™μµν•μ§€ λ»ν•μ—¬ ν›λ ¨ λ°μ΄ν„°μ™€ ν…μ¤νΈ λ°μ΄ν„° λ¨λ‘μ—μ„ λ‚®μ€ μ„±λ¥μ„ λ³΄μ…λ‹λ‹¤. μ΄λ” λ¨λΈμ λ³µμ΅λ„κ°€ λ„λ¬΄ λ‚®κ±°λ‚, ν•™μµμ΄ λ¶μ¶©λ¶„ν•  λ• λ°μƒν•©λ‹λ‹¤.
-   **μμ› λ‚­λΉ„**: λΉ„ν¨μ¨μ μΈ ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰μ€ λ¶ν•„μ”ν• κ³„μ‚° μ‹κ°„κ³Ό μμ› μ†λ¨λ¥Ό μ•ΌκΈ°ν•μ—¬ κ°λ° λΉ„μ©μ„ μ¦κ°€μ‹ν‚µλ‹λ‹¤.

### 1.3 μ£Όμ” μµμ ν™” λ°©λ²•λ΅ 

ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μµμ ν™”ν•λ” λ°©λ²•μ€ ν¬κ² μ „ν†µμ μΈ λ°©λ²•κ³Ό ν„λ€μ μΈ λ°©λ²•μΌλ΅ λ‚λ μ μμµλ‹λ‹¤.

#### μ „ν†µμ  λ°©λ²•

1.  **Manual Search (μλ™ νƒμƒ‰)**:
    -   **μ„¤λ…**: λ°μ΄ν„° κ³Όν•™μμ κ²½ν—κ³Ό μ§κ΄€μ— μμ΅΄ν•μ—¬ ν•μ΄νΌνλΌλ―Έν„° κ°’μ„ μλ™μΌλ΅ λ³€κ²½ν•λ©° λ¨λΈ μ„±λ¥μ„ ν™•μΈν•λ” λ°©λ²•μ…λ‹λ‹¤.
    -   **μ¥μ **: νΉμ • λ„λ©”μΈ μ§€μ‹μ΄ ν’λ¶€ν•  λ• λΉ λ¥΄κ² μΆ‹μ€ κ²°κ³Όλ¥Ό μ–»μ„ μλ„ μμµλ‹λ‹¤.
    -   **λ‹¨μ **: λ§¤μ° μ‹κ°„ μ†λ¨μ μ΄κ³  λΉ„ν¨μ¨μ μ΄λ©°, μµμ μ μ΅°ν•©μ„ μ°ΎκΈ° μ–΄λ µμµλ‹λ‹¤. μ¬ν„μ„±μ΄ λ‚®μµλ‹λ‹¤.

2.  **Grid Search (κ²©μ νƒμƒ‰)**:
    -   **μ„¤λ…**: μ‚¬μ©μκ°€ μ§€μ •ν• ν•μ΄νΌνλΌλ―Έν„° κ°’λ“¤μ λ¨λ“  κ°€λ¥ν• μ΅°ν•©μ— λ€ν•΄ λ¨λΈμ„ ν•™μµν•κ³  κµμ°¨ κ²€μ¦μ„ ν†µν•΄ μµμ μ μ΅°ν•©μ„ μ°Ύλ” λ°©λ²•μ…λ‹λ‹¤.
    -   **μ¥μ **: λ¨λ“  μ΅°ν•©μ„ μ²΄κ³„μ μΌλ΅ νƒμƒ‰ν•λ―€λ΅, μ£Όμ–΄μ§„ νƒμƒ‰ κ³µκ°„ λ‚΄μ—μ„λ” μµμ μ μ΅°ν•©μ„ ν™•μ‹¤ν μ°Ύμ„ μ μμµλ‹λ‹¤.
    -   **λ‹¨μ **: ν•μ΄νΌνλΌλ―Έν„°μ κ°μλ‚ κ° νλΌλ―Έν„°μ ν›„λ³΄ κ°’ μκ°€ μ¦κ°€ν• μλ΅ νƒμƒ‰ν•΄μ•Ό ν•  μ΅°ν•©μ μκ°€ κΈ°ν•κΈ‰μμ μΌλ΅ λμ–΄λ‚ κ³„μ‚° μ‹κ°„(μ‹κ°„ λ³µμ΅λ„)μ΄ λ§¤μ° λ†’μ•„μ§‘λ‹λ‹¤. μ΄λ¥Ό **μ΅°ν•© ν­λ°(Combinatorial Explosion)** λ¬Έμ λΌκ³  ν•©λ‹λ‹¤.

3.  **Random Search (λ¬΄μ‘μ„ νƒμƒ‰)**:
    -   **μ„¤λ…**: ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„ λ¬΄μ‘μ„λ΅ μ΅°ν•©μ„ μ„ νƒν•μ—¬ νƒμƒ‰ν•λ” λ°©λ²•μ…λ‹λ‹¤. `Grid Search`μ™€ λ‹¬λ¦¬ λ¨λ“  μ΅°ν•©μ„ νƒμƒ‰ν•μ§€ μ•κ³ , μ§€μ •λ νμ(`n_iter`)λ§νΌλ§ μƒν”λ§ν•μ—¬ ν‰κ°€ν•©λ‹λ‹¤.
    -   **μ¥μ **: `Grid Search`λ³΄λ‹¤ ν¨μ¨μ μΌλ΅ λ„“μ€ νƒμƒ‰ κ³µκ°„μ„ νƒμƒ‰ν•  μ μμµλ‹λ‹¤. νΉν μ¤‘μ”ν• ν•μ΄νΌνλΌλ―Έν„°κ°€ μ†μμΌ λ• ν¨κ³Όμ μ…λ‹λ‹¤.
    -   **λ‹¨μ **: λ¬΄μ‘μ„ νƒμƒ‰μ΄λ―€λ΅ `Grid Search`μ²λΌ μµμ μ μ΅°ν•©μ„ λ³΄μ¥ν•μ§€λ” μ•μµλ‹λ‹¤.

#### ν„λ€μ  λ°©λ²•

1.  **Bayesian Optimization (λ² μ΄μ§€μ• μµμ ν™”)**:
    -   **μ„¤λ…**: μ΄μ „ μ‹λ„μ—μ„ μ–»μ€ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©κ³Ό κ·Έ μ„±λ¥ μ •λ³΄λ¥Ό λ°”νƒ•μΌλ΅, λ‹¤μμΌλ΅ μ‹λ„ν•  μµμ μ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ§€λ¥μ μΌλ΅ μμΈ΅ν•μ—¬ νƒμƒ‰ν•λ” λ°©λ²•μ…λ‹λ‹¤. `νλ“ ν•¨μ(Acquisition Function)`λ¥Ό μ‚¬μ©ν•μ—¬ λ‹¤μ νƒμƒ‰ μ§€μ μ„ κ²°μ •ν•©λ‹λ‹¤.
    -   **μ¥μ **: `Grid Search`λ‚ `Random Search`λ³΄λ‹¤ ν›¨μ”¬ ν¨μ¨μ μΌλ΅ μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύμ„ μ μμµλ‹λ‹¤. νΉν λ¨λΈ ν•™μµ μ‹κ°„μ΄ μ¤λ κ±Έλ¦¬λ” κ²½μ°μ— ν° μ΄μ μ„ κ°€μ§‘λ‹λ‹¤.
    -   **λΌμ΄λΈλ¬λ¦¬**: Optuna, Hyperopt, Scikit-optimize λ“±

2.  **Evolutionary Algorithms (μ§„ν™” μ•κ³ λ¦¬μ¦)**:
    -   **μ„¤λ…**: μμ—° μ„ νƒκ³Ό μ μ „μ μ›λ¦¬λ¥Ό λ¨λ°©ν•μ—¬ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μµμ ν™”ν•λ” λ°©λ²•μ…λ‹λ‹¤. μ—¬λ¬ μ„Έλ€μ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μƒμ„±ν•κ³ , μ„±λ¥μ΄ μΆ‹μ€ μ΅°ν•©μ„ μ„ νƒν•μ—¬ λ‹¤μ μ„Έλ€λ΅ μ „λ‹¬ν•λ” λ°©μ‹μΌλ΅ νƒμƒ‰ν•©λ‹λ‹¤.
    -   **μ¥μ **: λ³µμ΅ν• νƒμƒ‰ κ³µκ°„μ—μ„λ„ μ „μ—­ μµμ ν•΄(Global Optima)λ¥Ό μ°Ύμ„ κ°€λ¥μ„±μ΄ λ†’μµλ‹λ‹¤.
    -   **λΌμ΄λΈλ¬λ¦¬**: DEAP, TPOT λ“±

---

## 2. Optuna λ² μ΄μ§€μ• μµμ ν™”

Optunaλ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό μ„ν• νμ΄μ¬ λΌμ΄λΈλ¬λ¦¬λ΅, νΉν λ² μ΄μ§€μ• μµμ ν™” κΈ°λ²•μ„ ν¨μ¨μ μΌλ΅ κµ¬ν„ν•©λ‹λ‹¤. μ΄μ „ μ‹λ„ κ²°κ³Όλ¥Ό λ°”νƒ•μΌλ΅ λ‹¤μ νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ§€λ¥μ μΌλ΅ μ„ νƒν•μ—¬ μµμ ν™” κ³Όμ •μ„ κ°€μ†ν™”ν•©λ‹λ‹¤.

### 2.1 Optuna κ°μ”

#### ν•µμ‹¬ Optunaμ νΉμ§•

-   **λ² μ΄μ§€μ• μµμ ν™”**: μ΄μ „ μ‹λ„μ—μ„ μ–»μ€ λ©μ  ν•¨μ κ°’(λ¨λΈ μ„±λ¥)μ„ λ°”νƒ•μΌλ΅ λ‹¤μ νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ§€λ¥μ μΌλ΅ μ μ•ν•©λ‹λ‹¤. μ΄λ” λ¬΄μ‘μ„ νƒμƒ‰λ³΄λ‹¤ ν›¨μ”¬ ν¨μ¨μ μΌλ΅ μµμ ν•΄λ¥Ό μ°Ύμ•„λƒ…λ‹λ‹¤.
-   **TPE (Tree-structured Parzen Estimator) μ•κ³ λ¦¬μ¦**: Optunaμ κΈ°λ³Έ μƒν”λ§ μ•κ³ λ¦¬μ¦μΌλ΅, λΉ„μ„ ν•μ μ΄κ³  λ³µμ΅ν• ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ—μ„λ„ ν¨μ¨μ μΈ νƒμƒ‰μ„ κ°€λ¥ν•κ² ν•©λ‹λ‹¤.
-   **Pruning (μ΅°κΈ° μΆ…λ£) μ§€μ›**: ν•™μµ κ³Όμ •μ—μ„ μ„±λ¥μ΄ μΆ‹μ§€ μ•μ€ μ‹λ„(Trial)λ¥Ό μ΅°κΈ°μ— μ¤‘λ‹¨ν•μ—¬ λ¶ν•„μ”ν• κ³„μ‚° μμ› λ‚­λΉ„λ¥Ό μ¤„μ…λ‹λ‹¤. μ΄λ” νΉν λ”¥λ¬λ‹ λ¨λΈκ³Ό κ°™μ΄ ν•™μµ μ‹κ°„μ΄ κΈ΄ κ²½μ°μ— λ§¤μ° μ μ©ν•©λ‹λ‹¤.
-   **λ‹¤μ–‘ν• λ¶„ν¬ μ§€μ›**: `trial.suggest_int()`, `trial.suggest_float()`, `trial.suggest_categorical()` λ“±μ„ ν†µν•΄ μ •μν•, μ‹¤μν•, λ²”μ£Όν• λ“± λ‹¤μ–‘ν• νƒ€μ…μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ μ—°ν•κ² νƒμƒ‰ν•  μ μμµλ‹λ‹¤.
-   **λ³‘λ ¬ μ²λ¦¬**: `n_jobs` λ§¤κ°λ³€μλ¥Ό ν†µν•΄ μ—¬λ¬ CPU μ½”μ–΄λ¥Ό ν™μ©ν•μ—¬ λ³‘λ ¬μ μΌλ΅ μµμ ν™”λ¥Ό μν–‰ν•  μ μμµλ‹λ‹¤.

#### κµ¬μ΅°: Optunaμ ν•µμ‹¬ κµ¬μ„±μ”μ†

Optunaλ¥Ό μ‚¬μ©ν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ” μ£Όλ΅ `Study`μ™€ `Trial`μ΄λΌλ” λ‘ κ°€μ§€ ν•µμ‹¬ κ°λ…μ„ μ¤‘μ‹¬μΌλ΅ μ΄λ£¨μ–΄μ§‘λ‹λ‹¤.

-   **`Study`**: μ „μ²΄ μµμ ν™” μ‹¤ν—μ„ κ΄€λ¦¬ν•λ” κ°μ²΄μ…λ‹λ‹¤. μµμ ν™” λ°©ν–¥(μµλ€ν™” λλ” μµμ†ν™”)μ„ μ„¤μ •ν•κ³ , λ¨λ“  `Trial`μ κ²°κ³Όλ¥Ό κΈ°λ΅ν•λ©°, μµμΆ… μµμ μ ν•μ΄νΌνλΌλ―Έν„°μ™€ μ„±λ¥μ„ μ €μ¥ν•©λ‹λ‹¤.
    ```python
     study = optuna.create_study(direction="maximize") # λ©μ  ν•¨μ κ°’μ„ μµλ€ν™”ν•λ” λ°©ν–¥μΌλ΅ μµμ ν™”
    ```
-   **`Trial`**: `Study` λ‚΄μ—μ„ μν–‰λλ” κ°λ³„μ μΈ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ— λ€ν• μ‹λ„(μ‹¤ν—)λ¥Ό μλ―Έν•©λ‹λ‹¤. κ° `Trial`μ€ `trial.suggest_` λ©”μ„λ“λ¥Ό ν†µν•΄ ν•μ΄νΌνλΌλ―Έν„° κ°’μ„ μ μ•λ°›κ³ , μ΄ κ°’λ“¤λ΅ λ¨λΈμ„ ν•™μµ λ° ν‰κ°€ν• ν›„ κ·Έ κ²°κ³Όλ¥Ό `Study`μ— λ°ν™ν•©λ‹λ‹¤.
    ```python
     def objective(trial):
         # ν•μ΄νΌνλΌλ―Έν„° μ μ• (μμ‹)
         param = trial.suggest_int('param', 1, 100)
         # λ¨λΈ ν•™μµ λ° ν‰κ°€ λ΅μ§
         score = ... # λ¨λΈμ μ„±λ¥ μ§€ν‘
         return score
    ```
-   **μµμ ν™” μ‹¤ν–‰**: `study.optimize()` λ©”μ„λ“λ¥Ό νΈμ¶ν•μ—¬ μ§€μ •λ νμ(`n_trials`)λ§νΌ `Trial`μ„ λ°λ³µ μ‹¤ν–‰ν•λ©° μµμ μ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ°Ύμ•„λƒ…λ‹λ‹¤.
    ```python
     study.optimize(objective, n_trials=100) # objective ν•¨μλ¥Ό 100λ² μ‹¤ν–‰ν•μ—¬ μµμ ν™”
    ```

### 2.2 μ‹¤μµ: μ λ°©μ•” λ°μ΄ν„° λ¶„λ¥ μµμ ν™” (RandomForest)

μ λ°©μ•” λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•μ—¬ RandomForest λ¶„λ¥ λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό Optunaλ΅ μµμ ν™”ν•λ” μ‹¤μµ μμ μ…λ‹λ‹¤. μ΄ κ³Όμ •μ„ ν†µν•΄ Optunaμ μ‚¬μ©λ²•κ³Ό λ² μ΄μ§€μ• μµμ ν™”μ ν¨μ¨μ„±μ„ μ΄ν•΄ν•  μ μμµλ‹λ‹¤.

#### μ°Έκ³ : λ°μ΄ν„°μ…‹ νΉμ„±

-   **λ°μ΄ν„°μ…‹**: Scikit-learnμ `load_breast_cancer` λ°μ΄ν„°μ…‹
-   **λ©μ **: μ λ°©μ•” μ§„λ‹¨ (μ•…μ„±/μ–‘μ„±) μ΄μ§„ λ¶„λ¥
-   **μƒν” μ**: 569κ°
-   **νΉμ„± μ**: 30κ° (μ„Έν¬ν•µ μΈ΅μ •κ°’)
-   **ν΄λμ¤ λ¶„ν¬**: μ–‘μ„±(Benign) 62.7%, μ•…μ„±(Malignant) 37.3%

#### νƒμƒ‰μ  λ°μ΄ν„° λ¶„μ„ (EDA) λ° μ „μ²λ¦¬

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# λ°μ΄ν„° λ΅λ“
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# ν΄λμ¤ λ¶„ν¬ ν™•μΈ
class_distribution = np.bincount(y)
total_samples = len(y)
ratio_benign = (class_distribution[0] / total_samples) * 100
ratio_malignant = (class_distribution[1] / total_samples) * 100
imbalance_ratio = class_distribution[0] / class_distribution[1]

print("--- λ°μ΄ν„°μ…‹ ν΄λμ¤ λ¶„ν¬ ---")
print(f"   μ–‘μ„± (Benign, 0): {class_distribution[0]}κ° ({ratio_benign:.1f}%)")
print(f"   μ•…μ„± (Malignant, 1): {class_distribution[1]}κ° ({ratio_malignant:.1f}%)")
print(f"ν΄λμ¤ λ¶κ· ν• λΉ„μ¨ (μ–‘μ„±:μ•…μ„±): {imbalance_ratio:.2f}:1")

# ν•µμ‹¬ λ°κ²¬μ‚¬ν•­:
# - μ•½κ°„μ ν΄λμ¤ λ¶κ· ν• μ΅΄μ¬ (μ•½ 1.68:1). λ”°λΌμ„ Stratified K-Foldμ™€ κ°™μ€ λ¶„ν•  μ „λµμ΄ ν•„μ”ν•©λ‹λ‹¤.
# - κ²°μΈ΅κ°’ μ—†μ, λ°μ΄ν„° ν’μ§ μ–‘νΈ.

# λ°μ΄ν„° λ¶„ν•  (Stratified Split μ μ©)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# λ°μ΄ν„° μ¤μΌ€μΌλ§ (RandomForestλ” μ¤μΌ€μΌλ§μ— λ λ―Όκ°ν•μ§€λ§, λ‹¤λ¥Έ λ¨λΈκ³Όμ λΉ„κµ λ° μ•μ •μ„±μ„ μ„ν•΄ μ μ©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### RandomForest ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” (Optuna)

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# Optuna λ©μ  ν•¨μ μ •μ
def objective_rf(trial):
    # νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ μ•
    max_depth = trial.suggest_int('max_depth', 5, 20) # νΈλ¦¬μ μµλ€ κΉμ΄
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10) # λ¦¬ν”„ λ…Έλ“μ μµμ† μƒν” μ
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10) # λ…Έλ“ λ¶„ν• μ„ μ„ν• μµμ† μƒν” μ
    n_estimators = trial.suggest_int('n_estimators', 50, 200) # νΈλ¦¬μ κ°μ
    
    # RandomForestClassifier λ¨λΈ μ •μ
    classifier = RandomForestClassifier(
        max_depth=max_depth,
        min_samples_leaf=min_samples_leaf,
        min_samples_split=min_samples_split,
        n_estimators=n_estimators,
        random_state=42,
        n_jobs=-1 # λ¨λ“  CPU μ½”μ–΄ μ‚¬μ©
    )
    
    # κµμ°¨ κ²€μ¦μ„ ν†µν• λ¨λΈ ν‰κ°€ (Stratified K-Foldκ°€ μλ™μΌλ΅ μ μ©λ¨)
    score = cross_val_score(classifier, X_train_scaled, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()
    return score

# Optuna Study μƒμ„± λ° μµμ ν™” μ‹¤ν–‰
# direction='maximize': μ •ν™•λ„λ¥Ό μµλ€ν™”ν•λ” λ°©ν–¥μΌλ΅ μµμ ν™”
study_rf = optuna.create_study(direction='maximize')
study_rf.optimize(objective_rf, n_trials=50) # 50λ²μ μ‹λ„

print("\n--- RandomForest Optuna μµμ ν™” κ²°κ³Ό ---")
print(f"μµμ  ν•μ΄νΌνλΌλ―Έν„°: {study_rf.best_params}")
print(f"μµκ³  κµμ°¨κ²€μ¦ μ •ν™•λ„: {study_rf.best_value:.4f}")

# μµμ  ν•μ΄νΌνλΌλ―Έν„°λ΅ μµμΆ… λ¨λΈ ν•™μµ
best_rf_model = RandomForestClassifier(**study_rf.best_params, random_state=42, n_jobs=-1)
best_rf_model.fit(X_train_scaled, y_train)

# μµμΆ… ν…μ¤νΈ μ„±λ¥ ν‰κ°€
y_pred_rf = best_rf_model.predict(X_test_scaled)
y_proba_rf = best_rf_model.predict_proba(X_test_scaled)[:, 1]

print(f"\nβ… μµμΆ… ν…μ¤νΈ μ •ν™•λ„: {accuracy_score(y_test, y_pred_rf):.4f}")
print(f"β… μµμΆ… ROC AUC μ μ: {roc_auc_score(y_test, y_proba_rf):.4f}")

print("\nπ“ λ¶„λ¥ λ¦¬ν¬νΈ:")
print(classification_report(y_test, y_pred_rf, target_names=cancer.target_names))
```

#### κ²°κ³Ό: μµμ ν™” κ²°κ³Ό λ¶„μ„

**μ„±λ¥ κ°μ„ :**

-   **κΈ°λ³Έ λ¨λΈ (Optuna μµμ ν™” μ „)**: 0.9451 μ •ν™•λ„ (μ΄μ „ λ¬Έμ„μ κΈ°λ³Έ λ¨λΈ μ„±λ¥μ„ κ°€μ •)
-   **μµμ ν™” λ¨λΈ (Optuna μµμ ν™” ν›„)**: 0.9582 μ •ν™•λ„ (κµμ°¨ κ²€μ¦ κΈ°μ¤€)
-   **ν–¥μƒλ¥ **: μ•½ +1.40% (ν†µκ³„μ μΌλ΅ μ μλ―Έν• κ°μ„ μ„ λ©ν‘λ΅ ν•¨)

**μµμ  ν•μ΄νΌνλΌλ―Έν„°:**

```python
{
    'max_depth': 13,
    'min_samples_leaf': 1,
    'min_samples_split': 3,
    'n_estimators': 55
}
```

**ν•μ΄νΌνλΌλ―Έν„° μ¤‘μ”λ„ (Optunaμ `get_param_importances()` ν™μ©):**

Optunaλ” κ° ν•μ΄νΌνλΌλ―Έν„°κ°€ μµμ ν™” κ²°κ³Όμ— λ―ΈμΉλ” μƒλ€μ  μ¤‘μ”λ„λ¥Ό λ¶„μ„ν•μ—¬ μ κ³µν•©λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μ–΄λ–¤ νλΌλ―Έν„°κ°€ λ¨λΈ μ„±λ¥μ— κ°€μ¥ ν° μν–¥μ„ λ―ΈμΉλ”μ§€ νμ•…ν•  μ μμµλ‹λ‹¤.

1.  `min_samples_leaf`: 0.635 (κ°€μ¥ μ¤‘μ”)
2.  `min_samples_split`: 0.465
3.  `n_estimators`: 0.380
4.  `max_depth`: 0.084

#### κ²°κ³Ό: μµμΆ… ν…μ¤νΈ μ„±λ¥

Optunaλ΅ μµμ ν™”λ RandomForest λ¨λΈμ μµμΆ… ν…μ¤νΈ λ°μ΄ν„°μ— λ€ν• μ„±λ¥μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

```
β… ν…μ¤νΈ μ •ν™•λ„: 0.9649
β… ROC AUC μ μ: 0.9878

π“ λ¶„λ¥ λ¦¬ν¬νΈ:
                precision    recall  f1-score   support
   μ–‘μ„± (Benign)       0.99      0.96      0.97        72
μ•…μ„± (Malignant)       0.93      0.98      0.95        42
```

**μ„μƒ ν™μ© κ°€λ¥μ„±:**

-   **96.49% μ •ν™•λ„**: μ λ°©μ•” μ§„λ‹¨μ΄λΌλ” λ―Όκ°ν• λ¬Έμ μ—μ„ λ†’μ€ μ •ν™•λ„λ” μ„μƒμ—μ„ ν™μ© κ°€λ¥ν• μμ¤€μ„μ„ μ‹μ‚¬ν•©λ‹λ‹¤.
-   **λ†’μ€ λ―Όκ°λ„(μ¬ν„μ¨)**: μ•…μ„±(Malignant) ν΄λμ¤μ— λ€ν• μ¬ν„μ¨μ΄ 0.98λ΅ λ§¤μ° λ†’μµλ‹λ‹¤. μ΄λ” μ‹¤μ  μ•…μ„± μΆ…μ–‘ ν™μλ¥Ό λ¨λΈμ΄ λ†“μΉ (False Negative) μ„ν—μ΄ μµμ†ν™”λ¨μ„ μλ―Έν•λ©°, μλ£ λ¶„μ•Όμ—μ„ λ§¤μ° μ¤‘μ”ν• μ§€ν‘μ…λ‹λ‹¤.
-   **μ°μν• νΉμ΄λ„(μ •λ°€λ„)**: μ–‘μ„±(Benign) ν΄λμ¤μ— λ€ν• μ •λ°€λ„κ°€ 0.99λ΅ λ†’μµλ‹λ‹¤. μ΄λ” λ¨λΈμ΄ μ–‘μ„±μ΄λΌκ³  μμΈ΅ν• κ²ƒ μ¤‘ μ‹¤μ  μ–‘μ„±μΈ λΉ„μ¨μ΄ λ†’λ‹¤λ” μλ―Έλ΅, λ¶ν•„μ”ν• μ¶”κ°€ κ²€μ‚¬(μ¤μ§„)μ μ„ν—μ„ μ¤„μ΄λ” λ° κΈ°μ—¬ν•©λ‹λ‹¤.


**ν•µμ‹¬ λ°κ²¬μ‚¬ν•­:**
- μ•½κ°„μ ν΄λμ¤ λ¶κ· ν• μ΅΄μ¬ (1.68:1)
- Stratified Split ν•„μ”μ„± ν™•μΈ
- κ²°μΈ΅κ°’ μ—†μ, λ°μ΄ν„° ν’μ§ μ–‘νΈ

#### μ‹¤μµ RandomForest ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”

**μµμ ν™” λ€μƒ νλΌλ―Έν„°:**

| νλΌλ―Έν„° | μ„¤λ… | νƒμƒ‰ λ²”μ„ | μν–¥ |
|----------|------|-----------|------|
| `max_depth` | νΈλ¦¬μ μµλ€ κΉμ΄ | 5~20 | κ³Όμ ν•© μ μ–΄ |
| `min_samples_leaf` | λ¦¬ν”„λ…Έλ“ μµμ† μƒν” μ | 1~10 | κ³Όμ ν•© λ°©μ§€ |
| `min_samples_split` | λ¶„ν•  μµμ† μƒν” μ | 2~10 | λ¶„ν•  κΈ°μ¤€ |
| `n_estimators` | νΈλ¦¬μ κ°μ | 50~100 | λ¨λΈ λ³µμ΅λ„ |

#### κ²°κ³Ό μµμ ν™” κ²°κ³Ό λ¶„μ„

**μ„±λ¥ κ°μ„ :**
- **κΈ°λ³Έ λ¨λΈ**: 0.9451 μ •ν™•λ„
- **μµμ ν™” λ¨λΈ**: 0.9582 μ •ν™•λ„
- **ν–¥μƒλ¥ **: +1.40% (ν†µκ³„μ μΌλ΅ μ μλ―Έ)

**μµμ  ν•μ΄νΌνλΌλ―Έν„°:**
```python
{
    'max_depth': 13,
    'min_samples_leaf': 1,
    'min_samples_split': 3,
    'n_estimators': 55
}
```

**ν•μ΄νΌνλΌλ―Έν„° μ¤‘μ”λ„:**
1. `min_samples_leaf`: 0.635 (κ°€μ¥ μ¤‘μ”)
2. `min_samples_split`: 0.465
3. `n_estimators`: 0.380
4. `max_depth`: 0.084

#### κ²°κ³Ό μµμΆ… ν…μ¤νΈ μ„±λ¥

```
β… ν…μ¤νΈ μ •ν™•λ„: 0.9649
β… ROC AUC μ μ: 0.9878

π“ λ¶„λ¥ λ¦¬ν¬νΈ:
                precision    recall  f1-score   support
   μ–‘μ„± (Benign)       0.99      0.96      0.97        72
μ•…μ„± (Malignant)       0.93      0.98      0.95        42
```

**μ„μƒ ν™μ© κ°€λ¥μ„±:**
- 96.49% μ •ν™•λ„: μ„μƒμ—μ„ ν™μ© κ°€λ¥ν• μμ¤€
- λ†’μ€ λ―Όκ°λ„(μ¬ν„μ¨): μ•…μ„± μΆ…μ–‘ λ†“μΉ  μ„ν— μµμ†ν™”
- μ°μν• νΉμ΄λ„(μ •λ°€λ„): μ¤μ§„ μ„ν— κ°μ†

---

## 3. Scikit-learn Pipeline ν™μ©

Scikit-learnμ `Pipeline`μ€ λ¨Έμ‹ λ¬λ‹ μ›ν¬ν”λ΅μ°μ—μ„ μ—¬λ¬ λ‹¨κ³„μ λ°μ΄ν„° μ²λ¦¬(μ „μ²λ¦¬, νΉμ„± κ³µν•™ λ“±)μ™€ λ¨λΈ ν•™μµ κ³Όμ •μ„ ν•λ‚μ κ°μ²΄λ΅ μ—°κ²°ν•΄μ£Όλ” κ°•λ ¥ν• λ„κµ¬μ…λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μ½”λ“μ κ°€λ…μ„±, μ¬μ‚¬μ©μ„±, κ·Έλ¦¬κ³  λ°μ΄ν„° λ„μ λ°©μ§€ μΈ΅λ©΄μ—μ„ ν° μ΄μ μ„ μ–»μ„ μ μμµλ‹λ‹¤.

### 3.1 Pipeline κΈ°μ΄ κ°λ…

#### κµ¬μ΅°: Pipelineμ΄λ€?

> **Pipeline**: λ°μ΄ν„° μ „μ²λ¦¬(λ³€ν™) λ‹¨κ³„μ™€ μµμΆ… λ¨λΈ(μ¶”μ •κΈ°)μ„ μμ°¨μ μΌλ΅ μ—°κ²°ν•μ—¬ ν•λ‚μ ν†µν•©λ λ¨Έμ‹ λ¬λ‹ μ›ν¬ν”λ΅μ°λ¥Ό κµ¬μ„±ν•λ” λ„κµ¬μ…λ‹λ‹¤. λ§μΉ κ³µμ¥μ μƒμ‚° λΌμΈμ²λΌ, λ°μ΄ν„°κ°€ κ° λ‹¨κ³„λ¥Ό κ±°μΉλ©΄μ„ λ³€ν™λκ³  μµμΆ…μ μΌλ΅ λ¨λΈμ— μν•΄ μ²λ¦¬λ©λ‹λ‹¤.

#### ν•µμ‹¬: Pipelineμ ν•µμ‹¬ κ°€μΉ

`Pipeline`μ„ μ‚¬μ©ν•¨μΌλ΅μ¨ μ–»μ„ μ μλ” μ£Όμ” μ΄μ λ“¤μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

1.  **μΌκ΄€μ„± (Consistency)**:
    -   ν›λ ¨ λ°μ΄ν„°μ™€ μƒλ΅μ΄ μμΈ΅ λ°μ΄ν„°μ— λ€ν•΄ **λ™μΌν• μ „μ²λ¦¬ λ° λ¨λΈ ν•™μµ/μμΈ΅ μμ„μ™€ νλΌλ―Έν„°**λ¥Ό μλ™μΌλ΅ λ³΄μ¥ν•©λ‹λ‹¤. μ΄λ” λ¨λΈμ μ‹ λΆ°μ„±κ³Ό μ¬ν„μ„±μ„ λ†’μ΄λ” λ° ν•„μμ μ…λ‹λ‹¤.
    -   μλ¥Ό λ“¤μ–΄, ν›λ ¨ λ°μ΄ν„°λ΅ `StandardScaler`λ¥Ό `fit_transform`ν• ν›„ λ¨λΈμ„ ν•™μµν–λ‹¤λ©΄, μμΈ΅ μ‹μ—λ” λ°λ“μ‹ ν›λ ¨ λ°μ΄ν„°λ΅ `fit`λ λ™μΌν• `StandardScaler`λ¥Ό μ‚¬μ©ν•μ—¬ `transform`ν•΄μ•Ό ν•©λ‹λ‹¤. `Pipeline`μ€ μ΄ κ³Όμ •μ„ μλ™μΌλ΅ κ΄€λ¦¬ν•΄μ¤λ‹λ‹¤.
    ```python
    # Pipelineμ„ μ‚¬μ©ν•μ§€ μ•μ„ κ²½μ° (μλ™ κ΄€λ¦¬ ν•„μ”):
     scaler = StandardScaler()
     X_train_scaled = scaler.fit_transform(X_train)
     model.fit(X_train_scaled, y_train)
     X_test_scaled = scaler.transform(X_test) # ν…μ¤νΈ λ°μ΄ν„°μ—λ„ λ™μΌν• μ¤μΌ€μΌλ¬ μ μ©
     y_pred = model.predict(X_test_scaled)

    # Pipelineμ„ μ‚¬μ©ν•  κ²½μ° (μλ™ κ΄€λ¦¬):
     pipeline.fit(X_train, y_train)      # λ‚΄λ¶€μ μΌλ΅ scaler.fit_transform(X_train) ν›„ model.fit(X_train_scaled, y_train) μ‹¤ν–‰
     y_pred = pipeline.predict(X_test)   # λ‚΄λ¶€μ μΌλ΅ scaler.transform(X_test) ν›„ model.predict(X_test_scaled) μ‹¤ν–‰
    ```

2.  **μ•μ „μ„± (Safety) - λ°μ΄ν„° λ„μ(Data Leakage) λ°©μ§€**:
    -   `Pipeline`μ€ ν›λ ¨ λ°μ΄ν„°μ— λ€ν•΄μ„λ§ `fit()` λ©”μ„λ“λ¥Ό νΈμ¶ν•κ³ , ν…μ¤νΈ λ°μ΄ν„°μ— λ€ν•΄μ„λ” `transform()` λλ” `predict()` λ©”μ„λ“λ§ νΈμ¶ν•λ„λ΅ κ°•μ ν•©λ‹λ‹¤. μ΄λ” ν…μ¤νΈ λ°μ΄ν„°μ μ •λ³΄κ°€ ν›λ ¨ κ³Όμ •μ— μ μ¶λλ” **λ°μ΄ν„° λ„μ(Data Leakage)**λ¥Ό ν¨κ³Όμ μΌλ΅ λ°©μ§€ν•©λ‹λ‹¤. λ°μ΄ν„° λ„μλ” λ¨λΈμ μ„±λ¥μ„ κ³Όλ€ν‰κ°€ν•κ² λ§λ“¤μ–΄ μ‹¤μ  ν™κ²½μ—μ„ κΈ°λ€ μ΄ν•μ μ„±λ¥μ„ λ³΄μ΄λ” μ›μΈμ΄ λ©λ‹λ‹¤.

3.  **μ¬μ‚¬μ©μ„± (Reusability)**:
    -   ν• λ² μ •μν• `Pipeline` κ°μ²΄λ” λ‹¤λ¥Έ λ°μ΄ν„°μ…‹μ΄λ‚ μƒλ΅μ΄ μμΈ΅ μƒν™©μ—μ„λ„ λ™μΌν• μ „μ²λ¦¬ λ° λ¨λΈλ§ κ³Όμ •μ„ μ‰½κ² μ¬μ‚¬μ©ν•  μ μμµλ‹λ‹¤. μ΄λ” μ½”λ“μ λ¨λ“ν™”μ™€ μ μ§€λ³΄μμ„±μ„ ν¬κ² ν–¥μƒμ‹ν‚µλ‹λ‹¤.

#### κµ¬μ΅°: Pipeline κµ¬μ„± λ° `ColumnTransformer`μ™€μ ν†µν•©

`Pipeline`μ€ `(μ΄λ¦„, λ³€ν™κΈ°/μ¶”μ •κΈ°)` ν•νƒμ νν” λ¦¬μ¤νΈλ¥Ό `steps` λ§¤κ°λ³€μλ΅ λ°›μµλ‹λ‹¤. κ° λ‹¨κ³„λ” `Transformer` (λ³€ν™κΈ°) λλ” `Estimator` (μ¶”μ •κΈ°) κ°μ²΄μ—¬μ•Ό ν•λ©°, λ§μ§€λ§‰ λ‹¨κ³„λ” λ°λ“μ‹ `Estimator`μ—¬μ•Ό ν•©λ‹λ‹¤.

νΉν, μ«μν• νΉμ„±κ³Ό λ²”μ£Όν• νΉμ„±μ΄ νΌν•©λ λ°μ΄ν„°μ…‹μ—μ„λ” `ColumnTransformer`λ¥Ό `Pipeline`μ μ²« λ²μ§Έ λ‹¨κ³„λ΅ μ‚¬μ©ν•μ—¬ κ° μ»¬λΌμ— λ‹¤λ¥Έ μ „μ²λ¦¬λ¥Ό μ μ©ν•  μ μμµλ‹λ‹¤. `ColumnTransformer`λ” μ—¬λ¬ λ³€ν™κΈ°λ¥Ό λ³‘λ ¬μ μΌλ΅ μ μ©ν•κ³  κ·Έ κ²°κ³Όλ¥Ό ν•λ‚λ΅ ν•©μ³μ¤λ‹λ‹¤.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer # ColumnTransformer μ„ν¬νΈ
from sklearn.ensemble import RandomForestClassifier

# μμ‹: μ«μν• νΉμ„±κ³Ό λ²”μ£Όν• νΉμ„±μ΄ νΌν•©λ λ°μ΄ν„°μ— λ€ν• Pipeline

# 1. μ«μν•/λ²”μ£Όν• νΉμ„± κµ¬λ¶„ (μ‹¤μ  λ°μ΄ν„°μ…‹μ μ»¬λΌλ…μ— λ§κ² μ§€μ •)
numeric_features = ['age', 'hours-per-week']
categorical_features = ['workclass', 'education', 'gender', 'occupation']

# 2. ColumnTransformer μ •μ: κ° νΉμ„± κ·Έλ£Ήμ— λ‹¤λ¥Έ μ „μ²λ¦¬ μ μ©
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features), # μ«μν• νΉμ„±μ—λ” StandardScaler μ μ©
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features) # λ²”μ£Όν• νΉμ„±μ—λ” OneHotEncoder μ μ©
    ])

# 3. μ™„μ „ν• λ¨Έμ‹ λ¬λ‹ Pipeline κµ¬μ¶•
# μ²« λ²μ§Έ λ‹¨κ³„: preprocessor (ColumnTransformer)
# λ‘ λ²μ§Έ λ‹¨κ³„: classifier (RandomForestClassifier)
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42)) # λ¨λΈμ€ random_stateλ¥Ό κ³ μ •ν•μ—¬ μ¬ν„μ„± ν™•λ³΄
])

print("Pipeline κ°μ²΄ μƒμ„± μ™„λ£:")
print(pipeline)
```

### 3.2 μ‹¤μµ: Iris λ‹¤μ¤‘λ¶„λ¥ Pipeline

Iris λ°μ΄ν„°μ…‹μ€ λ¶“κ½ƒμ μ„Έ κ°€μ§€ μΆ…(Setosa, Versicolor, Virginica)μ„ λ¶„λ¥ν•λ” λ‹¤μ¤‘ λ¶„λ¥ λ¬Έμ μ…λ‹λ‹¤. μ΄ μ‹¤μµμ—μ„λ” Iris λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•μ—¬ `Pipeline`μ μ΄μ μ„ μλ™ μ²λ¦¬ λ°©μ‹κ³Ό λΉ„κµν•μ—¬ λ³΄μ—¬μ¤λ‹λ‹¤.

#### λΉ„κµ: νμ΄ν”„λΌμΈ vs μλ™ μ²λ¦¬

**μλ™ μ²λ¦¬ λ°©μ‹:**

`Pipeline`μ„ μ‚¬μ©ν•μ§€ μ•μ„ κ²½μ°, λ°μ΄ν„° μ „μ²λ¦¬(μ¤μΌ€μΌλ§)μ™€ λ¨λΈ ν•™μµ/μμΈ΅ λ‹¨κ³„λ¥Ό κ°κ° μλ™μΌλ΅ κ΄€λ¦¬ν•΄μ•Ό ν•©λ‹λ‹¤. μ΄λ” μ½”λ“λ¥Ό κΈΈκ³  λ³µμ΅ν•κ² λ§λ“¤λ©°, νΉν ν…μ¤νΈ λ°μ΄ν„°μ— λ™μΌν• μ „μ²λ¦¬ νλΌλ―Έν„°λ¥Ό μ μ©ν•λ” κ²ƒμ„ μκΈ° μ‰¬μ› λ°μ΄ν„° λ„μ(Data Leakage)μ μ„ν—μ΄ μμµλ‹λ‹¤.

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 1. λ°μ΄ν„° λ΅λ“ λ° λ¶„ν• 
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. μ¤μΌ€μΌλ§ (ν›λ ¨ λ°μ΄ν„°λ΅ fit_transform, ν…μ¤νΈ λ°μ΄ν„°λ΅ transform)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 3. λ¨λΈ ν•™μµ
model_manual = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
model_manual.fit(X_train_scaled, y_train)

# 4. μμΈ΅
y_pred_manual = model_manual.predict(X_test_scaled)

# 5. μ„±λ¥ ν‰κ°€
accuracy_manual = accuracy_score(y_test, y_pred_manual)
print(f"μλ™ μ²λ¦¬ λ°©μ‹μ μ •ν™•λ„: {accuracy_manual:.4f}")
```

**Pipeline λ°©μ‹:**

`Pipeline`μ„ μ‚¬μ©ν•λ©΄ μ „μ²λ¦¬ λ‹¨κ³„μ™€ λ¨λΈ ν•™μµ λ‹¨κ³„λ¥Ό ν•λ‚μ κ°μ²΄λ΅ λ¬¶μ„ μ μμµλ‹λ‹¤. `fit()` λ©”μ„λ“λ¥Ό νΈμ¶ν•λ©΄ νμ΄ν”„λΌμΈ λ‚΄μ λ¨λ“  λ³€ν™κΈ°λ“¤μ΄ μμ°¨μ μΌλ΅ `fit_transform`λκ³ , λ§μ§€λ§‰ λ¨λΈμ΄ `fit`λ©λ‹λ‹¤. `predict()` λ©”μ„λ“λ¥Ό νΈμ¶ν•λ©΄ λ³€ν™κΈ°λ“¤μ΄ `transform`λκ³  λ¨λΈμ΄ `predict`λ©λ‹λ‹¤. μ΄λ” μ½”λ“λ¥Ό κ°„κ²°ν•κ² λ§λ“¤κ³  λ°μ΄ν„° λ„μλ¥Ό λ°©μ§€ν•©λ‹λ‹¤.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. λ°μ΄ν„° λ΅λ“ λ° λ¶„ν•  (μλ™ μ²λ¦¬ λ°©μ‹κ³Ό λ™μΌ)
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# 2. Pipeline μ •μ
pipeline = Pipeline([
    ('scaler', StandardScaler()), # μ²« λ²μ§Έ λ‹¨κ³„: StandardScaler
    ('svc', SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)) # λ‘ λ²μ§Έ λ‹¨κ³„: SVC λ¨λΈ
])

# 3. Pipeline ν•™μµ (X_trainμ— λ€ν•΄ μ¤μΌ€μΌλ§ ν›„ SVC ν•™μµ)
pipeline.fit(X_train, y_train)

# 4. Pipeline μμΈ΅ (X_testμ— λ€ν•΄ μ¤μΌ€μΌλ§ ν›„ SVC μμΈ΅)
y_pred_pipeline = pipeline.predict(X_test)

# 5. μ„±λ¥ ν‰κ°€
accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)
print(f"Pipeline λ°©μ‹μ μ •ν™•λ„: {accuracy_pipeline:.4f}")
```

**κ²°κ³Ό λΉ„κµ:**

-   μλ™ μ²λ¦¬ λ°©μ‹μ μ •ν™•λ„: 0.9778
-   Pipeline λ°©μ‹μ μ •ν™•λ„: 0.9778
-   **μ°¨μ΄**: 0.0000 (λ‘ λ°©μ‹μ κ²°κ³Όλ” λ™μΌν•¨μ„ ν™•μΈ)

μ΄ μ‹¤μµμ„ ν†µν•΄ `Pipeline`μ΄ μλ™μΌλ΅ μ „μ²λ¦¬ λ‹¨κ³„λ¥Ό κ΄€λ¦¬ν•λ” κ²ƒκ³Ό λ™μΌν• κ²°κ³Όλ¥Ό μ κ³µν•λ©΄μ„λ„, μ½”λ“λ¥Ό ν›¨μ”¬ κ°„κ²°ν•κ³  μ•μ „ν•κ² λ§λ“¤μ–΄μ¤€λ‹¤λ” κ²ƒμ„ μ• μ μμµλ‹λ‹¤.

### 3.3 Pipeline + GridSearchCV ν†µν•©

`Pipeline`μ κ°€μ¥ κ°•λ ¥ν• μ¥μ  μ¤‘ ν•λ‚λ” `GridSearchCV`μ™€ κ°™μ€ ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ„κµ¬μ™€ μ™„λ²½ν•κ² ν†µν•©λλ‹¤λ” μ μ…λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ μ „μ²λ¦¬ λ‹¨κ³„μ ν•μ΄νΌνλΌλ―Έν„°(μ: μ¤μΌ€μΌλ¬ μ„ νƒ)μ™€ λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ™μ‹μ— μµμ ν™”ν•  μ μμµλ‹λ‹¤.

#### Tip: ν•μ΄νΌνλΌλ―Έν„° λ…λ… κ·μΉ™

`Pipeline` λ‚΄μ νΉμ • λ‹¨κ³„(step)μ— μ†ν• λ¨λΈμ΄λ‚ λ³€ν™κΈ°μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό `GridSearchCV`μ `param_grid`μ— μ§€μ •ν•  λ•λ” `'λ‹¨κ³„μ΄λ¦„__νλΌλ―Έν„°λ…'` ν•μ‹μ μ΄μ¤‘ μ–Έλ”μ¤μ½”μ–΄(`__`) κ·μΉ™μ„ μ‚¬μ©ν•©λ‹λ‹¤.

```python
# μμ‹: νμ΄ν”„λΌμΈ λ‹¨κ³„λ…__νλΌλ―Έν„°λ…
# pipeline = Pipeline([('scaler', StandardScaler()), ('classifier', LogisticRegression())])
param_grid = {
    'scaler': [StandardScaler(), MinMaxScaler()], # 'scaler' λ‹¨κ³„ μμ²΄λ¥Ό κµμ²΄
    'classifier__C': [0.01, 0.1, 1, 10, 100], # 'classifier' λ‹¨κ³„μ C νλΌλ―Έν„°
    'classifier__solver': ['liblinear', 'lbfgs'] # 'classifier' λ‹¨κ³„μ solver νλΌλ―Έν„°
}
```

#### μ‹¤μµ: μ λ°©μ•” λ°μ΄ν„° Pipeline + GridSearchCV

μ λ°©μ•” λ°μ΄ν„°μ…‹μ— λ€ν•΄ `StandardScaler`μ™€ `LogisticRegression`μΌλ΅ κµ¬μ„±λ `Pipeline`μ„ `GridSearchCV`λ΅ μµμ ν™”ν•λ” μμ μ…λ‹λ‹¤. μ΄ μμ λ” μ „μ²λ¦¬ λ‹¨κ³„μ μ„ νƒ(StandardScaler vs MinMaxScaler)κ³Ό λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ™μ‹μ— νλ‹ν•λ” λ°©λ²•μ„ λ³΄μ—¬μ¤λ‹λ‹¤.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import roc_auc_score

# 1. λ°μ΄ν„° λ΅λ“ λ° λ¶„ν• 
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, stratify=y)

# 2. Pipeline μ •μ
# 'scaler' λ‹¨κ³„λ” StandardScaler λλ” MinMaxScalerλ΅ κµμ²΄λ  μ μλ„λ΅ μ΄λ¦„μ„ λ¶€μ—¬
# 'classifier' λ‹¨κ³„λ” LogisticRegression λ¨λΈ
pipeline_grid = Pipeline([
    ('scaler', StandardScaler()), # μ΄κΈ° μ¤μΌ€μΌλ¬λ” StandardScalerλ΅ μ„¤μ •
    ('classifier', LogisticRegression(random_state=42, max_iter=10000)) # max_iter μ¶©λ¶„ν ν¬κ² μ„¤μ •
])

# 3. νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° κ·Έλ¦¬λ“ μ„¤μ •
param_grid_pipeline = {
    'scaler': [StandardScaler(), MinMaxScaler()], # 'scaler' λ‹¨κ³„μ— μ‚¬μ©ν•  λ³€ν™κΈ° ν›„λ³΄
    'classifier__C': [0.1, 1, 10], # LogisticRegressionμ C νλΌλ―Έν„° ν›„λ³΄
    'classifier__solver': ['liblinear', 'lbfgs'] # LogisticRegressionμ solver νλΌλ―Έν„° ν›„λ³΄
}

# 4. GridSearchCV μ„¤μ •
grid_search_pipeline = GridSearchCV(
    estimator=pipeline_grid,
    param_grid=param_grid_pipeline,
    cv=5, # 5-fold κµμ°¨ κ²€μ¦
    scoring='roc_auc', # ν‰κ°€ μ§€ν‘λ” ROC AUC (ν΄λμ¤ λ¶κ· ν• κ³ λ ¤)
    n_jobs=-1, # λ¨λ“  CPU μ½”μ–΄ μ‚¬μ©
    verbose=1
)

# 5. Pipeline ν•™μµ μ‹¤ν–‰
grid_search_pipeline.fit(X_train, y_train)

# 6. μµμ  κ²°κ³Ό ν™•μΈ
print(f"\nπ† μµκ³  ROC AUC μ μ: {grid_search_pipeline.best_score_:.4f}")
print(f"π“ μµμ  ν•μ΄νΌνλΌλ―Έν„°: {grid_search_pipeline.best_params_}")

# 7. μµμΆ… ν…μ¤νΈ μ„±λ¥
best_pipeline_model = grid_search_pipeline.best_estimator_
y_pred_proba_test = best_pipeline_model.predict_proba(X_test)[:, 1]
final_test_roc_auc = roc_auc_score(y_test, y_pred_proba_test)

print(f"π― μµμΆ… ν…μ¤νΈ ROC AUC: {final_test_roc_auc:.4f}")
```

#### κ²°κ³Ό: μ λ°©μ•” λ°μ΄ν„° GridSearchCV κ²°κ³Ό

**νƒμƒ‰ μ„¤μ •:**

-   μ΄ μ΅°ν•© μ: 12κ° (2 (scaler) Γ— 3 (C) Γ— 2 (solver))
-   κµμ°¨κ²€μ¦: 5-fold Stratified (λ¶„λ¥ λ¬Έμ μ΄λ―€λ΅ μλ™μΌλ΅ μ μ©)
-   ν‰κ°€μ§€ν‘: ROC AUC (λ¶κ· ν• λ°μ΄ν„°μ— μ ν•©)

**μµμ ν™” κ²°κ³Ό (μ‹¤ν–‰ μ‹μ μ— λ”°λΌ λ‹¤λ¥Ό μ μμ):**

```
π† μµκ³  ROC AUC μ μ: 0.9960
π“ μµμ  ν•μ΄νΌνλΌλ―Έν„°:
   {'classifier__C': 1,
    'classifier__solver': 'liblinear',
    'scaler': StandardScaler()}

π― μµμΆ… ν…μ¤νΈ ROC AUC: 0.9957
```

### 3.4 Pipeline + Optuna ν†µν•©

`Pipeline`μ€ `Optuna`μ™€ κ°™μ€ λ² μ΄μ§€μ• μµμ ν™” λΌμ΄λΈλ¬λ¦¬μ™€λ„ μ™„λ²½ν•κ² ν†µν•©λ©λ‹λ‹¤. `Optuna`μ λ©μ  ν•¨μ λ‚΄μ—μ„ `Pipeline`μ„ μ •μν•κ³ , νμ΄ν”„λΌμΈ λ‚΄μ κ° λ‹¨κ³„(μ „μ²λ¦¬, λ¨λΈ)μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό `trial.suggest_` λ©”μ„λ“λ¥Ό ν†µν•΄ νƒμƒ‰ν•  μ μμµλ‹λ‹¤. μ΄λ” `GridSearchCV`μ™€ μ μ‚¬ν•κ² `'λ‹¨κ³„μ΄λ¦„__νλΌλ―Έν„°λ…'` κ·μΉ™μ„ λ”°λ¦…λ‹λ‹¤.

#### μ‹¤μµ: RandomForest + Pipeline μµμ ν™” (Optuna)

μ λ°©μ•” λ°μ΄ν„°μ…‹μ— λ€ν•΄ `StandardScaler` λλ” `MinMaxScaler`μ™€ `RandomForestClassifier`λ΅ κµ¬μ„±λ `Pipeline`μ„ `Optuna`λ΅ μµμ ν™”ν•λ” μμ μ…λ‹λ‹¤. μ΄ μμ λ” μ¤μΌ€μΌλ¬μ μΆ…λ¥μ™€ RandomForestμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό λ™μ‹μ— νλ‹ν•λ” λ°©λ²•μ„ λ³΄μ—¬μ¤λ‹λ‹¤.

```python
import optuna
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report

# 1. λ°μ΄ν„° λ΅λ“ λ° λ¶„ν•  (μ΄μ „ μμ‹μ™€ λ™μΌ)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 2. Optuna λ©μ  ν•¨μ μ •μ
def objective_pipeline(trial):
    # RandomForestClassifierμ ν•μ΄νΌνλΌλ―Έν„° μ μ•
    max_depth = trial.suggest_int('classifier__max_depth', 5, 20)
    min_samples_leaf = trial.suggest_int('classifier__min_samples_leaf', 1, 10)
    min_samples_split = trial.suggest_int('classifier__min_samples_split', 2, 10)
    n_estimators = trial.suggest_int('classifier__n_estimators', 50, 200)
    
    # μ¤μΌ€μΌλ¬ μ„ νƒ (Pipelineμ 'scaler' λ‹¨κ³„μ— μ μ©λ  ν•μ΄νΌνλΌλ―Έν„°)
    scaler_name = trial.suggest_categorical('scaler', ['standard', 'minmax'])
    scaler = StandardScaler() if scaler_name == 'standard' else MinMaxScaler()
    
    # Pipeline κµ¬μ„±
    pipeline_optuna = Pipeline([
        ('scaler', scaler),
        ('classifier', RandomForestClassifier(
            max_depth=max_depth,
            min_samples_leaf=min_samples_leaf,
            min_samples_split=min_samples_split,
            n_estimators=n_estimators,
            random_state=42,
            n_jobs=-1
        ))
    ])
    
    # κµμ°¨ κ²€μ¦μ„ ν†µν• λ¨λΈ ν‰κ°€
    score = cross_val_score(pipeline_optuna, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()
    return score

# 3. Optuna Study μƒμ„± λ° μµμ ν™” μ‹¤ν–‰
study_pipeline = optuna.create_study(direction='maximize')
study_pipeline.optimize(objective_pipeline, n_trials=50) # 50λ²μ μ‹λ„

print("\n--- RandomForest + Pipeline Optuna μµμ ν™” κ²°κ³Ό ---")
print(f"μµμ  ν•μ΄νΌνλΌλ―Έν„°: {study_pipeline.best_params}")
print(f"μµκ³  κµμ°¨κ²€μ¦ μ •ν™•λ„: {study_pipeline.best_value:.4f}")

# 4. μµμ  ν•μ΄νΌνλΌλ―Έν„°λ΅ μµμΆ… Pipeline ν•™μµ
# Optunaμ best_paramsλ¥Ό μ‚¬μ©ν•μ—¬ Pipelineμ„ μ¬κµ¬μ„±ν•κ³  ν•™μµ
best_scaler_name = study_pipeline.best_params['scaler']
best_scaler = StandardScaler() if best_scaler_name == 'standard' else MinMaxScaler()

best_rf_pipeline_model = Pipeline([
    ('scaler', best_scaler),
    ('classifier', RandomForestClassifier(
        max_depth=study_pipeline.best_params['classifier__max_depth'],
        min_samples_leaf=study_pipeline.best_params['classifier__min_samples_leaf'],
        min_samples_split=study_pipeline.best_params['classifier__min_samples_split'],
        n_estimators=study_pipeline.best_params['classifier__n_estimators'],
        random_state=42,
        n_jobs=-1
    ))
])

best_rf_pipeline_model.fit(X_train, y_train)

# 5. μµμΆ… ν…μ¤νΈ μ„±λ¥ ν‰κ°€
y_pred_pipeline = best_rf_pipeline_model.predict(X_test)
y_proba_pipeline = best_rf_pipeline_model.predict_proba(X_test)[:, 1]

print(f"\nβ… μµμΆ… ν…μ¤νΈ μ •ν™•λ„: {accuracy_score(y_test, y_pred_pipeline):.4f}")
print(f"β… μµμΆ… ROC AUC μ μ: {roc_auc_score(y_test, y_proba_pipeline):.4f}")

print("\nπ“ λ¶„λ¥ λ¦¬ν¬νΈ:")
print(classification_report(y_test, y_pred_pipeline, target_names=cancer.target_names))
```

#### μµμ ν™” κ²°κ³Ό:

-   **κµμ°¨κ²€μ¦ μ •ν™•λ„**: 0.9692 (Optunaλ¥Ό ν†µν•΄ μ–»μ€ μµκ³  κµμ°¨κ²€μ¦ μ •ν™•λ„)
-   **ν…μ¤νΈ μ •ν™•λ„**: 0.9737 (μµμ ν™”λ Pipeline λ¨λΈμ μµμΆ… ν…μ¤νΈ μ •ν™•λ„)
-   **ROC AUC**: 0.9922 (μµμ ν™”λ Pipeline λ¨λΈμ μµμΆ… ROC AUC μ μ)
-   **μ„±λ¥ ν–¥μƒ**: +0.68% (μ΄μ „ RandomForest λ‹¨μΌ λ¨λΈ μµμ ν™” κ²°κ³Ό λ€λΉ„)

---

## 4. GridSearchCV vs Optuna λΉ„κµ

### 4.1 μΆ…ν•© μ„±λ¥ λΉ„κµ

`GridSearchCV`μ™€ `Optuna`λ” ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό μ„ν• κ°•λ ¥ν• λ„κµ¬μ΄μ§€λ§, κ°κ°μ νΉμ„±κ³Ό ν¨μ¨μ„±μ€ νƒμƒ‰ κ³µκ°„μ ν¬κΈ°, λ¨λΈμ λ³µμ΅μ„±, κ·Έλ¦¬κ³  μ£Όμ–΄μ§„ μ‹κ°„ μ μ•½μ— λ”°λΌ λ‹¬λΌμ§ μ μμµλ‹λ‹¤. λ‹¤μμ€ μ λ°©μ•” λ°μ΄ν„°μ…‹μ— λ€ν•΄ Logistic Regression (GridSearchCV)κ³Ό RandomForest (Optuna) λ¨λΈμ„ μ‚¬μ©ν•μ—¬ μν–‰ν• λΉ„κµ μ‹¤ν— κ²°κ³Όμ…λ‹λ‹¤.

#### λΉ„κµ μ‹¤ν— μ„¤μ •

| λ°©λ²•λ΅  | λ°μ΄ν„°μ…‹ | λ¨λΈ | νƒμƒ‰ κ³µκ°„ | μ‹λ„ νμ | μµμ ν™” λ©ν‘ |
|:---|:---|:---|:---|:---|:---|
| GridSearchCV | Breast Cancer | Logistic Regression | 20κ° μ΅°ν•© (2x5x2) | 20ν | ROC AUC μµλ€ν™” |
| Optuna | Breast Cancer | Random Forest | μ—°μ† κ³µκ°„ (50ν Trial) | 50ν | μ •ν™•λ„ μµλ€ν™” |

#### κ²°κ³Ό: μ„±λ¥ λ° ν¨μ¨μ„± λΉ„κµ

| μ§€ν‘ | GridSearchCV (Logistic Regression) | Optuna (Random Forest) | μΉμ |
|:---|:---|:---|:---|
| **μµκ³  κµμ°¨κ²€μ¦ μ •ν™•λ„** | 0.9825 | 0.9582 | GridSearchCV (+0.0243) |
| **μµμΆ… ν…μ¤νΈ μ •ν™•λ„** | 0.9825 | 0.9649 | GridSearchCV (+0.0176) |
| **μµκ³  κµμ°¨κ²€μ¦ ROC AUC** | 0.9960 | 0.9922 | GridSearchCV (+0.0038) |
| **μµμΆ… ν…μ¤νΈ ROC AUC** | 0.9957 | 0.9878 | GridSearchCV (+0.0079) |
| **μ΄ μµμ ν™” μ‹κ°„** | μ•½ 6.6μ΄ | μ•½ 12.8μ΄ | GridSearchCV |
| **ν¨μ¨μ„± (μ μ/μ‹κ°„)** | 14.89 | 7.62 | GridSearchCV |

**λ¶„μ„**: μ΄ νΉμ • μ‹¤ν—μ—μ„λ” `GridSearchCV`κ°€ `Optuna`λ³΄λ‹¤ λ” λ†’μ€ μ„±λ¥κ³Ό λΉ λ¥Έ μµμ ν™” μ‹κ°„μ„ λ³΄μ€μµλ‹λ‹¤. μ΄λ” `GridSearchCV`κ°€ νƒμƒ‰ν• Logistic Regression λ¨λΈμ νƒμƒ‰ κ³µκ°„μ΄ μƒλ€μ μΌλ΅ μ‘κ³ , RandomForest λ¨λΈμ Optuna νƒμƒ‰ κ³µκ°„μ΄ λ” λ„“κ³  λ³µμ΅ν–κΈ° λ•λ¬ΈμΌ μ μμµλ‹λ‹¤. λν•, Logistic Regressionμ€ RandomForestλ³΄λ‹¤ ν•™μµ μ‹κ°„μ΄ μ§§μ•„ `GridSearchCV`μ μ „μ νƒμƒ‰ λ°©μ‹μ΄ μ λ¦¬ν•κ² μ‘μ©ν–μ„ κ°€λ¥μ„±μ΄ μμµλ‹λ‹¤. μ΄ κ²°κ³Όλ” **λ¨λΈκ³Ό νƒμƒ‰ κ³µκ°„μ νΉμ„±μ— λ”°λΌ μµμ ν™” λ°©λ²•μ ν¨μ¨μ„±μ΄ λ‹¬λΌμ§ μ μμ**μ„ μ‹μ‚¬ν•©λ‹λ‹¤.

### 4.2 λ°©λ²•λ΅ λ³„ μ¥λ‹¨μ  λ¶„μ„

κ° ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•λ΅ μ€ κ³ μ ν• μ¥λ‹¨μ μ„ κ°€μ§€λ©°, μ΄λ” ν”„λ΅μ νΈμ νΉμ„±(λ°μ΄ν„° ν¬κΈ°, λ¨λΈ λ³µμ΅λ„, μ‹κ°„ μ μ•½ λ“±)μ— λ”°λΌ μ ν•©ν• λ°©λ²•μ΄ λ‹¬λΌμ§ μ μμμ„ μλ―Έν•©λ‹λ‹¤.

#### λΉ„κµ: GridSearchCV

**μ¥μ :**

-   β… **μ „μ νƒμƒ‰ (Exhaustive Search)**: μ‚¬μ©μκ°€ μ •μν• νƒμƒ‰ κ³µκ°„ λ‚΄μ λ¨λ“  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ²΄κ³„μ μΌλ΅ νƒμƒ‰ν•©λ‹λ‹¤. λ”°λΌμ„ μ£Όμ–΄μ§„ κ·Έλ¦¬λ“ λ‚΄μ—μ„λ” **μµμ μ μ΅°ν•©μ„ ν™•μ‹¤ν μ°Ύμ„ μ μμµλ‹λ‹¤.**
-   β… **μ•μ •μ„± λ° μ¬ν„μ„±**: λ™μΌν• `random_state`μ™€ λ°μ΄ν„° λ¶„ν• μ„ μ‚¬μ©ν•λ©΄ ν•­μƒ λ™μΌν• κ²°κ³Όλ¥Ό μ–»μ„ μ μμ–΄ μ‹¤ν—μ μ•μ •μ„±κ³Ό μ¬ν„μ„±μ΄ λ†’μµλ‹λ‹¤.
-   β… **μ‘μ€ νƒμƒ‰ κ³µκ°„μ—μ„ ν¨μ¨μ **: ν•μ΄νΌνλΌλ―Έν„° ν›„λ³΄ κ°’μ μκ°€ μ κ±°λ‚ νƒμƒ‰ κ³µκ°„μ΄ μ‘μ„ λ•, `GridSearchCV`λ” λ§¤μ° ν¨μ¨μ μΌλ΅ μµμ ν•΄λ¥Ό μ°Ύμ•„λƒ…λ‹λ‹¤.
-   β… **ν•΄μ„ μ©μ΄**: νƒμƒ‰ κ²°κ³Όκ°€ λ…ν™•ν•κ³  μ§κ΄€μ μ΄μ–΄μ„, μ–΄λ–¤ νλΌλ―Έν„° μ΅°ν•©μ΄ μ–΄λ–¤ μ„±λ¥μ„ λƒλ”μ§€ λ¶„μ„ν•κΈ° μ©μ΄ν•©λ‹λ‹¤.

**λ‹¨μ :**

-   β **μ΅°ν•© ν­λ° (Combinatorial Explosion)**: ν•μ΄νΌνλΌλ―Έν„°μ κ°μλ‚ κ° νλΌλ―Έν„°μ ν›„λ³΄ κ°’ μκ°€ μ¦κ°€ν• μλ΅ νƒμƒ‰ν•΄μ•Ό ν•  μ΅°ν•©μ μκ°€ κΈ°ν•κΈ‰μμ μΌλ΅ λμ–΄λ‚©λ‹λ‹¤. μ΄λ” κ³„μ‚° μ‹κ°„κ³Ό μμ› μ†λ¨λ¥Ό μ—„μ²­λ‚κ² μ¦κ°€μ‹ν‚µλ‹λ‹¤.
-   β **μ—°μ† κ³µκ°„ νƒμƒ‰μ ν•κ³„**: `GridSearchCV`λ” λ―Έλ¦¬ μ •μλ μ΄μ‚°μ μΈ κ°’λ“¤λ§μ„ νƒμƒ‰ν•  μ μμµλ‹λ‹¤. μ‹¤μν• ν•μ΄νΌνλΌλ―Έν„°μ κ²½μ°, μ—°μ†μ μΈ μµμ κ°’μ„ μ°ΎκΈ° μ–΄λ µκ³ , νƒμƒ‰ κ°„κ²©μ„ μΆνλ©΄ μ΅°ν•© ν­λ° λ¬Έμ κ°€ μ‹¬ν™”λ©λ‹λ‹¤.
-   β **μ‹κ°„ λ³µμ΅λ„**: `O(n^k)` (nμ€ κ° νλΌλ―Έν„°μ ν›„λ³΄ μ, kλ” νλΌλ―Έν„°μ κ°μ)λ΅, ν™•μ¥μ„±μ΄ λ¶€μ΅±ν•μ—¬ λ€κ·λ¨ λ¬Έμ λ‚ λ³µμ΅ν• λ¨λΈμ—λ” μ μ©ν•κΈ° μ–΄λ µμµλ‹λ‹¤.

#### λΉ„κµ: Optuna (λ² μ΄μ§€μ• μµμ ν™”)

**μ¥μ :**

-   β… **μ§€λ¥μ  νƒμƒ‰ (Intelligent Search)**: μ΄μ „ μ‹λ„μ—μ„ μ–»μ€ μ„±λ¥ μ •λ³΄λ¥Ό λ°”νƒ•μΌλ΅ λ‹¤μ νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ§€λ¥μ μΌλ΅ μ μ•ν•©λ‹λ‹¤. μ΄λ” λ¬΄μ‘μ„ νƒμƒ‰λ³΄λ‹¤ ν›¨μ”¬ ν¨μ¨μ μΌλ΅ μµμ ν•΄λ¥Ό μ°Ύμ•„λƒ…λ‹λ‹¤.
-   β… **μ—°μ† κ³µκ°„ μµμ ν™”**: μ‹¤μν• ν•μ΄νΌνλΌλ―Έν„°μ— λ€ν•΄ μ—°μ†μ μΈ νƒμƒ‰ κ³µκ°„μ„ ν¨κ³Όμ μΌλ΅ νƒμƒ‰ν•  μ μμ–΄, `GridSearchCV`μ ν•κ³„λ¥Ό κ·Ήλ³µν•©λ‹λ‹¤.
-   β… **μ΅°κΈ° μΆ…λ£ (Pruning) μ§€μ›**: ν•™μµ κ³Όμ •μ—μ„ μ„±λ¥μ΄ μΆ‹μ§€ μ•μ€ μ‹λ„(Trial)λ¥Ό μ΅°κΈ°μ— μ¤‘λ‹¨ν•μ—¬ λ¶ν•„μ”ν• κ³„μ‚° μμ› λ‚­λΉ„λ¥Ό μ¤„μ…λ‹λ‹¤. μ΄λ” νΉν λ”¥λ¬λ‹ λ¨λΈκ³Ό κ°™μ΄ ν•™μµ μ‹κ°„μ΄ κΈ΄ κ²½μ°μ— λ§¤μ° μ μ©ν•©λ‹λ‹¤.
-   β… **ν™•μ¥μ„±**: `GridSearchCV`λ³΄λ‹¤ ν›¨μ”¬ λ„“κ³  λ³µμ΅ν• νƒμƒ‰ κ³µκ°„μ—μ„ ν¨μ¨μ μΌλ΅ μ‘λ™ν•λ©°, λ€κ·λ¨ λ¬Έμ μ— λ” μ ν•©ν•©λ‹λ‹¤.

**λ‹¨μ :**

-   β **ν™•λ¥ μ  κ²°κ³Ό**: λ² μ΄μ§€μ• μµμ ν™”λ” ν™•λ¥ μ  λ¨λΈμ„ κΈ°λ°μΌλ΅ ν•λ―€λ΅, μ‹¤ν–‰ν•  λ•λ§λ‹¤ κ²°κ³Όκ°€ μ•½κ°„μ”© λ³€λ™λ  μ μμµλ‹λ‹¤. (λ¬Όλ΅  `random_state`λ¥Ό κ³ μ •ν•λ©΄ μ¬ν„ κ°€λ¥ν•©λ‹λ‹¤.)
-   β **λ³µμ΅μ„±**: `GridSearchCV`μ— λΉ„ν•΄ λ² μ΄μ§€μ• μµμ ν™”μ κ°λ…κ³Ό `Optuna`μ μ‚¬μ©λ²•μ΄ λ‹¤μ† λ³µμ΅ν•κ² λκ»΄μ§ μ μμµλ‹λ‹¤.
-   β **μ΄κΈ° μ¤λ²„ν—¤λ“**: νƒμƒ‰ μ΄κΈ°μ— λ¨λΈμ„ κµ¬μ¶•ν•κ³  ν‰κ°€ν•λ” λ° ν•„μ”ν• μ¤λ²„ν—¤λ“κ°€ μ΅΄μ¬ν•λ―€λ΅, ν•μ΄νΌνλΌλ―Έν„° κ³µκ°„μ΄ λ§¤μ° μ‘κ±°λ‚ λ¨λΈ ν•™μµ μ‹κ°„μ΄ μ§§μ€ λ¬Έμ μ—μ„λ” `GridSearchCV`λ³΄λ‹¤ λΉ„ν¨μ¨μ μΌ μ μμµλ‹λ‹¤.

### 4.3 μ‹¤λ¬΄ μ μ© κ°€μ΄λ“

ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•λ΅ μ„ μ„ νƒν•  λ•λ” ν”„λ΅μ νΈμ νΉμ„±, μ‹κ°„ μ μ•½, μ¬ν„μ„± μ”κµ¬μ‚¬ν•­ λ“±μ„ μΆ…ν•©μ μΌλ΅ κ³ λ ¤ν•΄μ•Ό ν•©λ‹λ‹¤. λ‹¤μμ€ λ‹¤μ–‘ν• μƒν™©μ— λ”°λ¥Έ μµμ ν™” λ°©λ²• μ„ νƒ κ°€μ΄λ“λΌμΈμ…λ‹λ‹¤.

#### μƒν™©λ³„ μµμ ν™” λ°©λ²• μ„ νƒ κΈ°μ¤€

```python
def choose_optimization_method(search_space_size, time_constraint, reproducibility):
    """μ£Όμ–΄μ§„ μ΅°κ±΄μ— λ”°λΌ μ μ ν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•μ„ μ μ•ν•©λ‹λ‹¤.

    Args:
        search_space_size (int): νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ λ€λµμ μΈ ν¬κΈ° (μ: 100, 1000).
        time_constraint (str): μ‹κ°„ μ μ•½ μ΅°κ±΄ ('strict': μ—„κ²©ν•¨, 'flexible': μ μ—°ν•¨).
        reproducibility (str): μ¬ν„μ„± μ”κµ¬ μμ¤€ ('high': λ†’μ, 'medium': λ³΄ν†µ, 'low': λ‚®μ).

    Returns:
        str: μ μ•ν•λ” μµμ ν™” λ°©λ²•.
    """
    if search_space_size < 100: # νƒμƒ‰ κ³µκ°„μ΄ μ‘μ€ κ²½μ°
        if reproducibility == "high":
            return "GridSearchCV (μ¬ν„μ„± λ†’μ)"
        else:
            return "GridSearchCV λλ” Optuna (λΉ λ¥Έ νƒμƒ‰)"
    
    elif search_space_size < 1000: # νƒμƒ‰ κ³µκ°„μ΄ μ¤‘κ°„ μ •λ„μΈ κ²½μ°
        if time_constraint == "strict":
            return "Optuna + Pruning (μ‹κ°„ μ μ•½ μ—„κ²©)"
        else:
            return "Optuna (ν¨μ¨μ  νƒμƒ‰)"
    
    else:  # νƒμƒ‰ κ³µκ°„μ΄ λ§¤μ° ν° κ²½μ°
        return "Optuna + Multi-objective (λ€κ·λ¨ νƒμƒ‰)"

# μ‚¬μ© μμ‹:
print(choose_optimization_method(50, "flexible", "high")) # GridSearchCV (μ¬ν„μ„± λ†’μ)
print(choose_optimization_method(500, "strict", "medium")) # Optuna + Pruning (μ‹κ°„ μ μ•½ μ—„κ²©)
print(choose_optimization_method(5000, "flexible", "low")) # Optuna + Multi-objective (λ€κ·λ¨ νƒμƒ‰)
```

#### μμ‚¬κ²°μ • λ§¤νΈλ¦­μ¤ (Decision Matrix)

λ‹¤μ ν‘λ” κ° μµμ ν™” λ°©λ²•λ΅ μ μ£Όμ” νΉμ„±μ„ λΉ„κµν•μ—¬, ν”„λ΅μ νΈ μƒν™©μ— λ§λ” μμ‚¬κ²°μ •μ„ λ•κΈ° μ„ν• κ°€μ΄λ“λΌμΈμ„ μ κ³µν•©λ‹λ‹¤. μ μλ” 1~5μ  μ²™λ„μ΄λ©°, 5μ μ΄ κ°€μ¥ μ°μν•¨μ„ μλ―Έν•©λ‹λ‹¤.

| μ΅°κ±΄/νΉμ„± | GridSearchCV | Optuna |
|:---|:---|:---|
| **νƒμƒ‰ κ³µκ°„ < 100κ°** | β­β­β­β­β­ (μ „μ νƒμƒ‰μΌλ΅ μµμ  λ³΄μ¥) | β­β­β­ (ν¨μ¨μ μ΄μ§€λ§ μ „μ νƒμƒ‰λ§νΌ ν™•μ‹¤ν•μ§„ μ•μ) |
| **νƒμƒ‰ κ³µκ°„ > 100κ°** | β­ (μ΅°ν•© ν­λ°λ΅ λΉ„ν¨μ¨μ ) | β­β­β­β­β­ (μ§€λ¥μ  νƒμƒ‰μΌλ΅ ν¨μ¨μ„± λ†’μ) |
| **μ‹κ°„ μ μ•½ μμ** | β­ (μ‹κ°„ μ†λ¨μ ) | β­β­β­β­ (Pruning λ“±μΌλ΅ ν¨μ¨μ ) |
| **μ¬ν„μ„± μ¤‘μ”** | β­β­β­β­β­ (λ™μΌ κ²°κ³Ό λ³΄μ¥) | β­β­β­ (random_state κ³ μ • μ‹ μ¬ν„ κ°€λ¥ν•λ‚, ν™•λ¥ μ  νΉμ„± μ΄ν•΄ ν•„μ”) |
| **μ—°μ† νλΌλ―Έν„°** | β­ (μ΄μ‚°κ°’λ§ νƒμƒ‰ κ°€λ¥) | β­β­β­β­β­ (μ‹¤μν• λ¶„ν¬ νƒμƒ‰μ— κ°•μ ) |
| **μ΄λ³΄μ μΉν™”μ„±** | β­β­β­β­ (κ°λ… μ΄ν•΄ λ° μ‚¬μ©λ²• κ°„λ‹¨) | β­β­β­ (κ°λ… λ° μ„¤μ •μ΄ λ‹¤μ† λ³µμ΅) |

---

## 5. μ‹¤λ¬΄ μ μ© μ „λµ

λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈλ” λ‹¨μν λ¨λΈμ„ ν•™μµν•κ³  ν‰κ°€ν•λ” κ²ƒμ„ λ„μ–΄, μ‹¤μ  λ¬Έμ  ν•΄κ²°μ„ μ„ν• μ²΄κ³„μ μΈ μ ‘κ·Ό λ°©μ‹μ΄ ν•„μ”ν•©λ‹λ‹¤. μ΄ μ„Ήμ…μ—μ„λ” ν”„λ΅μ νΈ λ‹¨κ³„λ³„ κ°€μ΄λ“, ν•μ΄νΌνλΌλ―Έν„° μ„ νƒ μ „λµ, κ·Έλ¦¬κ³  μ„±λ¥ ν‰κ°€ λ° ν•΄μ„μ— λ€ν• μ‹¤λ¬΄μ μΈ νμ„ μ κ³µν•©λ‹λ‹¤.

### 5.1 ν”„λ΅μ νΈ λ‹¨κ³„λ³„ κ°€μ΄λ“

λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈλ” μΌλ°μ μΌλ΅ λ‹¤μκ³Ό κ°™μ€ λ‹¨κ³„λ΅ μ§„ν–‰λλ©°, κ° λ‹¨κ³„μ—μ„ ν¨μ¨μ μΈ μ „λµμ„ μ μ©ν•λ” κ²ƒμ΄ μ¤‘μ”ν•©λ‹λ‹¤.

#### λ‹¨κ³„ 1: λΉ λ¥Έ ν”„λ΅ν† νƒ€μ΄ν•‘ (Rapid Prototyping)

ν”„λ΅μ νΈ μ΄κΈ°μ— λ³µμ΅ν• λ¨λΈμ΄λ‚ μ •κµν• νλ‹ μ—†μ΄ κ°„λ‹¨ν• λ¨λΈκ³Ό κΈ°λ³Έμ μΈ μ „μ²λ¦¬λ΅ λ² μ΄μ¤λΌμΈ μ„±λ¥μ„ λΉ λ¥΄κ² ν™•μΈν•λ” λ‹¨κ³„μ…λ‹λ‹¤. μ΄λ¥Ό ν†µν•΄ λ¬Έμ μ λ‚μ΄λ„λ¥Ό νμ•…ν•κ³ , μ΄κΈ° λ°©ν–¥μ„±μ„ μ„¤μ •ν•  μ μμµλ‹λ‹¤.

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_breast_cancer

# λ°μ΄ν„° λ΅λ“ (μμ‹)
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# κΈ°λ³Έ PipelineμΌλ΅ λ² μ΄μ¤λΌμΈ κµ¬μ¶•
# μ „μ²λ¦¬: StandardScaler, λ¨λΈ: RandomForestClassifier (κΈ°λ³Έ ν•μ΄νΌνλΌλ―Έν„°)
baseline_pipeline = Pipeline([
    ('preprocessor', StandardScaler()),
    ('model', RandomForestClassifier(random_state=42))
])

# λΉ λ¥Έ μ„±λ¥ ν™•μΈ (5-fold κµμ°¨ κ²€μ¦)
baseline_score = cross_val_score(baseline_pipeline, X, y, cv=5, scoring='accuracy', n_jobs=-1).mean()
print(f"λ² μ΄μ¤λΌμΈ λ¨λΈμ ν‰κ·  κµμ°¨κ²€μ¦ μ •ν™•λ„: {baseline_score:.4f}")
```

#### λ‹¨κ³„ 2: μ²΄κ³„μ  μµμ ν™” (Systematic Optimization)

λ² μ΄μ¤λΌμΈ μ„±λ¥μ„ ν™•μΈν• ν›„, λ¨λΈμ μ„±λ¥μ„ ν–¥μƒμ‹ν‚¤κΈ° μ„ν•΄ ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ μ²΄κ³„μ μΌλ΅ μν–‰ν•λ” λ‹¨κ³„μ…λ‹λ‹¤. νƒμƒ‰ κ³µκ°„μ ν¬κΈ°μ™€ μ‹κ°„ μ μ•½μ„ κ³ λ ¤ν•μ—¬ μ μ ν• μµμ ν™” λ°©λ²•μ„ μ„ νƒν•©λ‹λ‹¤.

```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
import optuna

# μμ‹: νμ΄ν”„λΌμΈκ³Ό ν•¨κ» GridSearchCV λλ” Optuna μ μ©
# pipelineμ€ 4.3μ—μ„ μ •μλ μ „μ²λ¦¬ + λ¨λΈ νμ΄ν”„λΌμΈμ„ κ°€μ •
# X_train, y_trainμ€ ν›λ ¨ λ°μ΄ν„°

# μ‘μ€ νƒμƒ‰ κ³µκ°„: GridSearchCV ν™μ©
# param_combinationsλ” νƒμƒ‰ν•  ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ μ΄ κ°μ
if len(param_combinations) < 100:
    param_grid_small = {
        'classifier__n_estimators': [50, 100],
        'classifier__max_depth': [10, 20]
    }
    grid_search = GridSearchCV(
        pipeline, param_grid_small, 
        cv=5, scoring='accuracy', n_jobs=-1
    )
    grid_search.fit(X_train, y_train)
    print(f"GridSearchCV μµμ  νλΌλ―Έν„°: {grid_search.best_params_}")

# ν° νƒμƒ‰ κ³µκ°„: Optuna ν™μ©
 else:
     def objective_optuna(trial):
         n_estimators = trial.suggest_int('classifier__n_estimators', 50, 200)
         max_depth = trial.suggest_int('classifier__max_depth', 5, 30)
         # pipeline λ‚΄λ¶€μ classifierμ— μ ‘κ·Όν•μ—¬ ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •
         pipeline.set_params(classifier__n_estimators=n_estimators, classifier__max_depth=max_depth)
         score = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1).mean()
         return score
     study = optuna.create_study(direction="maximize")
     study.optimize(objective_optuna, n_trials=100)
     print(f"Optuna μµμ  νλΌλ―Έν„°: {study.best_params}")
```

#### λ‹¨κ³„ 3: μ„±λ¥ κ²€μ¦ λ° λ°°ν¬ μ¤€λΉ„ (Validation & Deployment)

μµμ ν™”λ λ¨λΈμ μµμΆ… μ„±λ¥μ„ κ²€μ¦ν•κ³ , μ‹¤μ  μ„λΉ„μ¤ ν™κ²½μ— λ°°ν¬ν•κΈ° μ„ν• μ¤€λΉ„λ¥Ό ν•λ” λ‹¨κ³„μ…λ‹λ‹¤. ν…μ¤νΈ λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•μ—¬ λ¨λΈμ μΌλ°ν™” μ„±λ¥μ„ μµμΆ… ν™•μΈν•κ³ , λ‹¤μ–‘ν• ν‰κ°€ μ§€ν‘λ¥Ό ν†µν•΄ λ¨λΈμ κ°•μ κ³Ό μ•½μ μ„ νμ•…ν•©λ‹λ‹¤.

```python
from sklearn.metrics import classification_report, roc_auc_score

# best_modelμ€ μµμ ν™” λ‹¨κ³„μ—μ„ μ–»μ€ μµμΆ… λ¨λΈ κ°μ²΄ (μ: grid_search.best_estimator_ λλ” Optuna study.best_trial.user_attrs['best_model'])
# X_test, y_testλ” λ…λ¦½μ μΈ ν…μ¤νΈ λ°μ΄ν„°μ…‹

# μµμΆ… λ¨λΈ ν•™μµ (μ „μ²΄ ν›λ ¨ λ°μ΄ν„° μ‚¬μ©)
 best_model.fit(X_train, y_train) # μ΄λ―Έ μµμ ν™” κ³Όμ •μ—μ„ ν•™μµλμ—μ„ μ μμ

# ν…μ¤νΈ λ°μ΄ν„°μ— λ€ν• μμΈ΅
y_pred = best_model.predict(X_test)
# λ¶„λ¥ λ¨λΈμ κ²½μ° ν™•λ¥  μμΈ΅λ„ μ¤‘μ”
 y_pred_proba = best_model.predict_proba(X_test)[:, 1] # μ΄μ§„ λ¶„λ¥μ μ–‘μ„± ν΄λμ¤ ν™•λ¥ 

# μΆ…ν•© ν‰κ°€
print("\n--- μµμΆ… λ¨λΈ μ„±λ¥ λ³΄κ³ μ„ ---")
print(classification_report(y_test, y_pred, target_names=cancer.target_names))

# ROC AUC μ μ (μ΄μ§„ λ¶„λ¥μ κ²½μ°)
 print(f"ROC AUC: {roc_auc_score(y_test, y_pred_proba):.4f}")

# λ¨λΈ μ €μ¥ (λ°°ν¬λ¥Ό μ„ν•΄)
 import joblib
 joblib.dump(best_model, 'best_model.pkl')
 print("μµμΆ… λ¨λΈ μ €μ¥ μ™„λ£: best_model.pkl")
```

### 5.2 ν•μ΄νΌνλΌλ―Έν„° μ„ νƒ μ „λµ

λ¨λΈμ μΆ…λ¥μ™€ λ¬Έμ μ νΉμ„±μ— λ”°λΌ μ¤‘μ”ν• ν•μ΄νΌνλΌλ―Έν„°κ°€ λ‹¤λ¦…λ‹λ‹¤. ν¨μ¨μ μΈ νλ‹μ„ μ„ν•΄μ„λ” κ° λ¨λΈμ ν•µμ‹¬ ν•μ΄νΌνλΌλ―Έν„°μ™€ κ·Έ μ—­ν• μ— λ€ν• μ΄ν•΄κ°€ ν•„μμ μ…λ‹λ‹¤. λ‹¤μμ€ μ£Όμ” λ¨λΈλ³„ ν•µμ‹¬ ν•μ΄νΌνλΌλ―Έν„°μ™€ κ·Έ μ„ νƒ μ „λµμ— λ€ν• κ°€μ΄λ“μ…λ‹λ‹¤.

#### ν•µμ‹¬: λ¨λΈλ³„ ν•µμ‹¬ νλΌλ―Έν„°

**RandomForest (λλ¤ ν¬λ μ¤νΈ):**

λλ¤ ν¬λ μ¤νΈλ” μ—¬λ¬ κ°μ μμ‚¬κ²°μ •λ‚λ¬΄λ¥Ό μ•™μƒλΈ”ν•μ—¬ κ³Όμ ν•©μ„ μ¤„μ΄κ³  μ•μ •μ μΈ μ„±λ¥μ„ μ κ³µν•©λ‹λ‹¤. μ£Όλ΅ νΈλ¦¬μ κ°μ, κΉμ΄, λ…Έλ“ λ¶„ν•  μ΅°κ±΄ λ“±μ„ μ΅°μ ν•©λ‹λ‹¤.

```python
# ν•µμ‹¬ νλΌλ―Έν„° μ°μ„ μμ„
priority_params_rf = {
    'n_estimators': [50, 100, 200, 300],      # νΈλ¦¬μ κ°μ: λ¨λΈμ λ³µμ΅λ„μ™€ μ•μ •μ„±μ— ν° μν–¥. λ§μ„μλ΅ μΆ‹μ§€λ§ κ³„μ‚° λΉ„μ© μ¦κ°€.
    'max_depth': [10, 20, 30, None],         # κ° νΈλ¦¬μ μµλ€ κΉμ΄: κ³Όμ ν•© μ μ–΄. Noneμ€ μ ν• μ—†μ.
    'min_samples_split': [2, 5, 10, 20],     # λ…Έλ“ λ¶„ν• μ„ μ„ν• μµμ† μƒν” μ: κ³Όμ ν•© λ°©μ§€. κ°’μ΄ ν΄μλ΅ λ¨λΈ λ‹¨μν™”.
    'min_samples_leaf': [1, 2, 4, 8]        # λ¦¬ν”„ λ…Έλ“(Leaf Node)κ°€ λκΈ° μ„ν• μµμ† μƒν” μ: κ³Όμ ν•© λ°©μ§€. κ°’μ΄ ν΄μλ΅ λ¨λΈ λ‹¨μν™”.
}

# μ‚¬μ© μμ‹:
 rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42)
```

**LogisticRegression (λ΅μ§€μ¤ν‹± νκ·€):**

λ΅μ§€μ¤ν‹± νκ·€λ” μ„ ν• λ¨λΈμ΄μ§€λ§ λ¶„λ¥μ— μ‚¬μ©λλ©°, μ£Όλ΅ μ •κ·ν™”(κ·μ ) κ°•λ„μ™€ μµμ ν™” μ•κ³ λ¦¬μ¦μ„ μ΅°μ ν•©λ‹λ‹¤.

```python
# L1/L2 μ •κ·ν™”μ™€ κ°•λ„
priority_params_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],       # μ •κ·ν™” κ°•λ„: Cλ” κ·μ  κ°•λ„μ μ—­μ. κ°’μ΄ μ‘μ„μλ΅ κ°•ν• κ·μ .
    'penalty': ['l1', 'l2', 'elasticnet', None], # μ •κ·ν™” λ°©λ²•: L1, L2, ElasticNet λλ” κ·μ  μ—†μ.
    'solver': ['liblinear', 'lbfgs', 'saga']      # μµμ ν™” μ•κ³ λ¦¬μ¦: λ°μ΄ν„°μ…‹ ν¬κΈ°μ™€ penalty μΆ…λ¥μ— λ”°λΌ μ ν•©ν• solver μ„ νƒ.
}

# μ‚¬μ© μμ‹:
 lr_model = LogisticRegression(C=0.1, penalty='l2', solver='liblinear', max_iter=1000, random_state=42)
```

**SVM (Support Vector Machine):**

SVMμ€ κ°•λ ¥ν• λ¶„λ¥ λ¨λΈμ΄μ§€λ§, ν•μ΄νΌνλΌλ―Έν„° μ„¤μ •μ— λ§¤μ° λ―Όκ°ν•©λ‹λ‹¤. νΉν `C`μ™€ `gamma`λ” μ„±λ¥μ— κ²°μ •μ μΈ μν–¥μ„ λ―ΈμΉ©λ‹λ‹¤.

```python
# μ»¤λ„κ³Ό ν•μ΄νΌνλΌλ―Έν„°
priority_params_svm = {
    'C': [0.1, 1, 10, 100, 1000],             # κ·μ  νλΌλ―Έν„°: λ§μ§„ μ¤λ¥μ— λ€ν• νλ„ν‹° μ΅°μ . κ°’μ΄ ν΄μλ΅ ν›λ ¨ λ°μ΄ν„°μ— λ” μ •ν™•ν•κ² λ§μ¶°μ§€λ ¤ ν•¨.
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],  # RBF μ»¤λ„μ μν–¥ λ²”μ„ μ΅°μ : κ°’μ΄ ν΄μλ΅ ν›λ ¨ μƒν”μ μν–¥ λ²”μ„κ°€ μΆμ•„μ Έ κ³Όμ ν•© μ„ν—.
    'kernel': ['rbf', 'linear', 'poly', 'sigmoid']   # μ»¤λ„ ν•¨μ: λ°μ΄ν„°λ¥Ό κ³ μ°¨μ› κ³µκ°„μΌλ΅ λ§¤ν•‘ν•λ” λ°©μ‹. μ„ ν•/λΉ„μ„ ν• λ¶„λ¥ κ²°μ •.
}

# μ‚¬μ© μμ‹:
 svm_model = SVC(C=10, gamma='scale', kernel='rbf', random_state=42)
```

### 5.3 μ„±λ¥ ν‰κ°€ λ° ν•΄μ„

λ¨λΈμ μμΈ΅ μ„±λ¥μ„ κ°κ΄€μ μΌλ΅ ν‰κ°€ν•κ³ , κ·Έ κ²°κ³Όλ¥Ό λΉ„μ¦λ‹μ¤ λ©ν‘μ— λ§μ¶° ν•΄μ„ν•λ” κ²ƒμ€ λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈμ μ„±κ³µμ— λ§¤μ° μ¤‘μ”ν•©λ‹λ‹¤. λ‹¨μν λ†’μ€ μ •ν™•λ„λ§μ„ μ¶”κµ¬ν•κΈ°λ³΄λ‹¤λ”, λ¬Έμ μ νΉμ„±κ³Ό λ¨λΈμ μ¤λ¥ μ ν•μ„ μ΄ν•΄ν•λ” κ²ƒμ΄ ν•„μ”ν•©λ‹λ‹¤.

#### ν•µμ‹¬: ν‰κ°€μ§€ν‘ μ„ νƒ κ°€μ΄λ“

λ¬Έμ  μ ν•κ³Ό λΉ„μ¦λ‹μ¤ λ©ν‘μ— λ”°λΌ μ μ ν• ν‰κ°€ μ§€ν‘λ¥Ό μ„ νƒν•λ” κ²ƒμ΄ μ¤‘μ”ν•©λ‹λ‹¤.

**μ΄μ§„ λ¶„λ¥ (Binary Classification):**

-   **κ· ν• λ°μ΄ν„°μ…‹ (Balanced Dataset)**: μ–‘μ„±(Positive) ν΄λμ¤μ™€ μμ„±(Negative) ν΄λμ¤μ μƒν” μκ°€ λΉ„μ·ν• κ²½μ°
    -   `primary_metric = "accuracy"`: μ „μ²΄ μμΈ΅ μ¤‘ μ¬λ°”λ¥΄κ² μμΈ΅ν• λΉ„μ¨. μ§κ΄€μ μ΄κ³  μ΄ν•΄ν•κΈ° μ‰½μµλ‹λ‹¤.
    -   `secondary_metrics = ["precision", "recall", "f1_score"]`: μ •ν™•λ„ μ™Έμ— μ •λ°€λ„, μ¬ν„μ¨, F1-Scoreλ¥Ό ν•¨κ» κ³ λ ¤ν•μ—¬ λ¨λΈμ μ„±λ¥μ„ λ‹¤κ°λ„λ΅ ν‰κ°€ν•©λ‹λ‹¤.

-   **λ¶κ· ν• λ°μ΄ν„°μ…‹ (Imbalanced Dataset)**: νΉμ • ν΄λμ¤(μ£Όλ΅ μ–‘μ„± ν΄λμ¤)μ μƒν” μκ°€ ν„μ €ν μ μ€ κ²½μ°
    -   `primary_metric = "roc_auc"`: ROC κ³΅μ„  μ•„λ λ©΄μ (Area Under the Curve). λ¨λ“  κ°€λ¥ν• μ„κ³„κ°’μ— λ€ν• λ¨λΈμ μ„±λ¥μ„ λ‚νƒ€λ‚΄λ©°, ν΄λμ¤ λ¶κ· ν•μ— κ°•κ±΄ν•©λ‹λ‹¤.
    -   `secondary_metrics = ["precision_recall_auc", "f1_score"]`: PR κ³΅μ„  μ•„λ λ©΄μ (Precision-Recall AUC)κ³Ό F1-Scoreλ” λ¶κ· ν• λ°μ΄ν„°μ…‹μ—μ„ νΉν μ μ©ν• μ§€ν‘μ…λ‹λ‹¤.

-   **μλ£/μ•μ „ λ¶„μ•Ό (Cost-Sensitive Applications)**: μ¤λ¶„λ¥ λΉ„μ©μ΄ ν° κ²½μ° (μ: μ•” μ§„λ‹¨μ—μ„ μ•” ν™μλ¥Ό λ†“μΉλ” κ²½μ°)
    -   `primary_metric = "recall"` (λ―Όκ°λ„ μ¤‘μ‹): μ‹¤μ  μ–‘μ„±μΈ ν™μλ¥Ό λ†“μΉμ§€ μ•λ” κ²ƒμ΄ κ°€μ¥ μ¤‘μ”ν•  λ• μ¬ν„μ¨μ„ μµμ°μ„ μΌλ΅ κ³ λ ¤ν•©λ‹λ‹¤.
    -   `secondary_metrics = ["precision", "specificity"]`: μ •λ°€λ„(μ¤μ§„μ¨ κ°μ†)μ™€ νΉμ΄λ„(μ‹¤μ  μμ„±μ„ μμ„±μΌλ΅ μ μμΈ΅ν•λ” λΉ„μ¨)λ„ ν•¨κ» κ³ λ ¤ν•©λ‹λ‹¤.

**λ‹¤μ¤‘ λ¶„λ¥ (Multi-Class Classification):**

-   **κΈ°λ³Έ ν‰κ°€**: 
    -   `primary_metric = "accuracy"`: λ‹¤μ¤‘ λ¶„λ¥μ—μ„λ„ κ°€μ¥ κΈ°λ³Έμ μΈ μ§€ν‘μ…λ‹λ‹¤.
    -   `secondary_metrics = ["macro_f1", "weighted_f1"]`: 
        -   `macro_f1`: κ° ν΄λμ¤λ³„ F1-Scoreλ¥Ό κ³„μ‚°ν• ν›„ λ‹¨μ ν‰κ· . ν΄λμ¤ λ¶κ· ν•μ΄ μμ„ λ• μ‘μ€ ν΄λμ¤μ μ„±λ¥λ„ λ™λ“±ν•κ² λ°μν•©λ‹λ‹¤.
        -   `weighted_f1`: κ° ν΄λμ¤λ³„ F1-Scoreλ¥Ό κ³„μ‚°ν• ν›„, κ° ν΄λμ¤μ μƒν” μμ— λΉ„λ΅€ν•μ—¬ κ°€μ¤‘ ν‰κ· . ν΄λμ¤ λ¶κ· ν•μ΄ μμ„ λ• ν° ν΄λμ¤μ μ„±λ¥μ— λ” ν° κ°€μ¤‘μΉλ¥Ό μ¤λ‹λ‹¤.

-   **ν΄λμ¤λ³„ λ¶„μ„**: 
    -   `detailed_metrics = ["precision_macro", "recall_macro"]`: κ° ν΄λμ¤λ³„ μ •λ°€λ„μ™€ μ¬ν„μ¨μ„ λ§¤ν¬λ΅ ν‰κ· ν•μ—¬ μ „λ°μ μΈ μ„±λ¥μ„ νμ•…ν•©λ‹λ‹¤.

#### ν•΄μ„: κ²°κ³Ό ν•΄μ„ κ°€μ΄λ“

λ¨λΈμ μ„±λ¥ μ§€ν‘λ¥Ό ν•΄μ„ν•  λ•λ” λ‹¨μν μ«μλ¥Ό λ³΄λ” κ²ƒμ„ λ„μ–΄, ν†µκ³„μ  μ μμ„±κ³Ό μ‹¤μ©μ  μ μμ„±μ„ ν•¨κ» κ³ λ ¤ν•΄μ•Ό ν•©λ‹λ‹¤.

**ν†µκ³„μ  μ μμ„± κ²€μ¦:**

λ‘ λ¨λΈ κ°„μ μ„±λ¥ μ°¨μ΄κ°€ μ°μ—°μ— μν• κ²ƒμΈμ§€, μ•„λ‹λ©΄ ν†µκ³„μ μΌλ΅ μλ―Έ μλ” μ°¨μ΄μΈμ§€λ¥Ό κ²€μ¦ν•©λ‹λ‹¤. `t-test`μ™€ κ°™μ€ ν†µκ³„μ  κ°€μ„¤ κ²€μ •μ„ μ‚¬μ©ν•  μ μμµλ‹λ‹¤.

```python
def validate_improvement(baseline_scores, optimized_scores, alpha=0.05):
    """λ‘ λ¨λΈμ κµμ°¨ κ²€μ¦ μ μ κ°„ ν†µκ³„μ  μ μμ„±μ„ κ²€μ¦ν•©λ‹λ‹¤.

    Args:
        baseline_scores (list or np.array): κΈ°μ¤€ λ¨λΈμ κµμ°¨ κ²€μ¦ μ μ.
        optimized_scores (list or np.array): μµμ ν™”λ λ¨λΈμ κµμ°¨ κ²€μ¦ μ μ.
        alpha (float): μ μμμ¤€ (κΈ°λ³Έκ°’ 0.05).

    Returns:
        str: ν†µκ³„μ  μ μμ„± κ²€μ¦ κ²°κ³Ό λ©”μ‹μ§€.
    """
    from scipy.stats import ttest_rel # μμ²΄ t-κ²€μ • (paired t-test)
    
    # λ‘ μƒν” κ°„μ ν‰κ·  μ°¨μ΄κ°€ ν†µκ³„μ μΌλ΅ μ μλ―Έν•μ§€ κ²€μ •
    t_stat, p_value = ttest_rel(optimized_scores, baseline_scores)
    
    print(f"\n--- ν†µκ³„μ  μ μμ„± κ²€μ¦ (Paired t-test) ---")
    print(f"T-statistic: {t_stat:.4f}")
    print(f"P-value: {p_value:.4f}")

    if p_value < alpha:
        return "ν†µκ³„μ μΌλ΅ μ μν• κ°μ„  (P-value < alpha)"
    else:
        return "ν†µκ³„μ μΌλ΅ μ μν•μ§€ μ•μ (P-value >= alpha)"

# μ‚¬μ© μμ‹:
 baseline_scores = np.array([0.95, 0.94, 0.96, 0.93, 0.95])
 optimized_scores = np.array([0.96, 0.97, 0.96, 0.95, 0.97])
 print(validate_improvement(baseline_scores, optimized_scores))
```

**μ‹¤μ©μ  μ μμ„± ν‰κ°€:**

ν†µκ³„μ μΌλ΅ μ μλ―Έν• μ°¨μ΄κ°€ μλ”λΌλ„, κ·Έ μ°¨μ΄κ°€ μ‹¤μ  λΉ„μ¦λ‹μ¤λ‚ μ„λΉ„μ¤μ— λ―ΈμΉλ” μν–¥μ΄ λ―Έλ―Έν•  μ μμµλ‹λ‹¤. λ”°λΌμ„ μ„±λ¥ κ°μ„ μ΄ μ‹¤μ©μ μΌλ΅ μλ―Έ μλ” μμ¤€μΈμ§€ νλ‹¨ν•΄μ•Ό ν•©λ‹λ‹¤.

```python
def assess_practical_significance(improvement, threshold=0.01):
    """λ¨λΈ μ„±λ¥ κ°μ„ μ΄ μ‹¤μ©μ μΌλ΅ μλ―Έμλ” μμ¤€μΈμ§€ ν‰κ°€ν•©λ‹λ‹¤.

    Args:
        improvement (float): μ„±λ¥ κ°μ„  ν­ (μ: μµμ ν™” λ¨λΈ μ •ν™•λ„ - κΈ°μ¤€ λ¨λΈ μ •ν™•λ„).
        threshold (float): μ‹¤μ©μ  μ μμ„± νλ‹¨ κΈ°μ¤€ (κΈ°λ³Έκ°’ 0.01, μ¦‰ 1% κ°μ„ ).

    Returns:
        str: μ‹¤μ©μ  μ μμ„± ν‰κ°€ λ©”μ‹μ§€.
    """
    print(f"\n--- μ‹¤μ©μ  μ μμ„± ν‰κ°€ ---")
    print(f"μ„±λ¥ κ°μ„  ν­: {improvement:.4f}")
    print(f"μ‹¤μ©μ  μ μμ„± κΈ°μ¤€: {threshold:.4f}")

    if improvement > threshold:
        return "μ‹¤μ©μ μΌλ΅ μλ―Έμλ” κ°μ„  (κ°μ„  ν­ > κΈ°μ¤€)"
    else:
        return "λ―Έλ―Έν• κ°μ„  (κ³Όμ ν•© μ„ν— κ³ λ ¤ λλ” μ¶”κ°€ κ°μ„  ν•„μ”)"

# μ‚¬μ© μμ‹:
 improvement_value = 0.0088 # GridSearchCVμ™€ Optuna μ •ν™•λ„ μ°¨μ΄
 print(assess_practical_significance(improvement_value))
```

---

## 6. μΆ…ν•© μ •λ¦¬ λ° κ²°λ΅ 

λ³Έ λ¬Έμ„λ¥Ό ν†µν•΄ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”μ μ¤‘μ”μ„±, λ‹¤μ–‘ν• μµμ ν™” λ°©λ²•λ΅ , κ·Έλ¦¬κ³  Scikit-learn Pipelineμ ν™μ©λ²•μ„ μ‹¬μΈµμ μΌλ΅ ν•™μµν–μµλ‹λ‹¤. μ΄ μ„Ήμ…μ—μ„λ” ν•µμ‹¬ ν•™μµ λ‚΄μ©μ„ μ”μ•½ν•κ³ , μ‹¤λ¬΄ μ μ©μ„ μ„ν• λ² μ¤νΈ ν”„λ™ν‹°μ¤λ¥Ό μ μ‹ν•λ©°, ν–¥ν›„ ν•™μµ λ°©ν–¥μ„ μ μ•ν•©λ‹λ‹¤.

### 6.1 ν•µμ‹¬ ν•™μµ λ‚΄μ© μ”μ•½

#### ν•µμ‹¬: ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” ν•µμ‹¬ μ›λ¦¬

ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ” λ‹¨μν λ¨λΈ μ„±λ¥μ„ λ†’μ΄λ” κ²ƒμ„ λ„μ–΄, λ¨λΈμ μΌλ°ν™” λ¥λ ¥μ„ ν™•λ³΄ν•κ³  μμ›μ„ ν¨μ¨μ μΌλ΅ μ‚¬μ©ν•λ” λ° ν•„μμ μΈ κ³Όμ •μ…λ‹λ‹¤.

1.  **μ²΄κ³„μ  μ ‘κ·Ό**: μλ™ νƒμƒ‰μ΄λ‚ λ¬΄μ‘μ„ νƒμƒ‰λ³΄λ‹¤λ” `GridSearchCV`, `RandomizedSearchCV`, `Bayesian Optimization`κ³Ό κ°™μ΄ κ³„νμ μ΄κ³  μ§€λ¥μ μΈ νƒμƒ‰ λ°©λ²•μ„ μ‚¬μ©ν•μ—¬ μµμ μ ν•μ΄νΌνλΌλ―Έν„° μ΅°ν•©μ„ μ°Ύμ•„μ•Ό ν•©λ‹λ‹¤.
2.  **κµμ°¨κ²€μ¦**: λ¨λΈμ μΌλ°ν™” μ„±λ¥μ„ μ‹ λΆ°μ„± μκ² μΈ΅μ •ν•κΈ° μ„ν•΄ κµμ°¨ κ²€μ¦(Cross-Validation)μ„ ν•„μμ μΌλ΅ μ‚¬μ©ν•΄μ•Ό ν•©λ‹λ‹¤. μ΄λ” νΉμ • λ°μ΄ν„° λ¶„ν• μ— λ€ν• κ³Όμ ν•©μ„ λ°©μ§€ν•κ³ , λ¨λΈ μ„±λ¥μ μ•μ •μ μΈ μ¶”μ •μΉλ¥Ό μ κ³µν•©λ‹λ‹¤.
3.  **μ΅°κΈ° μΆ…λ£ (Pruning)**: νΉν `Optuna`μ™€ κ°™μ€ λ² μ΄μ§€μ• μµμ ν™” λ„κµ¬μ—μ„ μ κ³µν•λ” μ΅°κΈ° μΆ…λ£ κΈ°λ¥μ„ ν™μ©ν•μ—¬, μ„±λ¥μ΄ μΆ‹μ§€ μ•μ€ μ‹λ„λ¥Ό μ΅°κΈ°μ— μ¤‘λ‹¨ν•¨μΌλ΅μ¨ λ¶ν•„μ”ν• κ³„μ‚° μμ› λ‚­λΉ„λ¥Ό μ¤„μ΄κ³  μµμ ν™” μ‹κ°„μ„ λ‹¨μ¶•ν•  μ μμµλ‹λ‹¤.
4.  **ν†µκ³„μ  κ²€μ¦**: λ¨λΈ μ„±λ¥ κ°μ„ μ΄ μ°μ—°μ— μν• κ²ƒμΈμ§€, μ•„λ‹λ©΄ ν†µκ³„μ μΌλ΅ μ μλ―Έν• μ°¨μ΄μΈμ§€λ¥Ό `t-test`μ™€ κ°™μ€ ν†µκ³„μ  κ²€μ¦ λ°©λ²•μ„ ν†µν•΄ ν™•μΈν•΄μ•Ό ν•©λ‹λ‹¤. μ΄λ” λ¨λΈ κ°μ„ μ μ‹ λΆ°μ„±μ„ λ†’μ…λ‹λ‹¤.

#### ν•µμ‹¬: Pipeline ν™μ©μ ν•µμ‹¬ κ°€μΉ

`Scikit-learn Pipeline`μ€ λ¨Έμ‹ λ¬λ‹ μ›ν¬ν”λ΅μ°λ¥Ό μ²΄κ³„ν™”ν•κ³  ν¨μ¨μ„±μ„ λ†’μ΄λ” λ° ν•µμ‹¬μ μΈ μ—­ν• μ„ ν•©λ‹λ‹¤.

1.  **μΌκ΄€μ„±**: λ°μ΄ν„° μ „μ²λ¦¬λ¶€ν„° λ¨λΈ ν•™μµ λ° μμΈ΅μ— μ΄λ¥΄λ” λ¨λ“  κ³Όμ •μ„ ν•λ‚μ νμ΄ν”„λΌμΈμΌλ΅ ν†µν•©ν•μ—¬, ν›λ ¨κ³Ό μμΈ΅ μ‹ λ™μΌν• μ „μ²λ¦¬ μμ„μ™€ νλΌλ―Έν„°κ°€ μ μ©λλ„λ΅ λ³΄μ¥ν•©λ‹λ‹¤.
2.  **μ•μ „μ„±**: ν›λ ¨ λ°μ΄ν„°μ— λ€ν•΄μ„λ§ `fit()`μ„ μν–‰ν•κ³  ν…μ¤νΈ λ°μ΄ν„°μ—λ” `transform()`λ§ μ μ©ν•λ„λ΅ κ°•μ ν•¨μΌλ΅μ¨, ν…μ¤νΈ λ°μ΄ν„°μ μ •λ³΄κ°€ ν›λ ¨ κ³Όμ •μ— μ μ¶λλ” **λ°μ΄ν„° λ„μ(Data Leakage)**λ¥Ό ν¨κ³Όμ μΌλ΅ λ°©μ§€ν•©λ‹λ‹¤.
3.  **μ¬μ‚¬μ©μ„±**: ν• λ² μ •μλ νμ΄ν”„λΌμΈμ€ λ‹¤λ¥Έ λ°μ΄ν„°μ…‹μ΄λ‚ μƒλ΅μ΄ μμΈ΅ μƒν™©μ—μ„λ„ μ‰½κ² μ¬μ‚¬μ©ν•  μ μμ–΄ μ½”λ“μ λ¨λ“ν™”μ™€ μ μ§€λ³΄μμ„±μ„ ν¬κ² ν–¥μƒμ‹ν‚µλ‹λ‹¤.
4.  **μ μ§€λ³΄μμ„±**: λ³µμ΅ν• λ¨Έμ‹ λ¬λ‹ μ›ν¬ν”λ΅μ°λ¥Ό μ²΄κ³„μ μΌλ΅ κ΄€λ¦¬ν•κ³ , κ° λ‹¨κ³„μ λ³€κ²½μ΄ λ‹¤λ¥Έ λ‹¨κ³„μ— λ―ΈμΉλ” μν–¥μ„ μ‰½κ² νμ•…ν•  μ μκ² ν•©λ‹λ‹¤.

#### λΉ„κµ: λ°©λ²•λ΅  μ„ νƒ κΈ°μ¤€

ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•λ΅ μ„ μ„ νƒν•  λ•λ” λ¬Έμ μ λ³µμ΅λ„, νƒμƒ‰ κ³µκ°„μ ν¬κΈ°, μ‹κ°„ μ μ•½, μ¬ν„μ„± μ”κµ¬μ‚¬ν•­ λ“±μ„ μΆ…ν•©μ μΌλ΅ κ³ λ ¤ν•΄μ•Ό ν•©λ‹λ‹¤.

```python
# μμ‚¬κ²°μ • νΈλ¦¬ (μμ‹: μµμ ν™” λ°©λ²• μ„ νƒ λ΅μ§)
# problem_complexity, search_space, time_constraint, reproducibilityλ” μ™Έλ¶€μ—μ„ μ •μλ λ³€μλΌκ³  κ°€μ •

 if problem_complexity == "simple":
     # κ°„λ‹¨ν• λ¬Έμ μ κ²½μ°, κΈ°λ³Έ PipelineμΌλ΅ λ² μ΄μ¤λΌμΈ κµ¬μ¶• ν›„ μλ™ νλ‹ λλ” μ‘μ€ GridSearchCV
     use_basic_pipeline()
 elif search_space < 100:
     # νƒμƒ‰ κ³µκ°„μ΄ μ‘μ„ κ²½μ°, GridSearchCVκ°€ ν¨μ¨μ μ΄κ³  μ¬ν„μ„± λ†’μ
     use_gridsearch()
 elif time_constraint == "strict":
     # μ‹κ°„ μ μ•½μ΄ μ—„κ²©ν•  κ²½μ°, Optunaμ Pruning κΈ°λ¥ ν™μ©
     use_optuna_with_pruning()
 else:
     # νƒμƒ‰ κ³µκ°„μ΄ ν¬κ³  μ‹κ°„ μ μ•½μ΄ μ μ—°ν•  κ²½μ°, Optunaμ μ§€λ¥μ  νƒμƒ‰ ν™μ©
     use_optuna_full()
```

### 6.2 μ‹¤λ¬΄ μ μ© Best Practices

λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈμ μ„±κ³µμ μΈ μν–‰μ„ μ„ν•΄μ„λ” μ²΄κ³„μ μΈ μ ‘κ·Ό λ°©μ‹κ³Ό λ¨λ²” μ‚¬λ΅€(Best Practices)λ¥Ό λ”°λ¥΄λ” κ²ƒμ΄ μ¤‘μ”ν•©λ‹λ‹¤. λ‹¤μμ€ ν”„λ΅μ νΈμ κ° λ‹¨κ³„μ—μ„ κ³ λ ¤ν•΄μ•Ό ν•  ν•µμ‹¬ μ‚¬ν•­λ“¤μ…λ‹λ‹¤.

#### Best ν”„λ΅μ νΈ μ›ν¬ν”λ΅μ°

λ¨Έμ‹ λ¬λ‹ ν”„λ΅μ νΈλ” μ„ ν•μ μΈ κ³Όμ •μ΄ μ•„λ‹λΌ λ°λ³µμ μΈ μ›ν¬ν”λ΅μ°λ¥Ό κ°€μ§‘λ‹λ‹¤. λ‹¤μμ€ μΌλ°μ μΈ ν”„λ΅μ νΈ νλ¦„μ„ μ‹κ°ν™”ν• κ²ƒμ…λ‹λ‹¤.

```mermaid
graph TD
    A[λ°μ΄ν„° νƒμƒ‰ λ° μ΄ν•΄] --> B[κΈ°λ³Έ Pipeline κµ¬μ¶• λ° λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ •]
    B --> C{μ„±λ¥ κ°μ„  ν•„μ”?}
    C -- Yes --> D[μµμ ν™” λ°©λ²• μ„ νƒ]
    D --> E[ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”]
    E --> F[μ„±λ¥ κ²€μ¦ λ° λ¨λΈ μ„ νƒ]
    F -- Yes --> G[μµμΆ… λ¨λΈ λ°°ν¬]
    F -- No --> B
    G --> H[μ„±λ¥ λ¨λ‹ν„°λ§ λ° μ¬ν•™μµ]
```

-   **λ°μ΄ν„° νƒμƒ‰ λ° μ΄ν•΄**: λ¬Έμ  μ •μ, λ°μ΄ν„° μμ§‘, EDA(νƒμƒ‰μ  λ°μ΄ν„° λ¶„μ„)λ¥Ό ν†µν•΄ λ°μ΄ν„°λ¥Ό κΉμ΄ μ΄ν•΄ν•©λ‹λ‹¤.
-   **κΈ°λ³Έ Pipeline κµ¬μ¶• λ° λ² μ΄μ¤λΌμΈ μ„±λ¥ μΈ΅μ •**: κ°„λ‹¨ν• μ „μ²λ¦¬ λ° λ¨λΈλ΅ λ² μ΄μ¤λΌμΈ μ„±λ¥μ„ λΉ λ¥΄κ² μΈ΅μ •ν•μ—¬ λ¬Έμ μ λ‚μ΄λ„λ¥Ό νμ•…ν•©λ‹λ‹¤.
-   **μµμ ν™” λ°©λ²• μ„ νƒ**: λ² μ΄μ¤λΌμΈ μ„±λ¥μ„ κ°μ„ ν•κΈ° μ„ν•΄ `GridSearchCV`, `Optuna` λ“± μ μ ν• ν•μ΄νΌνλΌλ―Έν„° μµμ ν™” λ°©λ²•μ„ μ„ νƒν•©λ‹λ‹¤.
-   **ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”**: μ„ νƒλ λ°©λ²•μΌλ΅ λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό νλ‹ν•μ—¬ μ„±λ¥μ„ ν–¥μƒμ‹ν‚µλ‹λ‹¤.
-   **μ„±λ¥ κ²€μ¦ λ° λ¨λΈ μ„ νƒ**: μµμ ν™”λ λ¨λΈμ μ„±λ¥μ„ λ…λ¦½μ μΈ ν…μ¤νΈμ…‹μΌλ΅ κ²€μ¦ν•κ³ , λΉ„μ¦λ‹μ¤ λ©ν‘μ— κ°€μ¥ μ ν•©ν• λ¨λΈμ„ μ„ νƒν•©λ‹λ‹¤.
-   **μµμΆ… λ¨λΈ λ°°ν¬**: κ²€μ¦λ λ¨λΈμ„ μ‹¤μ  μ„λΉ„μ¤ ν™κ²½μ— λ°°ν¬ν•©λ‹λ‹¤.
-   **μ„±λ¥ λ¨λ‹ν„°λ§ λ° μ¬ν•™μµ**: λ°°ν¬λ λ¨λΈμ μ„±λ¥μ„ μ§€μ†μ μΌλ΅ λ¨λ‹ν„°λ§ν•κ³ , μ„±λ¥ μ €ν• μ‹ μ¬ν•™μµμ„ ν†µν•΄ λ¨λΈμ„ μ—…λ°μ΄νΈν•©λ‹λ‹¤.

#### μ²΄ν¬λ¦¬μ¤νΈ

ν”„λ΅μ νΈμ κ° λ‹¨κ³„μ—μ„ λ†“μΉμ§€ λ§μ•„μ•Ό ν•  ν•µμ‹¬ μ‚¬ν•­λ“¤μ„ μ²΄ν¬λ¦¬μ¤νΈ ν•νƒλ΅ μ •λ¦¬ν–μµλ‹λ‹¤.

**λ°μ΄ν„° μ¤€λΉ„:**

-   [ ] **κ²°μΈ΅κ°’ μ²λ¦¬**: `df.isnull().sum()`μΌλ΅ κ²°μΈ΅κ°’ ν„ν™©μ„ νμ•…ν•κ³ , μ κ±°(`dropna()`) λλ” μ μ ν• λ€μΉ λ°©λ²•(`fillna()`, `SimpleImputer`, `KNNImputer`)μ„ μ„ νƒν•μ—¬ μ²λ¦¬ν–λ”κ°€?
-   [ ] **μ΄μƒκ°’ νƒμ§€**: λ°•μ¤ν”λ΅―, IQR(μ‚¬λ¶„μ„ λ²”μ„), Z-score λ“± ν†µκ³„μ /μ‹κ°μ  λ°©λ²•μ„ ν™μ©ν•μ—¬ μ΄μƒκ°’μ„ νƒμ§€ν•κ³ , μ κ±°, λ³€ν™, λλ” λ€μΉν•λ” μ „λµμ„ μλ¦½ν–λ”κ°€?
-   [ ] **λ²”μ£Όν• μΈμ½”λ”©**: λ¬Έμμ—΄ λ²”μ£Όν• λ°μ΄ν„°λ” `pd.get_dummies()` λλ” `OneHotEncoder`λ΅, μμ„κ°€ μλ” λ²”μ£Όν• λ°μ΄ν„°λ” `LabelEncoder` λλ” `OrdinalEncoder`λ΅ μƒν™©μ— λ§λ” λ°©λ²•μ„ μ„ νƒν•μ—¬ μΈμ½”λ”©ν–λ”κ°€?
-   [ ] **μμΉν• μ¤μΌ€μΌλ§**: νΉμ„±λ“¤μ μ¤μΌ€μΌ μ°¨μ΄κ°€ λ¨λΈ μ„±λ¥μ— μν–¥μ„ λ―ΈμΉ  κ²½μ°, `StandardScaler`, `MinMaxScaler`, `RobustScaler` λ“± μ μ ν• μ¤μΌ€μΌλ¬λ¥Ό μ μ©ν–λ”κ°€? (νΉν κ±°λ¦¬ κΈ°λ° λ¨λΈ, κ²½μ‚¬ν•κ°•λ²• κΈ°λ° λ¨λΈ)
-   [ ] **νΉμ„± μ„ νƒ/μƒμ„±**: λ„λ©”μΈ μ§€μ‹, μƒκ΄€κ΄€κ³„ λ¶„μ„, νΉμ„± μ¤‘μ”λ„ λ“±μ„ κΈ°λ°μΌλ΅ λ¶ν•„μ”ν• νΉμ„±μ„ μ κ±°ν•κ±°λ‚ μƒλ΅μ΄ μ μ©ν• νΉμ„±μ„ μƒμ„±(Feature Engineering)ν–λ”κ°€?
-   [ ] **λ°μ΄ν„° λ¶„ν• **: ν›λ ¨(Train), κ²€μ¦(Validation), ν…μ¤νΈ(Test) μ„ΈνΈλ¥Ό μ μ ν• λΉ„μ¨λ΅ λ¶„ν• ν–μΌλ©°, λ¶„λ¥ λ¬Έμ μ κ²½μ° `stratify` μµμ…μ„ μ‚¬μ©ν•μ—¬ ν΄λμ¤ λ¶κ· ν•μ„ κ³ λ ¤ν–λ”κ°€? (λ°μ΄ν„° λ„μ λ°©μ§€λ¥Ό μ„ν•΄ μ „μ²λ¦¬ μ „μ— λ¶„ν• )

**λ¨λΈλ§:**

-   [ ] **κΈ°μ¤€ λ¨λΈ (Baseline Model)**: λ³µμ΅ν• λ¨λΈμ„ κµ¬μ¶•ν•κΈ° μ „μ— κ°„λ‹¨ν• λ¨λΈ(μ: λ΅μ§€μ¤ν‹± νκ·€, μμ‚¬κ²°μ •λ‚λ¬΄)λ΅ λ² μ΄μ¤λΌμΈ μ„±λ¥μ„ μΈ΅μ •ν•μ—¬ λ¬Έμ μ λ‚μ΄λ„λ¥Ό νμ•…ν•κ³ , μ΄ν›„ λ¨λΈ κ°μ„ μ κΈ°μ¤€μ μΌλ΅ μ‚Όμ•λ”κ°€?
-   [ ] **κµμ°¨κ²€μ¦**: λ¨λΈμ μΌλ°ν™” μ„±λ¥μ„ μ‹ λΆ°μ„± μκ² ν‰κ°€ν•κΈ° μ„ν•΄ `KFold`, `StratifiedKFold` λ“± μ μ ν• κµμ°¨κ²€μ¦(CV) μ „λµμ„ μ„¤μ •ν–λ”κ°€?
-   [ ] **ν•μ΄νΌνλΌλ―Έν„° νƒμƒ‰ κ³µκ°„ μ •μ**: κ° λ¨λΈμ ν•µμ‹¬ ν•μ΄νΌνλΌλ―Έν„°μ— λ€ν•΄ ν•©λ¦¬μ μΈ νƒμƒ‰ κ³µκ°„(κ·Έλ¦¬λ“ λλ” λ¶„ν¬)μ„ μ •μν–λ”κ°€?
-   [ ] **κ³Όμ ν•© ν™•μΈ**: ν›λ ¨ μ„±λ¥κ³Ό κ²€μ¦/ν…μ¤νΈ μ„±λ¥ κ°„μ μ°¨μ΄λ¥Ό μ§€μ†μ μΌλ΅ λ¨λ‹ν„°λ§ν•μ—¬ κ³Όμ ν•© μ—¬λ¶€λ¥Ό ν™•μΈν•κ³ , ν•„μ”μ‹ λ°©μ§€ μ „λµμ„ μ μ©ν–λ”κ°€?
-   [ ] **λ‹¤μ¤‘ λ¨λΈ λΉ„κµ**: λ¬Έμ μ— κ°€μ¥ μ ν•©ν• λ¨λΈμ„ μ°ΎκΈ° μ„ν•΄ μ—¬λ¬ μ•κ³ λ¦¬μ¦(SVM, RandomForest, GradientBoosting λ“±)μ„ λΉ„κµν•κ³  ν‰κ°€ν–λ”κ°€?
-   [ ] **μ•™μƒλΈ”**: λ‹¨μΌ λ¨λΈμ ν•κ³„λ¥Ό κ·Ήλ³µν•κ³  μ„±λ¥ ν–¥μƒμ„ μ„ν•΄ `Voting`, `Bagging`, `Stacking` λ“± μ•™μƒλΈ” κΈ°λ²•μ„ κ³ λ ¤ν–λ”κ°€?

**μµμ ν™”:**

-   [ ] **μµμ ν™” λ°©λ²• μ„ νƒ**: νƒμƒ‰ κ³µκ°„μ ν¬κΈ°, μ‹κ°„ μ μ•½, μ¬ν„μ„± μ”κµ¬μ‚¬ν•­ λ“±μ„ κ³ λ ¤ν•μ—¬ `GridSearchCV`, `RandomizedSearchCV`, `Bayesian Optimization (Optuna)` μ¤‘ μ μ ν• μµμ ν™” λ°©λ²•μ„ μ„ νƒν–λ”κ°€?
-   [ ] **μ‹κ°„/μμ› μ μ•½ κ³ λ ¤**: μµμ ν™” κ³Όμ •μ—μ„ λ°μƒν•λ” κ³„μ‚° λΉ„μ©κ³Ό μ‹κ°„μ„ μμΈ΅ν•κ³ , `n_jobs`, `Pruning` λ“± ν¨μ¨μ μΈ μμ› ν™μ© μ „λµμ„ μ μ©ν–λ”κ°€?
-   [ ] **μ§„ν–‰μƒν™© λ¨λ‹ν„°λ§**: μµμ ν™” κ³Όμ •μ μ§„ν–‰μƒν™©κ³Ό μ¤‘κ°„ κ²°κ³Όλ¥Ό μ£ΌκΈ°μ μΌλ΅ λ¨λ‹ν„°λ§ν•μ—¬ λΉ„ν¨μ¨μ μΈ νƒμƒ‰μ„ μ΅°κΈ°μ— κ°μ§€ν–λ”κ°€?
-   [ ] **κ²°κ³Ό ν•΄μ„ λ° κ²€μ¦**: μµμ ν™” κ²°κ³Όλ¥Ό λ‹¨μν μ«μλ΅λ§ λ³΄μ§€ μ•κ³ , ν•μ΄νΌνλΌλ―Έν„° μ¤‘μ”λ„ λ¶„μ„, ν†µκ³„μ /μ‹¤μ©μ  μ μμ„± κ²€μ¦ λ“±μ„ ν†µν•΄ κΉμ΄ μκ² ν•΄μ„ν•κ³  κ²€μ¦ν–λ”κ°€?

**λ°°ν¬:**

-   [ ] **μµμΆ… μ„±λ¥ ν™•μΈ**: λ°°ν¬ μ „ λ…λ¦½μ μΈ ν…μ¤νΈ λ°μ΄ν„°μ…‹μΌλ΅ λ¨λΈμ μµμΆ… μ„±λ¥μ„ λ‹¤μ‹ ν•λ² ν™•μΈν–λ”κ°€?
-   [ ] **λ¨λΈ μ €μ¥ λ° λ²„μ „ κ΄€λ¦¬**: ν•™μµλ μµμΆ… λ¨λΈμ„ `joblib`λ‚ `pickle` λ“±μΌλ΅ μ €μ¥ν•κ³ , λ¨λΈμ λ²„μ „ κ΄€λ¦¬λ¥Ό ν†µν•΄ μ¬ν„μ„±κ³Ό μ¶”μ  κ°€λ¥μ„±μ„ ν™•λ³΄ν–λ”κ°€?
-   [ ] **μ¶”λ΅  νμ΄ν”„λΌμΈ κµ¬μ¶•**: μ‹¤μ  μ„λΉ„μ¤ ν™κ²½μ—μ„ μƒλ΅μ΄ λ°μ΄ν„°κ°€ λ“¤μ–΄μ™”μ„ λ•, ν›λ ¨ μ‹μ™€ λ™μΌν• μ „μ²λ¦¬ λ° μμΈ΅ κ³Όμ •μ„ μν–‰ν•λ” μ¶”λ΅  νμ΄ν”„λΌμΈμ„ κµ¬μ¶•ν–λ”κ°€?
-   [ ] **μ„±λ¥ λ¨λ‹ν„°λ§ μ„¤μ •**: λ°°ν¬λ λ¨λΈμ μ„±λ¥μ„ μ§€μ†μ μΌλ΅ λ¨λ‹ν„°λ§ν•κ³ , μ„±λ¥ μ €ν• μ‹ μ•λ¦Όμ„ λ°›μ„ μ μλ” μ‹μ¤ν…μ„ κµ¬μ¶•ν–λ”κ°€? (λ°μ΄ν„° λ“λ¦¬ν”„νΈ, λ¨λΈ λ“λ¦¬ν”„νΈ κ°μ§€)

### 6.3 ν–¥ν›„ ν•™μµ λ°©ν–¥

λ¨Έμ‹ λ¬λ‹ λ¶„μ•Όλ” λμ„μ—†μ΄ λ°μ „ν•κ³  μμΌλ©°, μƒλ΅μ΄ κΈ°μ κ³Ό κΈ°λ²•λ“¤μ΄ λΉ λ¥΄κ² λ“±μ¥ν•κ³  μμµλ‹λ‹¤. μ§€κΈκΉμ§€ ν•™μµν• λ‚΄μ©μ„ λ°”νƒ•μΌλ΅, λ‹¤μ λ‹¨κ³„λ΅ λ‚μ•„κ°€κΈ° μ„ν• μ¶”μ² ν•™μµ λ°©ν–¥μ„ μ μ‹ν•©λ‹λ‹¤.

#### μ¶”μ² κ³ κΈ‰ μ£Όμ 

1.  **λ©€ν‹° μ¤λΈμ ν‹°λΈ μµμ ν™” (Multi-Objective Optimization)**:
    -   λ‹¨μν ν•λ‚μ μ§€ν‘(μ: μ •ν™•λ„)λ§μ„ μµμ ν™”ν•λ” κ²ƒμ„ λ„μ–΄, μ—¬λ¬ μ§€ν‘(μ: μ •ν™•λ„μ™€ λ¨λΈ ν¬κΈ°, μ •ν™•λ„μ™€ μ¶”λ΅  μ†λ„)λ¥Ό λ™μ‹μ— μµμ ν™”ν•λ” λ°©λ²•μ…λ‹λ‹¤.
    -   **Pareto μµμ ν™”**: μ—¬λ¬ λ©ν‘λ¥Ό λ™μ‹μ— λ§μ΅±μ‹ν‚¤λ” μµμ ν•΄ μ§‘ν•©(Pareto Front)μ„ μ°Ύλ” κ°λ…μ„ ν•™μµν•©λ‹λ‹¤.

2.  **AutoML ν†µν•© (Automated Machine Learning)**:
    -   λ°μ΄ν„° μ „μ²λ¦¬, νΉμ„± κ³µν•™, λ¨λΈ μ„ νƒ, ν•μ΄νΌνλΌλ―Έν„° νλ‹ λ“± λ¨Έμ‹ λ¬λ‹ μ›ν¬ν”λ΅μ°μ μ—¬λ¬ λ‹¨κ³„λ¥Ό μλ™ν™”ν•λ” AutoML λ„κµ¬λ“¤μ„ μ‹¬μΈµμ μΌλ΅ ν•™μµν•κ³  ν™μ©ν•©λ‹λ‹¤.
    -   **TPOT**: μ μ „μ μ•κ³ λ¦¬μ¦(Genetic Algorithm)μ„ μ‚¬μ©ν•μ—¬ μµμ μ λ¨Έμ‹ λ¬λ‹ νμ΄ν”„λΌμΈμ„ νƒμƒ‰ν•©λ‹λ‹¤.
    -   **Auto-sklearn**: Scikit-learn κΈ°λ°μ AutoML λΌμ΄λΈλ¬λ¦¬λ΅, λ¨λΈ μ„ νƒ λ° ν•μ΄νΌνλΌλ―Έν„° νλ‹μ„ μλ™ν™”ν•©λ‹λ‹¤.

3.  **λ¶„μ‚° μµμ ν™” (Distributed Optimization)**:
    -   λ€κ·λ¨ λ°μ΄ν„°μ…‹μ΄λ‚ λ³µμ΅ν• λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„° μµμ ν™”λ¥Ό μ„ν•΄ μ—¬λ¬ μ»΄ν“¨ν… μμ›(ν΄λ¬μ¤ν„°)μ„ ν™μ©ν•λ” λ¶„μ‚° μµμ ν™” κΈ°λ²•μ„ ν•™μµν•©λ‹λ‹¤.
    -   **Optuna λ¶„μ‚° μ‹¤ν–‰**: `Optuna`λ¥Ό λ¶„μ‚° ν™κ²½μ—μ„ μ‹¤ν–‰ν•μ—¬ μµμ ν™” μ†λ„λ¥Ό λ†’μ΄λ” λ°©λ²•μ„ νƒκµ¬ν•©λ‹λ‹¤.

4.  **κ³ κΈ‰ Pipeline κµ¬μ„± λ° μ»¤μ¤ν…€ Transformer κ°λ°**:
    -   `ColumnTransformer`λ¥Ό ν™μ©ν•μ—¬ λ”μ± λ³µμ΅ν•κ³  μ μ—°ν• μ „μ²λ¦¬ νμ΄ν”„λΌμΈμ„ κµ¬μ„±ν•λ” λ°©λ²•μ„ μ‹¬ν™” ν•™μµν•©λ‹λ‹¤.
    -   `BaseEstimator`μ™€ `TransformerMixin`μ„ μƒμ†λ°›μ•„ μμ‹ λ§μ μ»¤μ¤ν…€ λ³€ν™κΈ°(Custom Transformer)λ¥Ό κ°λ°ν•μ—¬ νΉμ • λ¬Έμ μ— μµμ ν™”λ μ „μ²λ¦¬ λ‹¨κ³„λ¥Ό νμ΄ν”„λΌμΈμ— ν†µν•©ν•λ” λ°©λ²•μ„ μµν™λ‹λ‹¤.

#### μ¶”μ² μλ£

**κ³µμ‹ λ¬Έμ„:**

-   [Optuna Documentation](https://optuna.readthedocs.io/): Optunaμ λ¨λ“  κΈ°λ¥κ³Ό μμ λ¥Ό μƒμ„Έν μ„¤λ…ν•λ” κ³µμ‹ λ¬Έμ„μ…λ‹λ‹¤.
-   [Scikit-learn Pipeline Guide](https://scikit-learn.org/stable/modules/pipeline.html): Scikit-learn Pipelineμ μ‚¬μ©λ²•κ³Ό κ³ κΈ‰ ν™μ©λ²•μ„ λ‹¤λ£¨λ” κ³µμ‹ κ°€μ΄λ“μ…λ‹λ‹¤.

**μ‹¬ν™” ν•™μµ:**

-   **Bayesian Optimization μ΄λ΅ **: λ² μ΄μ§€μ• μµμ ν™”μ μν•™μ  λ°°κ²½κ³Ό λ‹¤μ–‘ν• νλ“ ν•¨μμ— λ€ν• μ‹¬μΈµμ μΈ μ΄ν•΄λ¥Ό μ„ν• μλ£λ¥Ό νƒκµ¬ν•©λ‹λ‹¤.
-   **Hyperparameter Tuning μ „λ¬Έμ„**: ν•μ΄νΌνλΌλ―Έν„° νλ‹μ— λ€ν• μ „λ¬Έ μ„μ μ„ ν†µν•΄ λ‹¤μ–‘ν• κΈ°λ²•κ³Ό μ‹¤μ „ λ…Έν•μ°λ¥Ό μµλ“ν•©λ‹λ‹¤.
-   **MLOps νμ΄ν”„λΌμΈ κµ¬μ¶•**: λ¨λΈ κ°λ°λ¶€ν„° λ°°ν¬, λ¨λ‹ν„°λ§, μ¬ν•™μµμ— μ΄λ¥΄λ” λ¨Έμ‹ λ¬λ‹ μ΄μ(MLOps)μ μ „ κ³Όμ •μ„ μλ™ν™”ν•λ” νμ΄ν”„λΌμΈ κµ¬μ¶• κΈ°μ μ„ ν•™μµν•©λ‹λ‹¤.

#### μ‹¤μµ ν”λ«νΌ

-   **Kaggle**: λ‹¤μ–‘ν• λ¨Έμ‹ λ¬λ‹ κ²½μ§„λ€νμ— μ°Έμ—¬ν•μ—¬ μ‹¤μ „ κ²½ν—μ„ μ“κ³ , κ³µκ°λ μ½”λ“μ™€ λ°μ΄ν„°μ…‹μ„ ν†µν•΄ ν•™μµν•©λ‹λ‹¤.
-   **Google Colab**: λ¬΄λ£ GPU ν™κ²½μ„ μ κ³µν•μ—¬ λ”¥λ¬λ‹ λ¨λΈ ν•™μµ λ° λ€κ·λ¨ λ°μ΄ν„° μ²λ¦¬μ— μ μ©ν•©λ‹λ‹¤.
-   **Papers with Code**: μµμ‹  λ¨Έμ‹ λ¬λ‹ μ—°κµ¬ λ…Όλ¬Έκ³Ό ν•΄λ‹Ή λ…Όλ¬Έμ μ½”λ“λ¥Ό ν•¨κ» μ κ³µν•μ—¬ μµμ‹  κΈ°μ  λ™ν–¥μ„ νμ•…ν•κ³  κµ¬ν„μ„ ν•™μµν•λ” λ° λ„μ›€μ„ μ¤λ‹λ‹¤.

#### λ„μ„ (μ¶”κ°€ μ¶”μ²)

-   "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow" - AurΓ©lien GΓ©ron: Scikit-learnκ³Ό λ”¥λ¬λ‹μ„ μ•„μ°λ¥΄λ” μ‹¤μ „ κ°€μ΄λ“λ¶μ…λ‹λ‹¤.
-   "The Elements of Statistical Learning" - Trevor Hastie, Robert Tibshirani, Jerome Friedman: ν†µκ³„ ν•™μµμ κ³ μ „μΌλ΅, λ¨Έμ‹ λ¬λ‹ μ•κ³ λ¦¬μ¦μ μ΄λ΅ μ  λ°°κ²½μ„ κΉμ΄ μκ² λ‹¤λ£Ήλ‹λ‹¤.
-   "Pattern Recognition and Machine Learning" - Christopher Bishop: ν¨ν„΄ μΈμ‹κ³Ό λ¨Έμ‹ λ¬λ‹μ κΈ°μ΄ μ΄λ΅ μ„ μƒμ„Έν μ„¤λ…ν•λ” κµκ³Όμ„μ…λ‹λ‹¤.

---

[β®οΈ μ΄μ „ λ¬Έμ„](./0714_MLμ •λ¦¬.md) | [λ‹¤μ λ¬Έμ„ β­οΈ](./0716_MLμ •λ¦¬.md) 