# ğŸ§  ë”¥ëŸ¬ë‹ ì‹¤ì „: ì´ì§„/ë‹¤ì¤‘ ë¶„ë¥˜, íšŒê·€ ëª¨ë¸ êµ¬ì¶• ë° í‰ê°€, ìµœì í™” ì „ëµ

> ë³¸ ë¬¸ì„œëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í™œìš©í•œ ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ ë¶„ë¥˜, íšŒê·€ ë¬¸ì œ í•´ê²° ê³¼ì •ì„ ì‹¬ì¸µì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ ì„¤ê³„, í•™ìŠµ, ê·¸ë¦¬ê³  ì„±ëŠ¥ í‰ê°€ ë° ìµœì í™”ì— ì´ë¥´ëŠ” ì „ë°˜ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤ì œ ë°ì´í„°ì…‹(ê½ƒ, ì“°ë ˆê¸°, ì£¼íƒ ê°€ê²©)ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ê° ë¬¸ì œ ìœ í˜•ë³„ í•µì‹¬ ê°œë…, ì½”ë“œ ì˜ˆì‹œ, ê·¸ë¦¬ê³  ì‹¤ë¬´ ì ìš© ê°€ì´ë“œë¥¼ í†µí•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ ê°œë°œ ì—­ëŸ‰ì„ ê°•í™”í•˜ëŠ” ë° ê¸°ì—¬í•©ë‹ˆë‹¤.

---

## ëª©ì°¨

1.  [ë”¥ëŸ¬ë‹ ê¸°ë³¸ ê°œë…](#1-ë”¥ëŸ¬ë‹-ê¸°ë³¸-ê°œë…)
    *   [1.1 ë”¥ëŸ¬ë‹ì´ë€?](#11-ë”¥ëŸ¬ë‹ì´ë€)
    *   [1.2 ë¬¸ì œ ìœ í˜•ë³„ ë¶„ë¥˜](#12-ë¬¸ì œ-ìœ í˜•ë³„-ë¶„ë¥˜)
    *   [1.3 TensorFlow/Keras ê¸°ë³¸ êµ¬ì¡°](#13-tensorflowkeras-ê¸°ë³¸-êµ¬ì¡°)
2.  [ë°ì´í„° ì „ì²˜ë¦¬](#2-ë°ì´í„°-ì „ì²˜ë¦¬)
    *   [2.1 ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •](#21-ì´ë¯¸ì§€-ë°ì´í„°-ì „ì²˜ë¦¬-ê³¼ì •)
    *   [2.2 ìˆ˜ì¹˜ ë°ì´í„° ì •ê·œí™”](#22-ìˆ˜ì¹˜-ë°ì´í„°-ì •ê·œí™”)
    *   [2.3 ë°ì´í„° ë¶„í•  ì „ëµ](#23-ë°ì´í„°-ë¶„í• -ì „ëµ)
3.  [ì´ì§„ ë¶„ë¥˜ (Binary Classification)](#3-ì´ì§„-ë¶„ë¥˜-binary-classification)
    *   [3.1 ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„¤ê³„](#31-ì´ì§„-ë¶„ë¥˜-ëª¨ë¸-ì„¤ê³„)
    *   [3.2 í•µì‹¬ ì„¤ì • ìš”ì†Œ](#32-í•µì‹¬-ì„¤ì •-ìš”ì†Œ)
    *   [3.3 ëª¨ë¸ í›ˆë ¨ ë° ì½œë°±](#33-ëª¨ë¸-í›ˆë ¨-ë°-ì½œë°±)
    *   [3.4 ì„±ëŠ¥ ë¶„ì„](#34-ì„±ëŠ¥-ë¶„ì„)
4.  [ë‹¤ì¤‘ ë¶„ë¥˜ (Multi-class Classification)](#4-ë‹¤ì¤‘-ë¶„ë¥˜-multi-class-classification)
    *   [4.1 ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì„¤ê³„](#41-ë‹¤ì¤‘-ë¶„ë¥˜-ëª¨ë¸-ì„¤ê³„)
    *   [4.2 í´ë˜ìŠ¤ ì •ì˜ ë° ë§¤í•‘](#42-í´ë˜ìŠ¤-ì •ì˜-ë°-ë§¤í•‘)
    *   [4.3 ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬](#43-ë°ì´í„°-ë¶ˆê· í˜•-ì²˜ë¦¬)
    *   [4.4 ì„±ëŠ¥ í‰ê°€](#44-ì„±ëŠ¥-í‰ê°€)
5.  [íšŒê·€ ë¬¸ì œ (Regression)](#5-íšŒê·€-ë¬¸ì œ-regression)
    *   [5.1 íšŒê·€ ëª¨ë¸ ì„¤ê³„](#51-íšŒê·€-ëª¨ë¸-ì„¤ê³„)
    *   [5.2 íšŒê·€ ë¬¸ì œ íŠ¹ì„±](#52-íšŒê·€-ë¬¸ì œ-íŠ¹ì„±)
    *   [5.3 ë°ì´í„°ì…‹ ë¶„ì„ ë° ì¤€ë¹„ (California Housing)](#53-ë°ì´í„°ì…‹-ë¶„ì„-ë°-ì¤€ë¹„-california-housing)
    *   [5.4 ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ë¶„ì„](#54-ì„±ëŠ¥-í‰ê°€-ë°-ê²°ê³¼-ë¶„ì„)
6.  [ëª¨ë¸ í‰ê°€ ë° ìµœì í™”](#6-ëª¨ë¸-í‰ê°€-ë°-ìµœì í™”)
    *   [6.1 ì¢…í•© í‰ê°€ í”„ë ˆì„ì›Œí¬](#61-ì¢…í•©-í‰ê°€-í”„ë ˆì„ì›Œí¬)
    *   [6.2 ê³¼ì í•© ì§„ë‹¨ ë° í•´ê²°](#62-ê³¼ì í•©-ì§„ë‹¨-ë°-í•´ê²°)
    *   [6.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”](#63-í•˜ì´í¼íŒŒë¼ë¯¸í„°-ìµœì í™”)
    *   [6.4 ëª¨ë¸ ì•™ìƒë¸”](#64-ëª¨ë¸-ì•™ìƒë¸”)
7.  [ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ](#7-ì‹¤ë¬´-ì ìš©-ê°€ì´ë“œ)
    *   [7.1 í”„ë¡œì íŠ¸ ì›Œí¬í”Œë¡œìš°: ì•„ì´ë””ì–´ì—ì„œ ìš´ì˜ê¹Œì§€](#71-í”„ë¡œì íŠ¸-ì›Œí¬í”Œë¡œìš°-ì•„ì´ë””ì–´ì—ì„œ-ìš´ì˜ê¹Œì§€)
    *   [7.2 ë¬¸ì œ ìœ í˜•ë³„ ì ìš© ì‚¬ë¡€ì™€ í•µì‹¬ ê³¼ì œ](#72-ë¬¸ì œ-ìœ í˜•ë³„-ì ìš©-ì‚¬ë¡€ì™€-í•µì‹¬-ê³¼ì œ)
    *   [7.3 ë°°í¬ ë° ìš´ì˜ (MLOpsì˜ ì‹œì‘)](#73-ë°°í¬-ë°-ìš´ì˜-mlopsì˜-ì‹œì‘)
    *   [7.4 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§](#74-ì„±ëŠ¥-ëª¨ë‹ˆí„°ë§)
    *   [7.5 ìµœì‹  íŠ¸ë Œë“œ ë° ë°œì „ ë°©í–¥](#75-ìµœì‹ -íŠ¸ë Œë“œ-ë°-ë°œì „-ë°©í–¥)
8.  [í•™ìŠµ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„](#8-í•™ìŠµ-ì •ë¦¬-ë°-ë‹¤ìŒ-ë‹¨ê³„)
---

## 1. ë”¥ëŸ¬ë‹ ê¸°ë³¸ ê°œë…

### 1.1 ë”¥ëŸ¬ë‹ì´ë€?

**ë”¥ëŸ¬ë‹(Deep Learning)**ì€ ì¸ê³µì‹ ê²½ë§(Artificial Neural Network, ANN)ì„ ì—¬ëŸ¬ ì¸µ(Layer)ìœ¼ë¡œ ê¹Šê²Œ ìŒ“ì•„ ì˜¬ë ¤ ë°ì´í„° ë‚´ì˜ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤. 'ê¹Šë‹¤(Deep)'ëŠ” ê²ƒì€ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ(Hidden Layer)ì´ ì—¬ëŸ¬ ê°œë¼ëŠ” ì˜ë¯¸ì´ë©°, ì´ë¥¼ í†µí•´ ë°ì´í„°ì˜ ì¶”ìƒì ì¸ í‘œí˜„ì„ ê³„ì¸µì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### í•µì‹¬ íŠ¹ì§•

-   **ë‹¤ì¸µ ì‹ ê²½ë§**: ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ìµœì†Œí•œ í•˜ë‚˜ì˜ ì…ë ¥ì¸µ, í•˜ë‚˜ ì´ìƒì˜ ì€ë‹‰ì¸µ, ê·¸ë¦¬ê³  í•˜ë‚˜ì˜ ì¶œë ¥ì¸µìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ì¸µì˜ ë‰´ëŸ°(ë…¸ë“œ)ë“¤ì€ ì„œë¡œ ì—°ê²°ë˜ì–´ ì‹ í˜¸ë¥¼ ì „ë‹¬í•˜ë©°, ì´ ì—°ê²° ê°•ë„(ê°€ì¤‘ì¹˜)ë¥¼ í•™ìŠµì„ í†µí•´ ì¡°ì •í•©ë‹ˆë‹¤.
-   **ìë™ íŠ¹ì„± ì¶”ì¶œ (Automatic Feature Extraction)**: ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•ê³¼ ë‹¬ë¦¬, ë”¥ëŸ¬ë‹ì€ ë°ì´í„°ì—ì„œ ì§ì ‘ì ìœ¼ë¡œ ìœ ìš©í•œ íŠ¹ì„±(Feature)ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ëŠ” ì´ë¯¸ì§€, ìŒì„±, í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë¹„ì •í˜• ë°ì´í„° ì²˜ë¦¬ì—ì„œ í° ê°•ì ì´ë©°, ìˆ˜ë™ìœ¼ë¡œ íŠ¹ì„±ì„ ì„¤ê³„í•˜ëŠ” ë²ˆê±°ë¡œì›€ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.
-   **ë¹„ì„ í˜• ë³€í™˜**: ì‹ ê²½ë§ì˜ ê° ì¸µì— í™œì„±í™” í•¨ìˆ˜(Activation Function)ë¥¼ ì ìš©í•˜ì—¬ ë¹„ì„ í˜•ì ì¸ ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ë³µì¡í•œ ë¹„ì„ í˜• í•¨ìˆ˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ê³  í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
-   **ë²”ìš©ì„±**: ë”¥ëŸ¬ë‹ì€ ë¶„ë¥˜(Classification), íšŒê·€(Regression), êµ°ì§‘(Clustering)ê³¼ ê°™ì€ ì „í†µì ì¸ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¿ë§Œ ì•„ë‹ˆë¼, ì´ë¯¸ì§€ ìƒì„±, ìì—°ì–´ ë²ˆì—­, ìŒì„± ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë³µì¡í•œ ë¬¸ì œë“¤ì„ í•´ê²°í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 1.2 ë¬¸ì œ ìœ í˜•ë³„ ë¶„ë¥˜

ë”¥ëŸ¬ë‹ì€ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìœ¼ë©°, ë¬¸ì œì˜ íŠ¹ì„±ì— ë”°ë¼ ëª¨ë¸ì˜ ì¶œë ¥ì¸µ êµ¬ì¡°, í™œì„±í™” í•¨ìˆ˜, ì†ì‹¤ í•¨ìˆ˜ ë“±ì´ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë‹¤ìŒ í‘œëŠ” ì£¼ìš” ë¬¸ì œ ìœ í˜•ë³„ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ì„¤ê³„ íŒ¨í„´ì„ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤.

| ë¬¸ì œ ìœ í˜• | ì„¤ëª… | ì¶œë ¥ì¸µ ë‰´ëŸ° ìˆ˜ | ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ | ì†ì‹¤í•¨ìˆ˜ | ì‹¤ìŠµ ì˜ˆì œ |
|:---|:---|:---|:---|:---|:---|
| **ì´ì§„ ë¶„ë¥˜ (Binary Classification)** | ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ìƒí˜¸ ë°°íƒ€ì ì¸ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. (ì˜ˆ: ì°¸/ê±°ì§“, ìŠ¤íŒ¸/ì •ìƒ, ì•…ì„±/ì–‘ì„±) | 1ê°œ | `Sigmoid` | `Binary Crossentropy` | ê½ƒ ë¶„ë¥˜ (ë°ì´ì§€ vs ë¯¼ë“¤ë ˆ), ìœ ë°©ì•” ì§„ë‹¨ |
| **ë‹¤ì¤‘ ë¶„ë¥˜ (Multi-class Classification)** | ë°ì´í„°ë¥¼ ì„¸ ê°œ ì´ìƒì˜ ìƒí˜¸ ë°°íƒ€ì ì¸ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤. (ì˜ˆ: MNIST ìˆ«ì ë¶„ë¥˜, ë¶“ê½ƒ í’ˆì¢… ë¶„ë¥˜) | í´ë˜ìŠ¤ ìˆ˜ | `Softmax` | `Categorical Crossentropy` (ì›-í•« ì¸ì½”ë”© ë¼ë²¨) ë˜ëŠ” `Sparse Categorical Crossentropy` (ì •ìˆ˜ ë¼ë²¨) | ì“°ë ˆê¸° ë¶„ë¥˜ (6ê°œ í´ë˜ìŠ¤), MNIST ìˆ«ì ë¶„ë¥˜ |
| **íšŒê·€ (Regression)** | ì—°ì†ì ì¸ ìˆ˜ì¹˜ ê°’ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤. (ì˜ˆ: ì£¼íƒ ê°€ê²©, ì£¼ì‹ ê°€ê²©, ì˜¨ë„) | 1ê°œ (ë‹¨ì¼ ê°’ ì˜ˆì¸¡) ë˜ëŠ” ì˜ˆì¸¡í•  ê°’ì˜ ê°œìˆ˜ (ë‹¤ì¤‘ ê°’ ì˜ˆì¸¡) | `Linear` (í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ) | `Mean Squared Error` (MSE) ë˜ëŠ” `Mean Absolute Error` (MAE) | ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡ |

### 1.3 TensorFlow/Keras ê¸°ë³¸ êµ¬ì¡°

TensorFlowì™€ KerasëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë˜ëŠ” í•µì‹¬ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. Kerasì˜ `Sequential` APIëŠ” ì¸µ(Layer)ì„ ìˆœì„œëŒ€ë¡œ ìŒ“ì•„ ì˜¬ë¦¬ëŠ” ê°€ì¥ ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ëª¨ë¸ êµ¬ì„± ë°©ë²•ì…ë‹ˆë‹¤.

```python
# ìµœì‹  ê¶Œì¥ ë°©ì‹
import tensorflow as tf
from tensorflow.keras import models, layers

# 1. ëª¨ë¸ ìƒì„±: Sequential APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì¸µì„ ìˆœì„œëŒ€ë¡œ ìŒ“ì•„ ì˜¬ë¦½ë‹ˆë‹¤.
model = models.Sequential([
    # ì…ë ¥ì¸µ: Input ë ˆì´ì–´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì •ì˜í•˜ì—¬ ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    # input_shapeì€ (íŠ¹ì„± ìˆ˜,) í˜•íƒœë¡œ ì§€ì •í•˜ë©°, ì²« ë²ˆì§¸ ì°¨ì›(ë°°ì¹˜ í¬ê¸°)ì€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë©ë‹ˆë‹¤.
    layers.Input(shape=(input_dim,)),  # â­ ëª…ì‹œì  Input ë ˆì´ì–´ ì‚¬ìš© ê¶Œì¥
    
    # ì€ë‹‰ì¸µ: Dense (ì™„ì „ ì—°ê²°) ì¸µì„ ì¶”ê°€í•©ë‹ˆë‹¤. ê° ë‰´ëŸ°ì€ ì´ì „ ì¸µì˜ ëª¨ë“  ë‰´ëŸ°ê³¼ ì—°ê²°ë©ë‹ˆë‹¤.
    # units: í•´ë‹¹ ì¸µì˜ ë‰´ëŸ°(ë…¸ë“œ) ê°œìˆ˜
    # activation: í™œì„±í™” í•¨ìˆ˜ (ì˜ˆ: 'relu', 'tanh')
    layers.Dense(units, activation='relu'),
    
    # í•„ìš”ì— ë”°ë¼ ë” ë§ì€ ì€ë‹‰ì¸µì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # layers.Dense(units_2, activation='relu'),
    
    # ì¶œë ¥ì¸µ: ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” ë‰´ëŸ° ìˆ˜ì™€ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
    # output_units: ì¶œë ¥ ë‰´ëŸ°ì˜ ê°œìˆ˜ (ì´ì§„ ë¶„ë¥˜: 1, ë‹¤ì¤‘ ë¶„ë¥˜: í´ë˜ìŠ¤ ìˆ˜, íšŒê·€: 1 ë˜ëŠ” ì˜ˆì¸¡í•  ê°’ì˜ ê°œìˆ˜)
    # output_activation: ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ (ì´ì§„ ë¶„ë¥˜: 'sigmoid', ë‹¤ì¤‘ ë¶„ë¥˜: 'softmax', íšŒê·€: 'linear' ë˜ëŠ” ì—†ìŒ)
    layers.Dense(output_units, activation='output_activation')
])

# 2. ëª¨ë¸ ì»´íŒŒì¼: ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.
model.compile(
    optimizer='adam', # ì˜µí‹°ë§ˆì´ì €: ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ (ì˜ˆ: 'adam', 'rmsprop', 'sgd')
    loss='loss_function', # ì†ì‹¤ í•¨ìˆ˜: ëª¨ë¸ì˜ ì˜ˆì¸¡ê³¼ ì‹¤ì œ ì •ë‹µ ê°„ì˜ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ (ì˜ˆ: 'binary_crossentropy', 'categorical_crossentropy', 'mse')
    metrics=['metric'] # í‰ê°€ì§€í‘œ: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ëŠ” ì§€í‘œ (ì˜ˆ: ['accuracy'], ['mae'])
)

# ëª¨ë¸ ìš”ì•½ ì •ë³´ ì¶œë ¥ (ì¸µë³„ ì¶œë ¥ í˜•íƒœ, íŒŒë¼ë¯¸í„° ìˆ˜ ë“±)
model.summary()
```


---

## 2. ë°ì´í„° ì „ì²˜ë¦¬

### 2.1 ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •

ë”¥ëŸ¬ë‹ ëª¨ë¸, íŠ¹íˆ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì€ ì…ë ¥ ì´ë¯¸ì§€ì˜ í˜•íƒœì™€ í”½ì…€ ê°’ ë²”ìœ„ì— ë§¤ìš° ë¯¼ê°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ í•™ìŠµ ì „ì— ì´ë¯¸ì§€ë¥¼ ì¼ê´€ëœ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ì „ì²˜ë¦¬ ê³¼ì •ì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë‹¤ìŒì€ ê½ƒ ë¶„ë¥˜ ì‹¤ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ í‘œì¤€ ê³¼ì •ì…ë‹ˆë‹¤.

#### ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ê½ƒ ë¶„ë¥˜ ì‹¤ìŠµ ê¸°ë°˜)

```python
from PIL import Image
import numpy as np
import os

def process_image(image_path, target_size=(80, 80)):
    """ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼ì„ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì…ë ¥ì— ì í•©í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.

    Args:
        image_path (str): ì „ì²˜ë¦¬í•  ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ.
        target_size (tuple): ì´ë¯¸ì§€ì˜ ëª©í‘œ í¬ê¸° (width, height).

    Returns:
        np.array: ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ì˜ NumPy ë°°ì—´ (RGB, 0-255).
    """
    # 1. ì´ë¯¸ì§€ ë¡œë“œ: PIL (Pillow) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì—½ë‹ˆë‹¤.
    img = Image.open(image_path)
    
    # 2. RGB ë³€í™˜: ì´ë¯¸ì§€ê°€ RGB í˜•ì‹ì´ ì•„ë‹ ê²½ìš° (ì˜ˆ: í‘ë°±, RGBA), RGBë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ 3ì±„ë„ RGB ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
    if img.mode != 'RGB':
        img = img.convert('RGB')
    
    # 3. í¬ê¸° í†µì¼: ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ë™ì¼í•œ í¬ê¸°(target_size)ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.
    # ì´ëŠ” ì‹ ê²½ë§ì˜ ì…ë ¥ì¸µ í¬ê¸°ë¥¼ ê³ ì •í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.
    img_resized = img.resize(target_size)
    
    # 4. NumPy ë°°ì—´ ë³€í™˜: PIL Image ê°ì²´ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # í”½ì…€ ê°’ì€ 0-255 ë²”ìœ„ì˜ ì •ìˆ˜í˜•ìœ¼ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.
    pixel_array = np.array(img_resized)
    
    return pixel_array

# --- ì‚¬ìš© ì˜ˆì‹œ ---
# ê°€ìƒì˜ ì´ë¯¸ì§€ íŒŒì¼ ìƒì„± (ì‹¤ì œë¡œëŠ” ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆì–´ì•¼ í•¨)
 from PIL import ImageDraw
 dummy_img = Image.new('RGB', (100, 100), color = 'red')
 draw = ImageDraw.Draw(dummy_img)
 draw.text((20, 40), "Hello", fill=(0,0,0))
 dummy_img.save("dummy_image.jpg")

 if os.path.exists("dummy_image.jpg"):
     processed_img = process_image("dummy_image.jpg", target_size=(80, 80))
     print(f"ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í˜•íƒœ: {processed_img.shape}") # (80, 80, 3)
     print(f"ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í”½ì…€ ê°’ ë²”ìœ„: {processed_img.min()} ~ {processed_img.max()}")
 else:
     print("dummy_image.jpg íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
```

#### ë°ì´í„° ì €ì¥ ë° ë¡œë“œ (NPZ íŒŒì¼ í™œìš©)

ëŒ€ëŸ‰ì˜ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•œ í›„ì—ëŠ” ì´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” í˜•ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. NumPyì˜ `.npz` í˜•ì‹ì€ ì—¬ëŸ¬ NumPy ë°°ì—´ì„ í•˜ë‚˜ì˜ ì••ì¶•ëœ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆì–´ í¸ë¦¬í•©ë‹ˆë‹¤.

```python
# --- ë°ì´í„° ì €ì¥ ì˜ˆì‹œ ---
# imagesì™€ labelsëŠ” ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë°°ì—´ê³¼ í•´ë‹¹ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸ë¼ê³  ê°€ì •
 images = np.random.randint(0, 256, size=(10, 80, 80, 3), dtype=np.uint8) # ê°€ìƒ ì´ë¯¸ì§€ ë°ì´í„°
 labels = np.random.randint(0, 2, size=10) # ê°€ìƒ ë¼ë²¨ ë°ì´í„°

# np.savez() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ NumPy ë°°ì—´ì„ í•˜ë‚˜ì˜ .npz íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
# í‚¤-ê°’ ìŒ í˜•íƒœë¡œ ì €ì¥ë˜ë©°, ë‚˜ì¤‘ì— í‚¤ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 np.savez('imagedata0_train.npz', data=images, targets=labels)
 print("imagedata0_train.npz íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

# --- ë°ì´í„° ë¡œë“œ ì˜ˆì‹œ ---
# ì €ì¥ëœ .npz íŒŒì¼ì„ np.load() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
# allow_pickle=TrueëŠ” ê°ì²´ ë°°ì—´ì„ í¬í•¨í•˜ëŠ” .npz íŒŒì¼ì„ ë¡œë“œí•  ë•Œ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
 data_loaded = np.load('imagedata0_train.npz', allow_pickle=True)

# ì €ì¥ ì‹œ ì‚¬ìš©í–ˆë˜ í‚¤(ì˜ˆ: 'data', 'targets')ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ì—´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
 X_loaded = data_loaded['data']
 y_loaded = data_loaded['targets']

 print(f"\në¡œë“œëœ ì´ë¯¸ì§€ ë°ì´í„° í˜•íƒœ: {X_loaded.shape}")
 print(f"ë¡œë“œëœ ë¼ë²¨ ë°ì´í„° í˜•íƒœ: {y_loaded.shape}")
```

### 2.2 ìˆ˜ì¹˜ ë°ì´í„° ì •ê·œí™”

ìˆ˜ì¹˜í˜• ë°ì´í„°ëŠ” ê° íŠ¹ì„±(Feature)ì˜ ìŠ¤ì¼€ì¼(ê°’ì˜ ë²”ìœ„)ì´ í¬ê²Œ ë‹¤ë¥¼ ê²½ìš°, ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ í•™ìŠµì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ì¼€ì¼ì´ í° íŠ¹ì„±ì´ ëª¨ë¸ í•™ìŠµì— ì§€ë°°ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê±°ë‚˜, ìµœì í™” ê³¼ì •ì˜ ìˆ˜ë ´ì„ ë°©í•´í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ í•™ìŠµ ì „ì— ë°ì´í„°ë¥¼ ì •ê·œí™”(Normalization) ë˜ëŠ” í‘œì¤€í™”(Standardization)í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

#### ì •ê·œí™”ì˜ í•„ìš”ì„±

**Boston Housing ì‹¤ìŠµ**ì—ì„œ í™•ì¸ëœ ìŠ¤ì¼€ì¼ ì°¨ì´:

-   TAX (ì¬ì‚°ì„¸ìœ¨): 188 ~ 711 (ë²”ìœ„: 523)
-   NOX (ì§ˆì†Œì‚°í™”ë¬¼ ë†ë„): 0.39 ~ 0.87 (ë²”ìœ„: 0.49)
-   **ìŠ¤ì¼€ì¼ ì°¨ì´ ë¹„ìœ¨**: ì•½ 1076:1 (TAXì˜ ë²”ìœ„ê°€ NOXì˜ ë²”ìœ„ë³´ë‹¤ ì•½ 1000ë°° ì´ìƒ í¼)

ì´ì²˜ëŸ¼ íŠ¹ì„± ê°„ ìŠ¤ì¼€ì¼ ì°¨ì´ê°€ í´ ê²½ìš°, ê²½ì‚¬í•˜ê°•ë²• ê¸°ë°˜ì˜ ëª¨ë¸ì€ ìŠ¤ì¼€ì¼ì´ í° íŠ¹ì„±ì— ë” ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì—¬ ìµœì í™” ê²½ë¡œê°€ ë¹„íš¨ìœ¨ì ìœ¼ë¡œ ë˜ê±°ë‚˜ ìˆ˜ë ´ì´ ì–´ë ¤ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì •ê·œí™”ëŠ” í•„ìˆ˜ì ì¸ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤.

#### ì •ê·œí™” ë°©ë²• ì„ íƒ

ë”¥ëŸ¬ë‹ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì •ê·œí™” ë°©ë²•ì€ `StandardScaler` (í‘œì¤€í™”)ì™€ `MinMaxScaler` (ìµœì†Œ-ìµœëŒ€ ì •ê·œí™”)ì…ë‹ˆë‹¤. `Normalizer`ëŠ” ì£¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ ê°™ì´ ë²¡í„°ì˜ ë°©í–¥ì´ ì¤‘ìš”í•œ ê²½ìš°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
from sklearn.preprocessing import Normalizer, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston # Boston Housing ë°ì´í„°ì…‹ì€ scikit-learn 1.2ë¶€í„° ì œê±°ë¨
import pandas as pd
import numpy as np

# ë³´ìŠ¤í„´ ì£¼íƒê°€ê²© ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œë¥¼ ìœ„í•´ ì§ì ‘ ë°ì´í„° ìƒì„± ë˜ëŠ” ë‹¤ë¥¸ ë°ì´í„°ì…‹ ì‚¬ìš© ê¶Œì¥)
 from sklearn.datasets import fetch_california_housing
 housing = fetch_california_housing()
 X_housing = housing.data
 y_housing = housing.target

# ê°€ìƒì˜ ë°ì´í„° ìƒì„± (ì‹¤ì œ Boston Housing ë°ì´í„°ì…‹ì˜ íŠ¹ì„± ìŠ¤ì¼€ì¼ ëª¨ë°©)
np.random.seed(42)
X_dummy = np.random.rand(100, 2) * 100 # 2ê°œ íŠ¹ì„±
X_dummy[:, 0] = X_dummy[:, 0] * 500 + 100 # TAX ìŠ¤ì¼€ì¼
X_dummy[:, 1] = X_dummy[:, 1] * 0.5 + 0.3 # NOX ìŠ¤ì¼€ì¼
y_dummy = np.random.rand(100)

X_train, X_test, y_train, y_test = train_test_split(X_dummy, y_dummy, test_size=0.3, random_state=42)

print("--- ì •ê·œí™” ì „ ë°ì´í„° ìŠ¤ì¼€ì¼ (í›ˆë ¨ ë°ì´í„°) ---")
print(f"TAX (íŠ¹ì„± 0) í‰ê· : {X_train[:, 0].mean():.2f}, í‘œì¤€í¸ì°¨: {X_train[:, 0].std():.2f}")
print(f"NOX (íŠ¹ì„± 1) í‰ê· : {X_train[:, 1].mean():.2f}, í‘œì¤€í¸ì°¨: {X_train[:, 1].std():.2f}")

# 1. StandardScaler (í‘œì¤€í™”): í‰ê· ì„ 0, í‘œì¤€í¸ì°¨ë¥¼ 1ë¡œ ë³€í™˜
scaler_std = StandardScaler()
X_train_scaled_std = scaler_std.fit_transform(X_train)
X_test_scaled_std = scaler_std.transform(X_test) # Data Leakage ë°©ì§€

print("--- StandardScaler ì ìš© í›„ ë°ì´í„° ìŠ¤ì¼€ì¼ (í›ˆë ¨ ë°ì´í„°) ---")
print(f"TAX (íŠ¹ì„± 0) í‰ê· : {X_train_scaled_std[:, 0].mean():.2f}, í‘œì¤€í¸ì°¨: {X_train_scaled_std[:, 0].std():.2f}")
print(f"NOX (íŠ¹ì„± 1) í‰ê· : {X_train_scaled_std[:, 1].mean():.2f}, í‘œì¤€í¸ì°¨: {X_train_scaled_std[:, 1].std():.2f}")

# 2. Normalizer (L2 ì •ê·œí™”): ê° ìƒ˜í”Œ(í–‰)ì˜ ë²¡í„° í¬ê¸°ë¥¼ 1ë¡œ ë§Œë“¦
# ì£¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ì„ì´ë‚˜ ë²¡í„°ì˜ ë°©í–¥ì´ ì¤‘ìš”í•œ ê²½ìš°ì— ì‚¬ìš©
normalizer_l2 = Normalizer(norm='l2')
X_train_normalized_l2 = normalizer_l2.fit_transform(X_train)
X_test_normalized_l2 = normalizer_l2.transform(X_test) # Data Leakage ë°©ì§€

print("--- Normalizer (L2) ì ìš© í›„ ë°ì´í„° ìŠ¤ì¼€ì¼ (í›ˆë ¨ ë°ì´í„°, ì²« 5ê°œ ìƒ˜í”Œì˜ L2 Norm) ---")
print(np.linalg.norm(X_train_normalized_l2[:5], axis=1))
```


### 2.3 ë°ì´í„° ë¶„í•  ì „ëµ

ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê³¼ì í•©(Overfitting)ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì„ í›ˆë ¨(Train) ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸(Test) ì„¸íŠ¸ë¡œ ë¶„í• í•˜ëŠ” ê²ƒì€ í•„ìˆ˜ì ì¸ ê³¼ì •ì…ë‹ˆë‹¤. íŠ¹íˆ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê³ ë ¤í•œ ë¶„í•  ì „ëµì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

```python
from sklearn.model_selection import train_test_split
import numpy as np

# ì˜ˆì‹œ ë°ì´í„° ìƒì„± (í´ë˜ìŠ¤ ë¶ˆê· í˜•ì„ ê°€ì •í•œ ê°€ìƒ ë°ì´í„°)
X_dummy_split = np.random.rand(100, 5) # 100ê°œ ìƒ˜í”Œ, 5ê°œ íŠ¹ì„±
y_dummy_split = np.concatenate([np.zeros(80), np.ones(20)]) # 80ê°œ í´ë˜ìŠ¤ 0, 20ê°œ í´ë˜ìŠ¤ 1

print("--- ë¶„í•  ì „ í´ë˜ìŠ¤ ë¶„í¬ ---")
print(f"í´ë˜ìŠ¤ 0: {np.sum(y_dummy_split == 0)}ê°œ, í´ë˜ìŠ¤ 1: {np.sum(y_dummy_split == 1)}ê°œ")

# ê³„ì¸µí™” ë¶„í•  (Stratified Split): íƒ€ê²Ÿ ë³€ìˆ˜(y)ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.
# test_size: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ (0.3ì€ 30%)
# random_state: ì¬í˜„ì„±ì„ ìœ„í•œ ë‚œìˆ˜ ì‹œë“œ
# stratify=y: yì˜ í´ë˜ìŠ¤ ë¶„í¬ì— ë”°ë¼ ê³„ì¸µì ìœ¼ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X_dummy_split, y_dummy_split, 
    test_size=0.3, 
    random_state=42, 
    stratify=y_dummy_split  # í´ë˜ìŠ¤ ë¶„í¬ ê· ë“± ìœ ì§€
)

print("\n--- ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬ (í›ˆë ¨ ì„¸íŠ¸) ---")
print(f"í´ë˜ìŠ¤ 0: {np.sum(y_train == 0)}ê°œ, í´ë˜ìŠ¤ 1: {np.sum(y_train == 1)}ê°œ")
print("--- ë¶„í•  í›„ í´ë˜ìŠ¤ ë¶„í¬ (í…ŒìŠ¤íŠ¸ ì„¸íŠ¸) ---")
print(f"í´ë˜ìŠ¤ 0: {np.sum(y_test == 0)}ê°œ, í´ë˜ìŠ¤ 1: {np.sum(y_test == 1)}ê°œ")

print(f"\ní›ˆë ¨ ë°ì´í„° í˜•íƒœ: {X_train.shape}, í›ˆë ¨ ë¼ë²¨ í˜•íƒœ: {y_train.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: {X_test.shape}, í…ŒìŠ¤íŠ¸ ë¼ë²¨ í˜•íƒœ: {y_test.shape}")
```

---

## 3. ì´ì§„ ë¶„ë¥˜ (Binary Classification)

ì´ì§„ ë¶„ë¥˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë‘ ê°œì˜ ìƒí˜¸ ë°°íƒ€ì ì¸ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìŠ¤íŒ¸ ë©”ì¼ ë¶„ë¥˜(ìŠ¤íŒ¸/ì •ìƒ), ì§ˆë³‘ ì§„ë‹¨(ì–‘ì„±/ìŒì„±), ì œí’ˆ ë¶ˆëŸ‰ ì—¬ë¶€(ë¶ˆëŸ‰/ì •ìƒ) ë“±ì´ ì´ì§„ ë¶„ë¥˜ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê½ƒ ë¶„ë¥˜ ì‹¤ìŠµ(ë°ì´ì§€ vs ë¯¼ë“¤ë ˆ)ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„¤ê³„, í›ˆë ¨, í‰ê°€ ê³¼ì •ì„ ìƒì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤.

### 3.1 ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ ì„¤ê³„

ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ `Sequential` APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ `Dense` (ì™„ì „ ì—°ê²°) ì¸µì„ ìŒ“ì•„ ì˜¬ë¦½ë‹ˆë‹¤. ì¶œë ¥ì¸µì€ 1ê°œì˜ ë‰´ëŸ°ê³¼ `sigmoid` í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥  ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.

#### ì•„í‚¤í…ì²˜ êµ¬ì„± (ê½ƒ ë¶„ë¥˜ ì‹¤ìŠµ)

```python
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam # Adam ì˜µí‹°ë§ˆì´ì € ì„í¬íŠ¸

def create_binary_model(input_shape):
    """ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹ ê²½ë§ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        input_shape (int): ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„±(feature) ê°œìˆ˜.

    Returns:
        tf.keras.Model: ì»´íŒŒì¼ëœ Sequential ëª¨ë¸ ê°ì²´.
    """
    model = models.Sequential([
        layers.Input(shape=(input_shape,)),  # ì…ë ¥ì¸µ: input_shapeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
        layers.Dense(128, activation='relu'), # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.5),  # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ
        layers.Dense(64, activation='relu'),  # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.5),  # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ
        layers.Dense(32, activation='relu'),  # ì„¸ ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dense(1, activation='sigmoid')  # ì¶œë ¥ì¸µ: ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•´ 1ê°œ ë‰´ëŸ°, sigmoid í™œì„±í™”
    ])
    
    # ëª¨ë¸ ì»´íŒŒì¼: ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜, í‰ê°€ì§€í‘œ ì„¤ì •
    model.compile(
        optimizer='adam', # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
        loss='binary_crossentropy',  # ì´ì§„ ë¶„ë¥˜ì— ìµœì í™”ëœ ì†ì‹¤ í•¨ìˆ˜
        metrics=['accuracy', 'precision', 'recall', 'auc'] # ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, AUCë¥¼ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©
    )
    
    return model

# --- ì‚¬ìš© ì˜ˆì‹œ ---
 input_dim = 784 # ì˜ˆì‹œ ì…ë ¥ ì°¨ì› (ì˜ˆ: 28x28 ì´ë¯¸ì§€ í‰íƒ„í™”)
 binary_model = create_binary_model(input_dim)
 binary_model.summary()
```

### 3.2 í•µì‹¬ ì„¤ì • ìš”ì†Œ

ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ êµ¬ì¶•í•˜ê¸° ìœ„í•œ í•µì‹¬ ì„¤ì • ìš”ì†Œë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

| êµ¬ì„±ìš”ì†Œ | ì„¤ì • | ì´ìœ  |
|:---|:---|:---|
| **ì¶œë ¥ì¸µ** | `layers.Dense(1, activation='sigmoid')` | ì´ì§„ ë¶„ë¥˜ëŠ” ë‘ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¥¼ ì˜ˆì¸¡í•˜ë¯€ë¡œ 1ê°œì˜ ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ê³ , 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥  ê°’ì„ ì¶œë ¥í•˜ê¸° ìœ„í•´ `sigmoid` í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. |
| **ì†ì‹¤í•¨ìˆ˜** | `binary_crossentropy` | ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì— ìµœì í™”ëœ ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë¼ë²¨(0 ë˜ëŠ” 1) ê°„ì˜ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. |
| **ì„ê³„ê°’ (Threshold)** | 0.5 (ì¼ë°˜ì ) | `sigmoid` ì¶œë ¥(í™•ë¥ )ì´ 0.5ë³´ë‹¤ í¬ë©´ ì–‘ì„±(1), ì‘ìœ¼ë©´ ìŒì„±(0)ìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²°ì • ê¸°ì¤€ì…ë‹ˆë‹¤. ë¬¸ì œì˜ íŠ¹ì„±(ì˜ˆ: FP/FN ë¹„ìš©)ì— ë”°ë¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. |
| **í‰ê°€ì§€í‘œ** | `accuracy`, `precision`, `recall`, `auc` | ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‹¤ê°ë„ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´ ì •í™•ë„ ì™¸ì— ì •ë°€ë„, ì¬í˜„ìœ¨, ROC AUC(Area Under Curve)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¹íˆ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” ê²½ìš° `precision`, `recall`, `auc`ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤. |

### 3.3 ëª¨ë¸ í›ˆë ¨ ë° ì½œë°±

ëª¨ë¸ í›ˆë ¨ì€ `model.fit()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•˜ë©°, ì´ ê³¼ì •ì—ì„œ `callbacks`ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ê³¼ì •ì„ ì œì–´í•˜ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `EarlyStopping`ê³¼ `ReduceLROnPlateau`ëŠ” ê³¼ì í•© ë°©ì§€ ë° í•™ìŠµ íš¨ìœ¨ì„± ì¦ëŒ€ì— ìœ ìš©í•œ ì½œë°±ì…ë‹ˆë‹¤.

```python
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
import numpy as np

# --- ë°ì´í„° ì¤€ë¹„ (ìœ ë°©ì•” ë°ì´í„°ì…‹) ---
cancer = load_breast_cancer()
X_data = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y_data = np.where(cancer.target == 0, 1, 0) # ì•…ì„±ì„ 1, ì–‘ì„±ì„ 0ìœ¼ë¡œ ë³€ê²½

X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_data, y_data, test_size=0.3, random_state=42, stratify=y_data)

scaler_b = StandardScaler()
X_train_scaled_b = scaler_b.fit_transform(X_train_b)
X_test_scaled_b = scaler_b.transform(X_test_b)

# --- ëª¨ë¸ ìƒì„± (3.1ì—ì„œ ì •ì˜ëœ create_binary_model í•¨ìˆ˜ ì‚¬ìš©) ---
binary_model = create_binary_model(input_shape=X_train_scaled_b.shape[1])

# ì½œë°± ì„¤ì •
callbacks = [
    # EarlyStopping: val_lossê°€ 10 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì¤‘ë‹¨, ìµœì  ê°€ì¤‘ì¹˜ ë³µì›
    EarlyStopping(
        monitor='val_loss',
        patience=10,
        restore_best_weights=True,
        verbose=1 # ì¡°ê¸° ì¢…ë£Œ ì‹œ ë©”ì‹œì§€ ì¶œë ¥
    ),
    # ReduceLROnPlateau: val_lossê°€ 5 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥ ì„ 0.5ë°° ê°ì†Œ
    ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=0.00001, # ìµœì†Œ í•™ìŠµë¥ 
        verbose=1 # í•™ìŠµë¥  ê°ì†Œ ì‹œ ë©”ì‹œì§€ ì¶œë ¥
    )
]

print("--- ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ---")
history_binary = binary_model.fit(
    X_train_scaled_b, y_train_b,
    epochs=200, # ì¶©ë¶„íˆ í° ì—í¬í¬ ìˆ˜ ì„¤ì • (EarlyStoppingì´ ì¡°ì ˆ)
    batch_size=32,
    validation_split=0.2, # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©
    callbacks=callbacks,
    verbose=1
)

print("\n--- ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ ---")
```

### 3.4 ì„±ëŠ¥ ë¶„ì„

ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë‹¤ì–‘í•œ ì§€í‘œë¥¼ í†µí•´ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤. íŠ¹íˆ í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” ê²½ìš° ì •í™•ë„ ì™¸ì˜ ì§€í‘œë“¤ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

#### ì£¼ìš” í‰ê°€ ì§€í‘œ

-   **Accuracy (ì •í™•ë„)**: ì „ì²´ ì˜ˆì¸¡ ì¤‘ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨. `(TP + TN) / (TP + TN + FP + FN)`
-   **Precision (ì •ë°€ë„)**: ëª¨ë¸ì´ 'ì–‘ì„±'ìœ¼ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì—ì„œ ì‹¤ì œë¡œ 'ì–‘ì„±'ì¸ ë¹„ìœ¨. `TP / (TP + FP)`
-   **Recall (ì¬í˜„ìœ¨)**: ì‹¤ì œ 'ì–‘ì„±'ì¸ ê²ƒ ì¤‘ì—ì„œ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ 'ì–‘ì„±'ìœ¼ë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨. `TP / (TP + FN)`
-   **AUC (Area Under the ROC Curve)**: ROC ê³¡ì„  ì•„ë˜ ë©´ì . ëª¨ë¸ì˜ ì „ë°˜ì ì¸ ë¶„ë¥˜ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸ì…ë‹ˆë‹¤.

#### ì‹¤ìŠµ ê²°ê³¼ (ë°ì´ì§€ vs ë¯¼ë“¤ë ˆ)

ê½ƒ ë¶„ë¥˜ ì‹¤ìŠµ(ë°ì´ì§€ vs ë¯¼ë“¤ë ˆ)ì—ì„œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì„ ì ìš©í•œ ê²°ê³¼ì…ë‹ˆë‹¤. (ì‹¤ì œ ì½”ë“œëŠ” ë°ì´í„°ì…‹ ì¤€ë¹„ê°€ ë³µì¡í•˜ì—¬ ìƒëµë˜ì—ˆì§€ë§Œ, ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

-   ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: ì•½ 55%
-   ì£¼ìš” ì´ìŠˆ: ë°ì´í„° ë¶ˆê· í˜•, ëª¨ë¸ ë³µì¡ë„

**ë¶„ì„**: 55%ì˜ ì •í™•ë„ëŠ” ë¬´ì‘ìœ„ ì˜ˆì¸¡(50%)ë³´ë‹¤ ì•½ê°„ ë‚˜ì€ ìˆ˜ì¤€ìœ¼ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë§¤ìš° ë‚®ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ì…‹ì˜ íŠ¹ì„±(ì˜ˆ: ë°ì´ì§€ì™€ ë¯¼ë“¤ë ˆ ì´ë¯¸ì§€ ê°„ì˜ ë¯¸ë¬˜í•œ ì°¨ì´, ë°°ê²½ ë…¸ì´ì¦ˆ ë“±)ì´ë‚˜ ëª¨ë¸ì˜ ë³µì¡ë„ ë¶€ì¡±, ë˜ëŠ” ë°ì´í„° ë¶ˆê· í˜•(ë°ì´ì§€ì™€ ë¯¼ë“¤ë ˆ ì´ë¯¸ì§€ ìˆ˜ì˜ ì°¨ì´)ê³¼ ê°™ì€ ë¬¸ì œë¡œ ì¸í•´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì´ë¯¸ì§€ ë°ì´í„°ì˜ ê²½ìš°, ì™„ì „ ì—°ê²° ì¸µë§Œìœ¼ë¡œëŠ” ë³µì¡í•œ ì‹œê°ì  íŒ¨í„´ì„ í•™ìŠµí•˜ê¸° ì–´ë µê¸° ë•Œë¬¸ì— CNN(Convolutional Neural Network)ê³¼ ê°™ì€ ì´ë¯¸ì§€ íŠ¹í™” ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤.


---

## 4. ë‹¤ì¤‘ ë¶„ë¥˜ (Multi-class Classification)

ë‹¤ì¤‘ ë¶„ë¥˜ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì„¸ ê°œ ì´ìƒì˜ ìƒí˜¸ ë°°íƒ€ì ì¸ í´ë˜ìŠ¤ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, MNIST ìˆ«ì ë¶„ë¥˜(0-9), ë¶“ê½ƒ í’ˆì¢… ë¶„ë¥˜(3ê°€ì§€ í’ˆì¢…), ì“°ë ˆê¸° ì¢…ë¥˜ ë¶„ë¥˜(í”Œë¼ìŠ¤í‹±, ìœ ë¦¬ ë“±) ë“±ì´ ë‹¤ì¤‘ ë¶„ë¥˜ì— í•´ë‹¹í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì“°ë ˆê¸° ë¶„ë¥˜ ì‹¤ìŠµì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„¤ê³„, ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬, ì„±ëŠ¥ í‰ê°€ ê³¼ì •ì„ ìƒì„¸íˆ ì‚´í´ë´…ë‹ˆë‹¤.

### 4.1 ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ ì„¤ê³„

ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ `Sequential` APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ `Dense` (ì™„ì „ ì—°ê²°) ì¸µì„ ìŒ“ì•„ ì˜¬ë¦½ë‹ˆë‹¤. ì¶œë ¥ì¸µì€ ë¶„ë¥˜í•  í´ë˜ìŠ¤ì˜ ê°œìˆ˜ë§Œí¼ì˜ ë‰´ëŸ°ê³¼ `softmax` í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

#### ì“°ë ˆê¸° ë¶„ë¥˜ ëª¨ë¸ (6ê°œ í´ë˜ìŠ¤)

```python
from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import Adam

def create_multiclass_model(input_shape, num_classes=6):
    """ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì‹ ê²½ë§ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        input_shape (int): ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„±(feature) ê°œìˆ˜.
        num_classes (int): ë¶„ë¥˜í•  í´ë˜ìŠ¤ì˜ ì´ ê°œìˆ˜.

    Returns:
        tf.keras.Model: ì»´íŒŒì¼ëœ Sequential ëª¨ë¸ ê°ì²´.
    """
    model = models.Sequential([
        layers.Input(shape=(input_shape,)),  # ì…ë ¥ì¸µ: input_shapeë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •
        layers.Dense(512, activation='relu'), # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.3), # ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë“œë¡­ì•„ì›ƒ
        layers.Dense(256, activation='relu'), # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.3),
        layers.Dense(128, activation='relu'), # ì„¸ ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.2),
        layers.Dense(64, activation='relu'),  # ë„¤ ë²ˆì§¸ ì€ë‹‰ì¸µ
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation='softmax')  # ì¶œë ¥ì¸µ: í´ë˜ìŠ¤ ìˆ˜ë§Œí¼ ë‰´ëŸ°, softmax í™œì„±í™”
    ])
    
    # ëª¨ë¸ ì»´íŒŒì¼: ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜, í‰ê°€ì§€í‘œ ì„¤ì •
    model.compile(
        optimizer='adam', # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
        loss='sparse_categorical_crossentropy',  # ë‹¤ì¤‘ ë¶„ë¥˜ ì†ì‹¤ í•¨ìˆ˜ (ë¼ë²¨ì´ ì •ìˆ˜ í˜•íƒœì¼ ë•Œ)
        metrics=['accuracy'] # ì •í™•ë„ë¥¼ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©
    )
    
    return model

# --- ì‚¬ìš© ì˜ˆì‹œ ---
 input_dim_mc = 784 # ì˜ˆì‹œ ì…ë ¥ ì°¨ì› (ì˜ˆ: 28x28 ì´ë¯¸ì§€ í‰íƒ„í™”)
 num_classes_mc = 6 # ì˜ˆì‹œ í´ë˜ìŠ¤ ìˆ˜
 multiclass_model = create_multiclass_model(input_dim_mc, num_classes_mc)
 multiclass_model.summary()
```

### 4.2 í´ë˜ìŠ¤ ì •ì˜ ë° ë§¤í•‘

ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ê° í´ë˜ìŠ¤ì— ëŒ€í•œ ëª…í™•í•œ ì •ì˜ì™€, ì´ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì IDë¡œ ë§¤í•‘í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í•´ì„í•˜ê³  í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•©ë‹ˆë‹¤.

#### ì“°ë ˆê¸° ë¶„ë¥˜ ë°ì´í„°ì…‹

ì“°ë ˆê¸° ë¶„ë¥˜ ë°ì´í„°ì…‹ì€ 6ê°€ì§€ ì¢…ë¥˜ì˜ ì“°ë ˆê¸° ì¹´í…Œê³ ë¦¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ì¹´í…Œê³ ë¦¬ì—ëŠ” ê³ ìœ í•œ IDê°€ ë¶€ì—¬ë©ë‹ˆë‹¤.

| í´ë˜ìŠ¤ ID | ì¹´í…Œê³ ë¦¬ | ì„¤ëª… | ë°ì´í„° ìˆ˜ |
|:---|:---|:---|:---|
| 0 | cardboard | ê³¨íŒì§€, ë°•ìŠ¤ | 403ê°œ |
| 1 | glass | ìœ ë¦¬ë³‘, ìœ ë¦¬ì»µ | 501ê°œ |
| 2 | metal | ìº”, ê¸ˆì† ìš©ê¸° | 410ê°œ |
| 3 | paper | ë¬¸ì„œ, ì‹ ë¬¸ | 594ê°œ |
| 4 | plastic | í˜íŠ¸ë³‘, í”Œë¼ìŠ¤í‹± | 482ê°œ |
| 5 | trash | ê¸°íƒ€ ì“°ë ˆê¸° | 137ê°œ |

```python
# í´ë˜ìŠ¤ ì´ë¦„ ì •ì˜ (ìˆœì„œëŠ” í´ë˜ìŠ¤ IDì™€ ì¼ì¹˜í•´ì•¼ í•¨)
class_names_trash = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']

# í´ë˜ìŠ¤ IDì™€ ì´ë¦„ ê°„ì˜ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±
class_id_to_name = {i: name for i, name in enumerate(class_names_trash)}
class_name_to_id = {name: i for i, name in enumerate(class_names_trash)}

print("í´ë˜ìŠ¤ ID -> ì´ë¦„ ë§¤í•‘:", class_id_to_name)
print("í´ë˜ìŠ¤ ì´ë¦„ -> ID ë§¤í•‘:", class_name_to_id)
```

### 4.3 ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬

ë‹¤ì¤‘ ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ê°„ ë°ì´í„° ìˆ˜ì˜ ì°¨ì´ê°€ í´ ê²½ìš° **ë°ì´í„° ë¶ˆê· í˜•(Imbalanced Data)** ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¤ìˆ˜ í´ë˜ìŠ¤ì— í¸í–¥ë˜ì–´ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì“°ë ˆê¸° ë¶„ë¥˜ ë°ì´í„°ì…‹ì˜ ê²½ìš°, 'paper' í´ë˜ìŠ¤ê°€ 594ê°œì¸ ë°˜ë©´ 'trash' í´ë˜ìŠ¤ëŠ” 137ê°œë¡œ, ì•½ 4.34:1ì˜ ë¶ˆê· í˜• ë¹„ìœ¨ì„ ë³´ì…ë‹ˆë‹¤.

#### ë¶ˆê· í˜• ë¶„ì„

-   ìµœëŒ€: 594ê°œ (paper)
-   ìµœì†Œ: 137ê°œ (trash)
-   ë¶ˆê· í˜• ë¹„ìœ¨: 4.34:1 (paper:trash)

#### í•´ê²° ë°©ì•ˆ

ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì£¼ìš” ì „ëµë“¤ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1.  **ê³„ì¸µí™” ë¶„í•  (Stratified Split)**: `train_test_split` í•¨ìˆ˜ ì‚¬ìš© ì‹œ `stratify=y` ì˜µì…˜ì„ ì„¤ì •í•˜ì—¬ í›ˆë ¨ ì„¸íŠ¸ì™€ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ëª¨ë‘ì—ì„œ ì›ë³¸ ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ë¹„ìœ¨ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ í‰ê°€ì˜ ì‹ ë¢°ì„±ì„ ë†’ì…ë‹ˆë‹¤.
    ```python
    from sklearn.model_selection import train_test_split
    # X_data, y_dataëŠ” ì „ì²´ ë°ì´í„°ì…‹ê³¼ ë¼ë²¨
     X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42, stratify=y_data)
    ```
2.  **í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ (Class Weight)**: `model.fit()` ë©”ì„œë“œì˜ `class_weight` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†Œìˆ˜ í´ë˜ìŠ¤ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œì„ ë” ì¤‘ìš”í•˜ê²Œ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.
    ```python
    from sklearn.utils import class_weight
    # y_trainì€ í›ˆë ¨ ë°ì´í„°ì˜ ë¼ë²¨
    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
    class_weights_dict = dict(enumerate(class_weights))
    # model.fit(..., class_weight=class_weights_dict)
    ```
3.  **ë°ì´í„° ì¦ê°• (Data Augmentation)**: íŠ¹íˆ ì´ë¯¸ì§€ ë°ì´í„°ì—ì„œ, ë¶€ì¡±í•œ í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ë¥¼ íšŒì „, í™•ëŒ€/ì¶•ì†Œ, ë’¤ì§‘ê¸° ë“± ì¸ìœ„ì ìœ¼ë¡œ ë³€í˜•í•˜ì—¬ ë°ì´í„°ì˜ ì–‘ì„ ëŠ˜ë¦½ë‹ˆë‹¤.
4.  **ë¦¬ìƒ˜í”Œë§ (Resampling)**: 
    -   **ì˜¤ë²„ìƒ˜í”Œë§ (Oversampling)**: ì†Œìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ëŠ˜ë¦½ë‹ˆë‹¤. (ì˜ˆ: SMOTE)
    -   **ì–¸ë”ìƒ˜í”Œë§ (Undersampling)**: ë‹¤ìˆ˜ í´ë˜ìŠ¤ì˜ ìƒ˜í”Œ ìˆ˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.

### 4.4 ì„±ëŠ¥ í‰ê°€

ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì •í™•ë„ ì™¸ì— í´ë˜ìŠ¤ë³„ ì •ë°€ë„, ì¬í˜„ìœ¨, F1-score ë“±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤. í˜¼ë™ í–‰ë ¬ì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì˜¤ë¥˜ ìœ í˜•ì„ ì‹œê°ì ìœ¼ë¡œ íŒŒì•…í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.

#### í˜¼ë™ í–‰ë ¬ ë¶„ì„

```python
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --- ê°€ìƒ ë°ì´í„° ë° ëª¨ë¸ ì˜ˆì¸¡ (ì‹¤ì œë¡œëŠ” ëª¨ë¸ í•™ìŠµ í›„ ì˜ˆì¸¡ ê²°ê³¼ ì‚¬ìš©) ---
# X_test_mc, y_test_mcëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ ì‹¤ì œ ë¼ë²¨
# multiclass_modelì€ í•™ìŠµëœ ë‹¤ì¤‘ ë¶„ë¥˜ ëª¨ë¸
 y_pred_mc_proba = multiclass_model.predict(X_test_mc) # ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ 
 y_pred_classes_mc = np.argmax(y_pred_mc_proba, axis=1) # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ ì„ íƒ

# ê°€ìƒ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± (ì‹¤ì œ ì“°ë ˆê¸° ë¶„ë¥˜ ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ ìˆ˜ì— ë§ì¶¤)
y_test_mc_dummy = np.array([0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5])
y_pred_classes_mc_dummy = np.array([0, 1, 2, 3, 4, 5, 1, 0, 3, 2, 5, 4, 0, 1, 2, 3, 4, 5])
class_names_trash = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']

# í˜¼ë™ í–‰ë ¬ ê³„ì‚°
cm_mc = confusion_matrix(y_test_mc_dummy, y_pred_classes_mc_dummy)
print("--- í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ---")
print(cm_mc)

# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
plt.figure(figsize=(10, 8))
sns.heatmap(cm_mc, annot=True, fmt='d', cmap='Blues',
            xticklabels=class_names_trash, yticklabels=class_names_trash)
plt.title('ì“°ë ˆê¸° ë¶„ë¥˜ í˜¼ë™ í–‰ë ¬')
plt.ylabel('ì‹¤ì œ ë ˆì´ë¸”')
plt.xlabel('ì˜ˆì¸¡ ë ˆì´ë¸”')
plt.show()

# ë¶„ë¥˜ ë³´ê³ ì„œ
print("\n--- ë¶„ë¥˜ ë³´ê³ ì„œ (Classification Report) ---")
print(classification_report(y_test_mc_dummy, y_pred_classes_mc_dummy, 
                          target_names=class_names_trash))
```

#### ì‹¤ìŠµ ê²°ê³¼ (ì“°ë ˆê¸° ë¶„ë¥˜)

ì“°ë ˆê¸° ë¶„ë¥˜ ëª¨ë¸ì˜ ìµœì¢… ì„±ëŠ¥ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. (ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ëŠ” ëª¨ë¸ í•™ìŠµ ë° ë°ì´í„°ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

-   ì „ì²´ ì •í™•ë„: 54.9%
-   í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í¸ì°¨ ì¡´ì¬:
    -   Trash í´ë˜ìŠ¤: ë†’ì€ ì •ë°€ë„(1.00), ë‚®ì€ ì¬í˜„ìœ¨(0.12) â†’ ëª¨ë¸ì´ Trashë¼ê³  ì˜ˆì¸¡í•˜ë©´ ê±°ì˜ ë§ì§€ë§Œ, ì‹¤ì œ Trashë¥¼ ì˜ ì°¾ì•„ë‚´ì§€ ëª»í•¨ (ë§ì€ Trashë¥¼ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜¤ë¶„ë¥˜)

**ë¶„ì„**: ì „ì²´ ì •í™•ë„ê°€ 54.9%ë¡œ ë‚®ì€ í¸ì´ë©°, íŠ¹íˆ 'trash' í´ë˜ìŠ¤ì—ì„œ ì¬í˜„ìœ¨ì´ ë§¤ìš° ë‚®ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŠ” 'trash' í´ë˜ìŠ¤ì˜ ë°ì´í„° ìˆ˜ê°€ ë‹¤ë¥¸ í´ë˜ìŠ¤ì— ë¹„í•´ í˜„ì €íˆ ì ê¸° ë•Œë¬¸ì— ë°œìƒí•˜ëŠ” ë°ì´í„° ë¶ˆê· í˜• ë¬¸ì œì˜ ì „í˜•ì ì¸ ì˜ˆì‹œì…ë‹ˆë‹¤. ëª¨ë¸ì´ 'trash' í´ë˜ìŠ¤ë¥¼ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í•˜ì—¬ ë‹¤ë¥¸ í´ë˜ìŠ¤ë¡œ ì˜¤ë¶„ë¥˜í•˜ëŠ” ê²½í–¥ì´ ê°•í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬, ë°ì´í„° ì¦ê°•, ë¦¬ìƒ˜í”Œë§(SMOTE ë“±)ê³¼ ê°™ì€ ë°ì´í„° ë¶ˆê· í˜• ì²˜ë¦¬ ê¸°ë²•ì„ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.


---

## 5. íšŒê·€ ë¬¸ì œ (Regression)

íšŒê·€ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ **ì—°ì†ì ì¸ ìˆ˜ì¹˜ ê°’ì„ ì˜ˆì¸¡**í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì£¼íƒ ê°€ê²©, ì£¼ì‹ ì‹œì„¸, ìˆ˜ìš”ëŸ‰, ì˜¨ë„ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. ë¶„ë¥˜ ë¬¸ì œì™€ í•µì‹¬ì ì¸ ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

-   **ì¶œë ¥**: í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ ì•„ë‹Œ, ì„ì˜ì˜ ì—°ì†ì ì¸ ê°’(ì˜ˆ: 25.6, 150000, -10.2)ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
-   **ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜**: íŠ¹ì • ë²”ìœ„(ì˜ˆ: 0~1)ë¡œ ê°’ì„ ì œí•œí•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜(`linear`) ReLUë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒìˆ˜ ì¶œë ¥ì„ ë°©ì§€í•˜ëŠ” ë“± ë¬¸ì œì— ë§ê²Œ ì„ íƒí•©ë‹ˆë‹¤.
-   **ì†ì‹¤ í•¨ìˆ˜**: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” `MSE`(Mean Squared Error, í‰ê·  ì œê³± ì˜¤ì°¨) ë˜ëŠ” `MAE`(Mean Absolute Error, í‰ê·  ì ˆëŒ€ ì˜¤ì°¨)ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
-   **í‰ê°€ ì§€í‘œ**: `Accuracy` ëŒ€ì‹  `MAE`, `RMSE`(Root Mean Squared Error), `RÂ²`(ê²°ì • ê³„ìˆ˜) ë“±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.

### 5.1 íšŒê·€ ëª¨ë¸ ì„¤ê³„

íšŒê·€ ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ `Sequential` APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ ê°œì˜ `Dense` ì¸µì„ ìŒ“ì•„ ì˜¬ë¦½ë‹ˆë‹¤. ì¶œë ¥ì¸µì€ ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’ì˜ ìˆ˜(ë³´í†µ 1ê°œ)ì— ë§ì¶° ë‰´ëŸ° ìˆ˜ë¥¼ ì •í•˜ê³ , í™œì„±í™” í•¨ìˆ˜ëŠ” ëŒ€ë¶€ë¶„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### California Housing ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸

> **ì°¸ê³ **: ê³¼ê±°ì—ëŠ” Boston Housing ë°ì´í„°ì…‹ì´ ë§ì´ ì‚¬ìš©ë˜ì—ˆìœ¼ë‚˜, ì¸ì¢…ì°¨ë³„ì  ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆì–´ í˜„ì¬ëŠ” ì‚¬ìš©ì´ ì§€ì–‘ë©ë‹ˆë‹¤. `scikit-learn` 1.2 ë²„ì „ë¶€í„°ëŠ” í•´ë‹¹ ë°ì´í„°ì…‹ì´ ì‚­ì œë˜ì—ˆìœ¼ë©°, **California Housing** ë°ì´í„°ì…‹ì´ ëŒ€ì²´ì¬ë¡œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
from tensorflow.keras import models, layers

def create_regression_model(input_shape):
    """íšŒê·€ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹ ê²½ë§ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        input_shape (tuple): ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì„± í˜•íƒœ.

    Returns:
        tf.keras.Model: ì»´íŒŒì¼ëœ Sequential ëª¨ë¸ ê°ì²´.
    """
    model = models.Sequential([
        layers.Input(shape=input_shape),
        layers.Dense(128, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(64, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(32, activation='relu', kernel_initializer='he_normal'),
        layers.Dense(1)  # ì¶œë ¥ì¸µ: ë‹¨ì¼ ì—°ì† ê°’ ì˜ˆì¸¡, í™œì„±í™” í•¨ìˆ˜ ì—†ìŒ (Linear)
    ])
    
    model.compile(
        optimizer='adam',
        loss='mse',      # ì†ì‹¤ í•¨ìˆ˜: í‰ê·  ì œê³± ì˜¤ì°¨
        metrics=['mae']  # í‰ê°€ì§€í‘œ: í‰ê·  ì ˆëŒ€ ì˜¤ì°¨
    )
    
    return model

# ì˜ˆì‹œ: California Housing ë°ì´í„°ì…‹ì€ 8ê°œì˜ íŠ¹ì„±ì„ ê°€ì§
input_shape_reg = (8,)
regression_model = create_regression_model(input_shape_reg)
regression_model.summary()
```

### 5.2 íšŒê·€ ë¬¸ì œ íŠ¹ì„±

#### ë¶„ë¥˜ vs íšŒê·€ ì‹¬ì¸µ ë¹„êµ

| íŠ¹ì„± | ë¶„ë¥˜ (Classification) | íšŒê·€ (Regression) |
| :--- | :--- | :--- |
| **ëª©í‘œ** | ë°ì´í„°ê°€ ì†í•  **ì¹´í…Œê³ ë¦¬** ì˜ˆì¸¡ | ë°ì´í„°ì˜ **ì—°ì†ì ì¸ ìˆ˜ì¹˜** ì˜ˆì¸¡ |
| **ì¶œë ¥ê°’** | í´ë˜ìŠ¤ í™•ë¥  ë¶„í¬ (ì˜ˆ: [0.1, 0.9]) | ì‹¤ì œ ìˆ«ì (ì˜ˆ: 25.4) |
| **ì¶œë ¥ì¸µ í™œì„±í™”** | `sigmoid` (ì´ì§„), `softmax` (ë‹¤ì¤‘) | `linear` (ì—†ìŒ) ë˜ëŠ” `relu` (ì–‘ìˆ˜ë§Œ) |
| **ì†ì‹¤ í•¨ìˆ˜** | `binary/categorical_crossentropy` | `mse`, `mae`, `huber_loss` |
| **í•µì‹¬ í‰ê°€ì§€í‘œ** | `Accuracy`, `Precision`, `Recall`, `F1-score`, `AUC` | `MAE`, `MSE`, `RMSE`, `RÂ²` (R-squared) |
| **ê²°ê³¼ í•´ì„** | "ì´ ì´ë¯¸ì§€ëŠ” 90% í™•ë¥ ë¡œ ê³ ì–‘ì´ì…ë‹ˆë‹¤." | "ì´ ì§‘ì˜ ì˜ˆìƒ ê°€ê²©ì€ $254,000ì…ë‹ˆë‹¤." |

### 5.3 ë°ì´í„°ì…‹ ë¶„ì„ ë° ì¤€ë¹„ (California Housing)

California Housing ë°ì´í„°ì…‹ì€ ìº˜ë¦¬í¬ë‹ˆì•„ì˜ ê° ë¸”ë¡ ê·¸ë£¹ì— ëŒ€í•œ 8ê°œì˜ íŠ¹ì„±ê³¼ ì¤‘ê°„ ì£¼íƒ ê°€ê²©(íƒ€ê²Ÿ)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.

####  EDA (íƒìƒ‰ì  ë°ì´í„° ë¶„ì„) ë° ì „ì²˜ë¦¬

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. ë°ì´í„° ë¡œë“œ
housing = fetch_california_housing()
X = pd.DataFrame(housing.data, columns=housing.feature_names)
y = housing.target # íƒ€ê²Ÿ: ì¤‘ê°„ ì£¼íƒ ê°€ê²© ($100,000 ë‹¨ìœ„)

# 2. ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 3. íŠ¹ì„± ìŠ¤ì¼€ì¼ë§ (StandardScaler ì‚¬ìš©)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test) # í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë³€í™˜

# 4. íƒ€ê²Ÿ ë¶„í¬ í™•ì¸ (ì‹œê°í™”)
plt.figure(figsize=(10, 6))
sns.histplot(y_train, kde=True, bins=30)
plt.title('Distribution of Median House Value (Training Data)')
plt.xlabel('Median House Value ($100,000s)')
plt.ylabel('Frequency')
plt.show()

print("--- ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ---")
print(f"í›ˆë ¨ ë°ì´í„° í˜•íƒœ: {X_train_scaled.shape}")
print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í˜•íƒœ: {X_test_scaled.shape}")
```

### 5.4 ì„±ëŠ¥ í‰ê°€ ë° ê²°ê³¼ ë¶„ì„

íšŒê·€ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì§€í‘œë“¤ì„ í†µí•´ ë‹¤ê°ì ìœ¼ë¡œ í‰ê°€í•´ì•¼ í•©ë‹ˆë‹¤.

#### ì£¼ìš” í‰ê°€ ì§€í‘œ ìƒì„¸

-   **MAE (Mean Absolute Error)**: `Î£|y_true - y_pred| / n`
    -   **ì˜ë¯¸**: ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ì ˆëŒ“ê°’ í‰ê· . ì§ê´€ì ì´ê³  í•´ì„í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤. (ì˜ˆ: MAEê°€ 0.5ì´ë©´ í‰ê· ì ìœ¼ë¡œ $50,000 ì˜¤ì°¨ê°€ ë°œìƒ)
-   **MSE (Mean Squared Error)**: `Î£(y_true - y_pred)Â² / n`
    -   **ì˜ë¯¸**: ì˜¤ì°¨ë¥¼ ì œê³±í•˜ì—¬ í‰ê· . í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹°ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. í›ˆë ¨ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.
-   **RMSE (Root Mean Squared Error)**: `sqrt(MSE)`
    -   **ì˜ë¯¸**: MSEì— ë£¨íŠ¸ë¥¼ ì”Œì›Œ ì›ë˜ íƒ€ê²Ÿê³¼ ê°™ì€ ë‹¨ìœ„ë¡œ ë§Œë“­ë‹ˆë‹¤. MAEì™€ í•¨ê»˜ ëª¨ë¸ ì„±ëŠ¥ì„ ì‹¤ì œ ë‹¨ìœ„ë¡œ í•´ì„í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.
-   **RÂ² (ê²°ì • ê³„ìˆ˜)**: `1 - (Î£(y_true - y_pred)Â²) / (Î£(y_true - y_mean)Â²) `
    -   **ì˜ë¯¸**: ëª¨ë¸ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ì–¼ë§ˆë‚˜ ì˜ ì„¤ëª…í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ì˜ ì„¤ëª…í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

#### ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ ì½”ë“œ

```python
import tensorflow as tf

# ëª¨ë¸ ìƒì„± (5.1ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ ì‚¬ìš©)
model = create_regression_model(input_shape=(X_train_scaled.shape[1],))

# ì¡°ê¸° ì¢…ë£Œ ì½œë°±
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# ëª¨ë¸ í›ˆë ¨
history = model.fit(
    X_train_scaled, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    callbacks=[early_stopping],
    verbose=0 # í›ˆë ¨ ê³¼ì • ì¶œë ¥ ìƒëµ
)

# ì„±ëŠ¥ í‰ê°€
loss, mae = model.evaluate(X_test_scaled, y_test, verbose=0)
mse = model.evaluate(X_test_scaled, y_test, verbose=0)[0] # lossê°€ mse
rmse = np.sqrt(mse)

print(f"--- í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥ ---")
print(f"MAE: {mae:.4f} (í‰ê· ì ìœ¼ë¡œ ì•½ ${mae*100000:,.0f}ì˜ ê°€ê²© ì˜¤ì°¨)")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")

# RÂ² ê³„ì‚°
from sklearn.metrics import r2_score
y_pred = model.predict(X_test_scaled).flatten()
r2 = r2_score(y_test, y_pred)
print(f"RÂ² (ê²°ì • ê³„ìˆ˜): {r2:.4f}")
```

#### ê²°ê³¼ ì‹œê°í™”

ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ **ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„**ì™€ **ì”ì°¨(Residuals) ë¶„í¬**ë¥¼ ì‹œê°í™”í•˜ëŠ” ê²ƒì´ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

```python
# 1. ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„
plt.figure(figsize=(8, 8))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], '--r', lw=2) # y=x ê¸°ì¤€ì„ 
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Values (RÂ²: {:.4f})".format(r2))
plt.grid(True)
plt.show()

# 2. ì”ì°¨(ì˜¤ì°¨) ë¶„í¬
residuals = y_test - y_pred
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True, bins=30)
plt.xlabel("Prediction Error (Residuals)")
plt.title("Distribution of Prediction Errors")
plt.axvline(0, color='red', linestyle='--')
plt.show()
```

**ë¶„ì„**:
-   **ì‚°ì ë„**: ì ë“¤ì´ ë¹¨ê°„ ì ì„ (y=x)ì— ê°€ê¹Œì´ ë¶„í¬í• ìˆ˜ë¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ì´ ì •í™•í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
-   **ì”ì°¨ ë¶„í¬**: ì”ì°¨(ì˜¤ì°¨)ê°€ 0ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì •ê·œë¶„í¬ì— ê°€ê¹Œìš´ í˜•íƒœë¥¼ ë³´ì¼ ë•Œ, ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ í•™ìŠµí–ˆë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 6. ëª¨ë¸ í‰ê°€ ë° ìµœì í™”

ëª¨ë¸ì„ í›ˆë ¨í•˜ëŠ” ê²ƒë§Œí¼ì´ë‚˜ ì¤‘ìš”í•œ ê²ƒì€ **ê°ê´€ì ì¸ ì„±ëŠ¥ í‰ê°€**ì™€ **ì§€ì†ì ì¸ ìµœì í™”**ì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•˜ê³ , ê³¼ì í•©ê³¼ ê°™ì€ ì¼ë°˜ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ë©°, ìµœìƒì˜ ì„±ëŠ¥ì„ ì´ëŒì–´ë‚´ëŠ” ë°©ë²•ì„ ë‹¤ë£¹ë‹ˆë‹¤.

### 6.1 ì¢…í•© í‰ê°€ í”„ë ˆì„ì›Œí¬

ë¬¸ì œ ìœ í˜•ì— ë”°ë¼ ì í•©í•œ í‰ê°€ì§€í‘œì™€ ì‹œê°í™” ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

| ë¬¸ì œ ìœ í˜• | í•µì‹¬ í‰ê°€ì§€í‘œ | ë³´ì¡° ì§€í‘œ ë° ê³ ë ¤ì‚¬í•­ | ì¶”ì²œ ì‹œê°í™” |
| :--- | :--- | :--- | :--- |
| **ì´ì§„ ë¶„ë¥˜** | **AUC**, **F1-Score** | `Accuracy` (í´ë˜ìŠ¤ê°€ ê· ë“±í•  ë•Œ), `Precision`, `Recall` (FP/FN ë¹„ìš©ì— ë”°ë¼ ì¤‘ìš”ë„ ì¡°ì ˆ) | ROC Curve, Precision-Recall Curve, í˜¼ë™ í–‰ë ¬ |
| **ë‹¤ì¤‘ ë¶„ë¥˜** | **Macro/Weighted F1-Score** | `Accuracy`, í´ë˜ìŠ¤ë³„ `Precision` & `Recall` (ë°ì´í„° ë¶ˆê· í˜• ì‹œ í•„ìˆ˜ í™•ì¸) | í˜¼ë™ í–‰ë ¬(íˆíŠ¸ë§µ), í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ ë§‰ëŒ€ê·¸ë˜í”„ |
| **íšŒê·€** | **RMSE**, **MAE** | `RÂ²`(ëª¨ë¸ì˜ ì„¤ëª…ë ¥), íƒ€ê²Ÿì˜ ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ ì§€í‘œì˜ ìƒëŒ€ì  í¬ê¸° | ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ ì‚°ì ë„, ì”ì°¨(Residuals) ë¶„í¬ë„ |

### 6.2 ê³¼ì í•© ì§„ë‹¨ ë° í•´ê²°

**ê³¼ì í•©(Overfitting)**ì€ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ë§Œ ì§€ë‚˜ì¹˜ê²Œ ìµœì í™”ë˜ì–´, ìƒˆë¡œìš´ ë°ì´í„°(ê²€ì¦/í…ŒìŠ¤íŠ¸ ë°ì´í„°)ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒì…ë‹ˆë‹¤.

#### ê³¼ì í•© ì§„ë‹¨

ê³¼ì í•©ì€ ì£¼ë¡œ **í›ˆë ¨ ì†ì‹¤(loss)ê³¼ ê²€ì¦ ì†ì‹¤(validation loss)ì˜ ì°¨ì´**ë¥¼ í†µí•´ ì§„ë‹¨í•©ë‹ˆë‹¤.

```python
# history ê°ì²´ë¥¼ ì‚¬ìš©í•œ ì†ì‹¤ ê³¡ì„  ì‹œê°í™”
def plot_loss_curves(history):
    plt.figure(figsize=(12, 5))
    
    # í›ˆë ¨/ê²€ì¦ ì†ì‹¤
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training vs. Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    
    # í›ˆë ¨/ê²€ì¦ ì •í™•ë„ (ë˜ëŠ” MAE ë“±)
    # 'accuracy' í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸
    if 'accuracy' in history.history:
        plt.subplot(1, 2, 2)
        plt.plot(history.history['accuracy'], label='Training Accuracy')
        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Training vs. Validation Accuracy')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()

    plt.tight_layout()
    plt.show()

plot_loss_curves(history) # model.fit()ì—ì„œ ë°˜í™˜ëœ history ê°ì²´ ì „ë‹¬
```

**ê³¼ì í•© ì§•í›„**:
-   í›ˆë ¨ ì†ì‹¤ì€ ê³„ì† ê°ì†Œí•˜ì§€ë§Œ, ê²€ì¦ ì†ì‹¤ì€ ì–´ëŠ ì‹œì ë¶€í„° ìƒìŠ¹í•˜ê±°ë‚˜ ì •ì²´ë©ë‹ˆë‹¤.
-   ë‘ ì†ì‹¤ ê³¡ì„  ì‚¬ì´ì˜ ê°„ê²©(gap)ì´ ì ì  ë²Œì–´ì§‘ë‹ˆë‹¤.

#### í•´ê²° ì „ëµ (Regularization)

ê³¼ì í•©ì„ ì™„í™”í•˜ëŠ” ê¸°ë²•ì„ **ê·œì œ(Regularization)**ë¼ê³  í•©ë‹ˆë‹¤. ë‹¤ìŒì€ ì£¼ìš” ê·œì œ ê¸°ë²•ë“¤ì„ í†µí•©ì ìœ¼ë¡œ ì ìš©í•œ ëª¨ë¸ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
from tensorflow.keras import layers, models, regularizers, callbacks
# ... (ë°ì´í„°ê°€ X_train_scaled, y_train ë“±ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆë‹¤ê³  ê°€ì •)

def create_regularized_model(input_shape):
    """ë‹¤ì–‘í•œ ê·œì œ ê¸°ë²•ì´ ì ìš©ëœ íšŒê·€ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    model = models.Sequential([
        layers.Input(shape=input_shape),
        
        # ì²« ë²ˆì§¸ Dense ë¸”ë¡: L2 ê·œì œ, ë°°ì¹˜ ì •ê·œí™”, ë“œë¡­ì•„ì›ƒ ì ìš©
        layers.Dense(128, kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(), # í™œì„±í™” í•¨ìˆ˜ ì´ì „ì— ì ìš©
        layers.Activation('relu'),
        layers.Dropout(0.5), # 50%ì˜ ë‰´ëŸ°ì„ ë¹„í™œì„±í™”

        # ë‘ ë²ˆì§¸ Dense ë¸”ë¡
        layers.Dense(64, kernel_regularizer=regularizers.l2(0.001)),
        layers.BatchNormalization(),
        layers.Activation('relu'),
        layers.Dropout(0.3), # 30%ì˜ ë‰´ëŸ°ì„ ë¹„í™œì„±í™”

        # ì¶œë ¥ì¸µ
        layers.Dense(1)
    ])

    model.compile(optimizer='adam', loss='mse', metrics=['mae'])
    return model

# ëª¨ë¸ ìƒì„±
 regularized_model = create_regularized_model((X_train_scaled.shape[1],))

# ì½œë°± ì •ì˜
# 1. ì¡°ê¸° ì¢…ë£Œ: ê²€ì¦ ì†ì‹¤ì´ 10 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í›ˆë ¨ ì¤‘ë‹¨ ë° ìµœì  ê°€ì¤‘ì¹˜ ë³µì›
early_stopping_cb = callbacks.EarlyStopping(patience=10, monitor='val_loss', restore_best_weights=True)
# 2. í•™ìŠµë¥  ê°ì†Œ: ê²€ì¦ ì†ì‹¤ì´ 5 ì—í¬í¬ ë™ì•ˆ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµë¥ ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì„
reduce_lr_cb = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

# ëª¨ë¸ í›ˆë ¨ ì‹œ ì½œë°± ì „ë‹¬
 history_regularized = regularized_model.fit(
     X_train_scaled, y_train,
     epochs=200, # ì¡°ê¸° ì¢…ë£Œê°€ ìˆìœ¼ë¯€ë¡œ ì¶©ë¶„íˆ ê¸¸ê²Œ ì„¤ì •
     batch_size=32,
     validation_split=0.2,
     callbacks=[early_stopping_cb, reduce_lr_cb],
     verbose=0 # í›ˆë ¨ ê³¼ì • ë¡œê·¸ ìƒëµ
 )
 print("Regularized model training complete.")
 plot_loss_curves(history_regularized)
```

### 6.3 í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”

**í•˜ì´í¼íŒŒë¼ë¯¸í„°**ëŠ” ëª¨ë¸ì´ í•™ìŠµì„ í†µí•´ ìŠ¤ìŠ¤ë¡œ ì°¾ëŠ” íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜, í¸í–¥)ê°€ ì•„ë‹Œ, ê°œë°œìê°€ ì§ì ‘ ì„¤ì •í•´ì•¼ í•˜ëŠ” ê°’ë“¤ì…ë‹ˆë‹¤. (ì˜ˆ: í•™ìŠµë¥ , ë°°ì¹˜ í¬ê¸°, ì¸µì˜ ìˆ˜ ë“±)

#### íŠœë‹ ì „ëµ: Keras Tuner

Keras TunerëŠ” `RandomSearch`, `Hyperband`, `BayesianOptimization` ë“± ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ì„ íš¨ìœ¨ì ìœ¼ë¡œ íƒìƒ‰í•©ë‹ˆë‹¤.

```python
import keras_tuner as kt

def build_model_for_tuner(hp):
    model = models.Sequential()
    model.add(layers.Input(shape=(X_train_scaled.shape[1],)))
    
    # ë‰´ëŸ° ìˆ˜ íƒìƒ‰ (32 ~ 256, 32 ê°„ê²©)
    hp_units = hp.Int('units', min_value=32, max_value=256, step=32)
    model.add(layers.Dense(units=hp_units, activation='relu'))
    
    # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨ íƒìƒ‰ (0.0 ~ 0.5)
    model.add(layers.Dropout(hp.Float('dropout', 0.0, 0.5, step=0.1)))
    
    model.add(layers.Dense(1)) # íšŒê·€ ë¬¸ì œì˜ ì¶œë ¥ì¸µ
    
    # í•™ìŠµë¥  íƒìƒ‰
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='mse', metrics=['mae'])
    return model

# Hyperband íŠœë„ˆ ì„¤ì •
 tuner = kt.Hyperband(build_model_for_tuner,
                      objective='val_mae', # ìµœì†Œí™”í•  ëª©í‘œ ì§€í‘œ
                      max_epochs=50,
                      factor=3, # ê° ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ìˆ˜ë¥¼ 1/3ë¡œ ì¤„ì„
                      directory='tuning_dir',
                      project_name='housing_price_tuning')

# ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰
 tuner.search(X_train_scaled, y_train, validation_split=0.2,
              callbacks=[tf.keras.callbacks.EarlyStopping(patience=5)])

# ìµœì ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° í™•ì¸
 best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
 print(f"Best learning rate: {best_hps.get('learning_rate')}")
 print(f"Best units: {best_hps.get('units')}")

# ìµœì ì˜ ëª¨ë¸ë¡œ ìµœì¢… í›ˆë ¨
 final_model = tuner.hypermodel.build(best_hps)
 final_model.fit(...)
```

### 6.4 ëª¨ë¸ ì•™ìƒë¸”

**ì•™ìƒë¸”(Ensemble)**ì€ ì—¬ëŸ¬ ê°œì˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ë§Œë“¤ì–´ ê·¸ ì˜ˆì¸¡ì„ ê²°í•©í•¨ìœ¼ë¡œì¨, ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ê°•ê±´í•˜ê³  ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

-   **ì‘ë™ ì›ë¦¬**: ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ë“¤ì´ ë§Œë“  ì˜¤ì°¨ëŠ” ì„œë¡œ ë‹¤ë¥¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ì´ ì˜¤ì°¨ë“¤ì„ í‰ê· ë‚´ê±°ë‚˜ ë‹¤ìˆ˜ê²°ë¡œ ê²°ì •í•˜ë©´ ìƒì‡„ë˜ì–´ ì „ì²´ì ì¸ ì„±ëŠ¥ì´ í–¥ìƒë©ë‹ˆë‹¤.
-   **ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ ë‹¤ì–‘ì„± í™•ë³´ ë°©ë²•**:
    1.  ì„œë¡œ ë‹¤ë¥¸ ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ê°™ì€ êµ¬ì¡°ì˜ ëª¨ë¸ì„ ì—¬ëŸ¬ ë²ˆ í›ˆë ¨.
    2.  ì„œë¡œ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°(êµ¬ì¡°, í•™ìŠµë¥  ë“±)ë¥¼ ê°€ì§„ ëª¨ë¸ë“¤ì„ í›ˆë ¨.
    3.  ì„œë¡œ ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ëª¨ë¸(ì˜ˆ: ì‹ ê²½ë§, XGBoost, LightGBM)ì„ ê²°í•©.

#### í‰ê·  ê¸°ë°˜ ì•™ìƒë¸” (Averaging Ensemble) ì˜ˆì‹œ

ê°€ì¥ ê°„ë‹¨í•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ì•™ìƒë¸” ë°©ë²• ì¤‘ í•˜ë‚˜ëŠ” ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ì‚°ìˆ  í‰ê· í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

```python
import numpy as np
from sklearn.metrics import mean_absolute_error

# ê°€ì •: X_train_scaled, y_train, X_test_scaled, y_testê°€ ì¤€ë¹„ë˜ì–´ ìˆìŒ
# ê°€ì •: create_regression_model í•¨ìˆ˜ê°€ 5.1 ì„¹ì…˜ì— ì •ì˜ë˜ì–´ ìˆìŒ

# 1. ì—¬ëŸ¬ ê°œì˜ ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
num_models = 5
models_list = []
for i in range(num_models):
    print(f"Training model {i+1}/{num_models}...")
    # ë§¤ë²ˆ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ë¡œ ì´ˆê¸°í™”ëœ ëª¨ë¸ ìƒì„±
    model = create_regression_model((X_train_scaled.shape[1],))
    model.fit(X_train_scaled, y_train,
              epochs=100,
              batch_size=32,
              validation_split=0.2,
              callbacks=[callbacks.EarlyStopping(patience=10, monitor='val_loss')],
              verbose=0)
    models_list.append(model)
    _, single_mae = model.evaluate(X_test_scaled, y_test, verbose=0)
    print(f"Model {i+1} MAE: {single_mae:.4f}")

# 2. ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ê²°í•©
predictions_list = [model.predict(X_test_scaled) for model in models_list]

# 3. ì˜ˆì¸¡ê°’ í‰ê· 
# predictions_listëŠ” (num_models, num_test_samples, 1) í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸
# ì´ë¥¼ (num_test_samples, num_models) í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ í‰ê·  ê³„ì‚°
ensemble_predictions = np.mean(np.hstack(predictions_list), axis=1)

# 4. ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)

print(f"\nEnsemble MAE: {ensemble_mae:.4f}")
```

---

## 7. ì‹¤ë¬´ ì ìš© ê°€ì´ë“œ

ì´ë¡ ê³¼ ì‹¤ìŠµì„ ë„˜ì–´, ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‹¤ì œ ê°€ì¹˜ë¡œ ì—°ê²°í•˜ê¸° ìœ„í•œ ì‹¤ë¬´ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 7.1 í”„ë¡œì íŠ¸ ì›Œí¬í”Œë¡œìš°: ì•„ì´ë””ì–´ì—ì„œ ìš´ì˜ê¹Œì§€

ë”¥ëŸ¬ë‹ í”„ë¡œì íŠ¸ëŠ” ì¼íšŒì„± ê°œë°œì´ ì•„ë‹Œ, ì§€ì†ì ì¸ ê°œì„ ì´ í•„ìš”í•œ **ë°˜ë³µì ì¸ ìƒëª…ì£¼ê¸°(Lifecycle)**ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

```mermaid
graph LR
    subgraph Phase 1: ê¸°íš ë° ì„¤ê³„
        A[1. ë¬¸ì œ ì •ì˜ & ëª©í‘œ ì„¤ì •] --> B[2. ë°ì´í„° í™•ë³´ ë° íƒìƒ‰(EDA)]
    end
    subgraph Phase 2: ê°œë°œ ë° ì‹¤í—˜
        B --> C[3. ë°ì´í„° ì „ì²˜ë¦¬ & íŠ¹ì„± ê³µí•™]
        C --> D[4. ê¸°ì¤€(Baseline) ëª¨ë¸ ìˆ˜ë¦½]
        D --> E[5. ëª¨ë¸ í›ˆë ¨ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹]
        E --> F[6. ì„±ëŠ¥ í‰ê°€ ë° ì˜¤ë¥˜ ë¶„ì„]
    end
    subgraph Phase 3: ë°°í¬ ë° ìš´ì˜
        F -- ì„±ëŠ¥ ë§Œì¡±? --> G[7. ëª¨ë¸ ë°°í¬ ë° ì„œë¹™]
        G --> H[8. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¬í•™ìŠµ]
    end
    F -- ì„±ëŠ¥ ë¶€ì¡± --> E
    H -- ë“œë¦¬í”„íŠ¸ ê°ì§€/ì„±ëŠ¥ ì €í•˜ --> B
```

-   **1. ë¬¸ì œ ì •ì˜ & ëª©í‘œ ì„¤ì •**: ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ ë”¥ëŸ¬ë‹ ë¬¸ì œ(ë¶„ë¥˜/íšŒê·€)ë¡œ ëª…í™•íˆ ë³€í™˜í•˜ê³ , ì„±ê³µì„ ì¸¡ì •í•  í•µì‹¬ ì§€í‘œ(KPI)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (ì˜ˆ: "ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ MAEë¥¼ $35,000 ì´í•˜ë¡œ ë‚®ì¶˜ë‹¤.")
-   **4. ê¸°ì¤€(Baseline) ëª¨ë¸ ìˆ˜ë¦½**: ë³µì¡í•œ ëª¨ë¸ì„ ì‹œë„í•˜ê¸° ì „ì—, ê°„ë‹¨í•œ ëª¨ë¸(ì˜ˆ: `LinearRegression`, ì–•ì€ ì‹ ê²½ë§)ë¡œ ìµœì†Œ ì„±ëŠ¥ì˜ ê¸°ì¤€ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. ì´ëŠ” ì´í›„ ëª¨ë¸ ê°œì„ ì˜ íš¨ê³¼ë¥¼ ì¸¡ì •í•˜ëŠ” ì²™ë„ê°€ ë©ë‹ˆë‹¤.
-   **6. ì˜¤ë¥˜ ë¶„ì„ (Error Analysis)**: ì „ì²´ ì„±ëŠ¥ ì§€í‘œë§Œ ë³´ì§€ ì•Šê³ , ëª¨ë¸ì´ ì–´ë–¤ ì¢…ë¥˜ì˜ ë°ì´í„°ì—ì„œ ì£¼ë¡œ í‹€ë¦¬ëŠ”ì§€ ì‹¬ì¸µ ë¶„ì„í•©ë‹ˆë‹¤. (ì˜ˆ: "ìœ ë… ì¹¨ì‹¤ ìˆ˜ê°€ 5ê°œ ì´ìƒì¸ ë¹„ì‹¼ ì£¼íƒì—ì„œ ì˜¤ì°¨ê°€ í¬ë‹¤.")
-   **8. ëª¨ë‹ˆí„°ë§ ë° ì¬í•™ìŠµ**: ë°°í¬ëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì˜ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë°ì´í„° ë¶„í¬ì˜ ë³€í™”(Drift)ë¥¼ ì§€ì†ì ìœ¼ë¡œ ê°ì§€í•˜ê³ , ì„±ëŠ¥ì´ ì¼ì • ìˆ˜ì¤€ ì´í•˜ë¡œ ë–¨ì–´ì§€ë©´ ìë™ìœ¼ë¡œ ì¬í•™ìŠµí•˜ëŠ” íŒŒì´í”„ë¼ì¸(MLOps)ì„ êµ¬ì¶•í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

### 7.2 ë¬¸ì œ ìœ í˜•ë³„ ì ìš© ì‚¬ë¡€ì™€ í•µì‹¬ ê³¼ì œ

| ë¶„ì•¼ | ì ìš© ì‚¬ë¡€ | ë¬¸ì œ ìœ í˜• | í•µì‹¬ ê³¼ì œ (Key Challenges) |
| :--- | :--- | :--- | :--- |
| **ê¸ˆìœµ** | ì‹ ìš©ì¹´ë“œ ì‚¬ê¸° íƒì§€ | ì´ì§„ ë¶„ë¥˜ | ê·¹ì‹¬í•œ ë°ì´í„° ë¶ˆê· í˜•, ì‹¤ì‹œê°„ ì¶”ë¡ (Low Latency) ìš”êµ¬ |
| **ì œì¡°** | ë°˜ë„ì²´ ì›¨ì´í¼ ë¶ˆëŸ‰ íƒì§€ | ë‹¤ì¤‘ ë¶„ë¥˜ | ì ì€ ë¶ˆëŸ‰ ë°ì´í„°, ë¹„ì •í˜• ì´ë¯¸ì§€ ë°ì´í„° ì²˜ë¦¬, ì„¤ëª… ê°€ëŠ¥ì„±(XAI) |
| **ì˜ë£Œ** | MRI ê¸°ë°˜ ì¢…ì–‘ ì•…ì„±/ì–‘ì„± íŒë³„ | ì´ì§„ ë¶„ë¥˜ | ë°ì´í„° í”„ë¼ì´ë²„ì‹œ, ë°ì´í„° ë¶€ì¡±, ëª¨ë¸ì˜ ì‹ ë¢°ì„± ë° ì•ˆì „ì„± |
| **E-ì»¤ë¨¸ìŠ¤**| ê³ ê°ì˜ ë‹¤ìŒ ë‹¬ êµ¬ë§¤ì•¡ ì˜ˆì¸¡ | íšŒê·€ | ë³µì¡í•œ ì‹œê³„ì—´ íŠ¹ì„±, ì™¸ë¶€ ìš”ì¸(í”„ë¡œëª¨ì…˜, ê³„ì ˆ) ë°˜ì˜, íŠ¹ì„± ê³µí•™ |

### 7.3 ë°°í¬ ë° ìš´ì˜ (MLOpsì˜ ì‹œì‘)

í›ˆë ¨ëœ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë§Œë“œëŠ” ê³¼ì •ì„ **ë°°í¬(Deployment)**ë¼ê³  í•©ë‹ˆë‹¤.

####  ëª¨ë¸ ë°°í¬ ì „ëµ: ê°„ë‹¨í•œ REST API ì„œë²„ êµ¬ì¶•

`Flask`ì™€ `joblib`ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ê³¼ ì „ì²˜ë¦¬ê¸°ë¥¼ ì„œë¹™í•˜ëŠ” ê°„ë‹¨í•œ API ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
# app.py
from flask import Flask, request, jsonify
import tensorflow as tf
import joblib
import numpy as np

app = Flask(__name__)

# ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
try:
    model = tf.keras.models.load_model('final_model.keras', compile=False)
    scaler = joblib.load('standard_scaler.pkl')
    print("Model and scaler loaded successfully.")
except Exception as e:
    print(f"Error loading model or scaler: {e}")
    model, scaler = None, None

@app.route('/predict', methods=['POST'])
def predict():
    if not model or not scaler:
        return jsonify({"error": "Model is not available."}), 500

    try:
        # JSON ìš”ì²­ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        data = request.get_json()
        # ì…ë ¥ ë°ì´í„°ëŠ” 2D ë°°ì—´ í˜•íƒœì—¬ì•¼ í•¨ (ì˜ˆ: [[...]])
        features = np.array(data['features'])

        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        scaled_features = scaler.transform(features)

        # ì˜ˆì¸¡ ìˆ˜í–‰
        prediction = model.predict(scaled_features)

        # ê²°ê³¼ ë°˜í™˜
        return jsonify({'predicted_value': prediction.flatten().tolist()})

    except Exception as e:
        return jsonify({"error": str(e)}), 400

# ì„œë²„ ì‹¤í–‰ (ì˜ˆ: python app.py)
# if __name__ == '__main__':
#     app.run(debug=True, port=5000)
```

### 7.4 ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

ë°°í¬ëœ ëª¨ë¸ì€ ì‹œê°„ì´ ì§€ë‚˜ë©´ì„œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìˆ˜ì…ë‹ˆë‹¤.

-   **ë°ì´í„° ë“œë¦¬í”„íŠ¸ (Data Drift)**: ì‹¤ì œ ìš´ì˜ í™˜ê²½ì˜ ë°ì´í„° ë¶„í¬ê°€ í›ˆë ¨ ì‹œì ì˜ ë°ì´í„° ë¶„í¬ì™€ ë‹¬ë¼ì§€ëŠ” í˜„ìƒ. (ì˜ˆ: ê³„ì ˆ ë³€í™”ë¡œ ì¸í•œ ì†Œë¹„ì í–‰ë™ íŒ¨í„´ ë³€í™”)
-   **ê°œë… ë“œë¦¬í”„íŠ¸ (Concept Drift)**: ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ ìì²´ê°€ ë³€í•˜ëŠ” í˜„ìƒ. (ì˜ˆ: ìƒˆë¡œìš´ ê¸ˆìœµ ì •ì±…ìœ¼ë¡œ ì¸í•œ ëŒ€ì¶œ ì‹¬ì‚¬ ê¸°ì¤€ ë³€ê²½)
-   **ëª¨ë‹ˆí„°ë§ ë°©ì•ˆ**: ìš´ì˜ ë°ì´í„°ì˜ í†µê³„ëŸ‰ì„ ì£¼ê¸°ì ìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì™€ ë¹„êµí•˜ê³ , ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥(MAE, F1-score ë“±)ì„ ì§€ì†ì ìœ¼ë¡œ ì¶”ì í•˜ì—¬ ì„±ëŠ¥ ì €í•˜ê°€ ê°ì§€ë˜ë©´ ëª¨ë¸ ì¬í•™ìŠµì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.

### 7.5 ìµœì‹  íŠ¸ë Œë“œ ë° ë°œì „ ë°©í–¥

-   **íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ë¶€ìƒ**: ìì—°ì–´ ì²˜ë¦¬ë¥¼ ë„˜ì–´ ì»´í“¨í„° ë¹„ì „(Vision Transformer), ì‹œê³„ì—´ ì˜ˆì¸¡ ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ SOTA(State-of-the-art) ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤.
-   **ìƒì„±í˜• AI (Generative AI)**: `GANs`, `Diffusion Models` ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸, ì˜¤ë””ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ê¸°ìˆ ì´ ê¸‰ê²©íˆ ë°œì „í•˜ê³  ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: DALL-E, Stable Diffusion, GPT)
-   **MLOps (Machine Learning Operations)**: ëª¨ë¸ ê°œë°œ, ë°°í¬, ìš´ì˜ì˜ ì „ ê³¼ì •ì„ ìë™í™”í•˜ê³  íš¨ìœ¨í™”í•˜ëŠ” ë¬¸í™” ë° ê¸°ìˆ  ìŠ¤íƒì˜ ì¤‘ìš”ì„±ì´ ë¶€ê°ë˜ê³  ìˆìŠµë‹ˆë‹¤.
-   **XAI (Explainable AI)**: `SHAP`, `LIME`ê³¼ ê°™ì€ ê¸°ë²•ì„ í†µí•´ "ë¸”ë™ë°•ìŠ¤"ë¡œ ì—¬ê²¨ì¡Œë˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê·¼ê±°ë¥¼ ì„¤ëª…í•˜ë ¤ëŠ” ì—°êµ¬ê°€ í™œë°œí•©ë‹ˆë‹¤.

---

## 8. í•™ìŠµ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„

1.  **ë¬¸ì œ ì •ì˜ ë° ëª¨ë¸ë§ ì—­ëŸ‰**
    -   ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ **ì´ì§„ ë¶„ë¥˜, ë‹¤ì¤‘ ë¶„ë¥˜, íšŒê·€** ì¤‘ ì í•©í•œ ë¬¸ì œë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    -   ë¬¸ì œ ìœ í˜•ì— ë§ëŠ” Keras ëª¨ë¸ êµ¬ì¡°, ì†ì‹¤ í•¨ìˆ˜, í‰ê°€ì§€í‘œë¥¼ **ìŠ¤ìŠ¤ë¡œ ì„¤ê³„í•˜ê³  ì„ íƒ**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2.  **End-to-End êµ¬í˜„ ë° ê°œë°œ ì—­ëŸ‰**
    -   `TensorFlow`ì™€ `scikit-learn`ì„ í™œìš©í•˜ì—¬ ë°ì´í„° ë¡œë”©, ì •ê·œí™”, ë¶„í• ë¶€í„° ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€ê¹Œì§€ **ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì½”ë“œë¡œ êµ¬í˜„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    -   ì´ë¯¸ì§€ ë° ì •í˜• ë°ì´í„°ë¥¼ ëª¨ë¸ ì…ë ¥ì— ë§ê²Œ **íš¨ê³¼ì ìœ¼ë¡œ ì „ì²˜ë¦¬**í•˜ëŠ” ê¸°ìˆ ì„ ìŠµë“í–ˆìŠµë‹ˆë‹¤.

3.  **ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” ë° ë¬¸ì œ í•´ê²° ì—­ëŸ‰**
    -   í›ˆë ¨/ê²€ì¦ ì†ì‹¤ ê³¡ì„ ì„ ì‹œê°í™”í•˜ì—¬ **ê³¼ì í•©ì„ ì§„ë‹¨**í•˜ê³ , `Dropout`, `L2 ê·œì œ`, `ì¡°ê¸° ì¢…ë£Œ` ë“± ìƒí™©ì— ë§ëŠ” ê·œì œ ê¸°ë²•ì„ ì ìš©í•˜ì—¬ **ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒ**ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    -   `Keras Tuner`ë¥¼ í™œìš©í•˜ì—¬ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ **ìë™ìœ¼ë¡œ íƒìƒ‰í•˜ê³  ìµœì í™”**í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    -   ë°ì´í„° ë¶ˆê· í˜•, íŠ¹ì„± ìŠ¤ì¼€ì¼ ì°¨ì´ ë“± ì‹¤ì „ ë°ì´í„°ì—ì„œ ë°œìƒí•˜ëŠ” **í•µì‹¬ ë¬¸ì œë“¤ì„ ì¸ì‹í•˜ê³  í•´ê²°**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ë‹¤ìŒ í•™ìŠµ ì¶”ì²œ: ì„±ì¥ ë¡œë“œë§µ

```mermaid
graph TD
    subgraph "Stage 1: Foundation (í˜„ì¬ ìœ„ì¹˜)"
        A[ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ë°<br>í•µì‹¬ ë¬¸ì œ ìœ í˜• ì •ë³µ<br>(ë³¸ ë¬¸ì„œ)]
    end

    subgraph "Stage 2: Specialization (ì‹¬í™” í•™ìŠµ)"
        A --> B{ë„ë©”ì¸ ì„ íƒ}
        B --> C[<b>ì»´í“¨í„° ë¹„ì „ (CV)</b><br>CNN, ResNet, YOLO, ViT]
        B --> D[<b>ìì—°ì–´ ì²˜ë¦¬ (NLP)</b><br>RNN, LSTM, Attention, Transformer, BERT]
        B --> E[<b>ì‹œê³„ì—´ / ì¶”ì²œì‹œìŠ¤í…œ</b><br>LSTM, GRU, Facebook Prophet]
    end

    subgraph "Stage 3: Advanced & Applied (ê³ ê¸‰ ë° ì‘ìš©)"
        C --> F[<b>ê³ ê¸‰ CV</b><br>GANs, Diffusion Models, NeRF]
        D --> G[<b>ê³ ê¸‰ NLP</b><br>LLMs, RAG, Fine-tuning]
        E --> H[<b>ê³ ê¸‰ ì‹œê³„ì—´/ì¶”ì²œ</b><br>Transformer-based models]
    end
    
    subgraph "Stage 4: Professionalization (ì „ë¬¸ê°€ ê³¼ì •)"
        F & G & H --> I[ì‹¤ë¬´ í”„ë¡œì íŠ¸ & Kaggle<br>í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¶•]
        I --> J[<b>MLOps ì—­ëŸ‰ ê°•í™”</b><br>Docker, CI/CD, MLflow, Cloud AI]
    end
```

---

[â®ï¸ ì´ì „ ë¬¸ì„œ](./0717_DLì •ë¦¬.md) | [ë‹¤ìŒ ë¬¸ì„œ â­ï¸](./0721_DLì •ë¦¬.md)