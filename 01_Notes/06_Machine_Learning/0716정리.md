# 머신러닝 모델 평가 및 데이터 시각화, 자연어 처리 종합 실습

## 문서 목표
본 문서는 머신러닝 모델의 성능을 객관적으로 평가하는 다양한 지표(이진 분류 및 회귀)와 시각화 기법을 심층적으로 다룹니다. 또한, Matplotlib과 Seaborn을 활용한 데이터 시각화의 기초부터 고급 기술까지 폭넓게 학습하며, 자연어 처리(NLP)의 핵심 개념과 텍스트 분석(CountVectorizer, TF-IDF, KoNLPy) 실습을 통해 감정 분석 모델을 구축하는 전 과정을 상세히 설명합니다. 각 섹션별 풍부한 코드 예시와 설명을 통해 실전 머신러닝 역량을 강화하는 데 기여합니다.

---

## 목차
- [1. 머신러닝 모델 평가 종합 실습](#1-머신러닝-모델-평가-종합-실습)
  - [1.1 목표 및 개요](#11-목표-및-개요)
  - [1.2 사용 데이터셋](#12-사용-데이터셋)
  - [1.3 이진분류 모델 평가 (유방암 데이터셋)](#13-이진분류-모델-평가-유방암-데이터셋)
  - [1.4 회귀 모델 평가 (캘리포니아 주택가격)](#14-회귀-모델-평가-캘리포니아-주택가격)
- [2. Matplotlib & Seaborn 데이터 시각화 완전 정복](#2-matplotlib--seaborn-데이터-시각화-완전-정복)
  - [2.1 환경 설정 및 기초](#21-환경-설정-및-기초)
  - [2.2 기본 차트 생성](#22-기본-차트-생성)
  - [2.3 고급 시각화 기법](#23-고급-시각화-기법)
  - [2.4 통계적 시각화](#24-통계적-시각화)
  - [2.5 실무 응용](#25-실무-응용)
- [3. 자연어 처리 및 텍스트 분석 종합 실습](#3-자연어-처리-및-텍스트-분석-종합-실습)
  - [3.1 목표 및 개요](#31-목표-및-개요)
  - [3.2 텍스트 분석 기초 이론](#32-텍스트-분석-기초-이론)
  - [3.3 벡터화 방법](#33-벡터화-방법)
  - [3.4 한글 텍스트 분석](#34-한글-텍스트-분석)
  - [3.5 감정 분석 모델 구축](#35-감정-분석-모델-구축)
  - [3.6 인터랙티브 시각화](#36-인터랙티브-시각화)

---

## 1. 머신러닝 모델 평가 종합 실습

이 섹션에서는 머신러닝 모델의 성능을 객관적으로 평가하는 다양한 지표와 시각화 기법을 실습합니다.

### 1.1 목표 및 개요

-   **이진분류 모델 평가지표** 이해 및 실습: 정확도, 정밀도, 재현율, F1-score, ROC-AUC 등 이진 분류 모델의 핵심 평가 지표들을 학습하고 실제 데이터셋에 적용합니다.
-   **회귀 모델 평가지표** 이해 및 실습: MAE, MSE, RMSE, R² 등 회귀 모델의 성능을 측정하는 주요 지표들을 학습하고 실습합니다.
-   **혼동행렬, ROC 곡선, 회귀 평가지표 시각화**: 각 평가 지표의 의미를 시각적으로 이해하고, 모델의 강점과 약점을 파악하는 방법을 익힙니다.

### 1.2 사용 데이터셋

-   **유방암 데이터셋 (Breast Cancer Wisconsin)**: Scikit-learn에서 제공하는 이진 분류 데이터셋으로, 유방암의 악성/양성 여부를 분류하는 모델 평가 실습에 사용됩니다.
-   **캘리포니아 주택가격 데이터셋 (California Housing)**: Scikit-learn에서 제공하는 회귀 데이터셋으로, 주택 가격을 예측하는 모델 평가 실습에 사용됩니다.
-   **아이리스 데이터셋 (Iris Plants)**: Scikit-learn에서 제공하는 다중 분류 데이터셋으로, 주로 데이터 시각화 예시에 활용됩니다.

### 1.3 이진분류 모델 평가 (유방암 데이터셋)

유방암 데이터셋을 사용하여 이진 분류 모델(Logistic Regression)을 학습하고, 다양한 평가 지표를 통해 모델의 성능을 분석합니다.

#### 1.3.1 데이터 준비 및 전처리

```python
import pandas as pd
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# 1. 유방암 데이터셋 로드
cancer = load_breast_cancer()
X = pd.DataFrame(cancer.data, columns=cancer.feature_names)
y = cancer.target

# 2. 타겟 변수 변환 (원본: 0: malignant(악성), 1: benign(양성))
# 실습의 편의를 위해 0: 양성(benign), 1: 악성(malignant)으로 변경
# 즉, 0이었던 악성을 1로, 1이었던 양성을 0으로 변경
y_changed = np.where(y == 0, 1, 0) 

# 3. 데이터 분할 (훈련 세트와 테스트 세트)
X_train, X_test, y_train, y_test = train_test_split(X, y_changed, test_size=0.3, random_state=42, stratify=y_changed)

# 4. 특성 스케일링 (Logistic Regression은 스케일링에 민감)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 5. 로지스틱 회귀 모델 학습
model_lr = LogisticRegression(max_iter=10000, random_state=42)
model_lr.fit(X_train_scaled, y_train)

# 6. 예측 수행
y_pred_lr = model_lr.predict(X_test_scaled)
y_proba_lr = model_lr.predict_proba(X_test_scaled)[:, 1] # 양성 클래스(1)에 대한 확률

print("--- 이진 분류 모델 평가 준비 완료 ---")
```

#### 1.3.2 혼동행렬 (Confusion Matrix)

혼동행렬은 분류 모델의 성능을 시각적으로 요약하여 보여주는 표입니다. 실제 클래스와 모델이 예측한 클래스 간의 관계를 나타내어, 모델이 어떤 유형의 오류를 범하는지 파악하는 데 매우 유용합니다.

-   **TN (True Negative)**: 실제 음성(Negative)을 음성으로 올바르게 예측한 경우 (예: 양성 종양을 양성으로 예측)
-   **FP (False Positive)**: 실제 음성(Negative)을 양성(Positive)으로 잘못 예측한 경우 (예: 양성 종양을 악성으로 오진)
-   **FN (False Negative)**: 실제 양성(Positive)을 음성(Negative)으로 잘못 예측한 경우 (예: 악성 종양을 양성으로 오진)
-   **TP (True Positive)**: 실제 양성(Positive)을 양성으로 올바르게 예측한 경우 (예: 악성 종양을 악성으로 예측)

```python
# 혼동행렬 계산
cm = confusion_matrix(y_test, y_pred_lr)
print("--- 혼동행렬 (Confusion Matrix) ---")
print(cm)

# 혼동행렬 시각화
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['양성 (Benign)', '악성 (Malignant)'],
            yticklabels=['양성 (Benign)', '악성 (Malignant)'])
plt.title('Confusion Matrix for Breast Cancer Prediction')
plt.ylabel('실제 레이블')
plt.xlabel('예측 레이블')
plt.show()
```

#### 1.3.3 분류 평가지표

혼동행렬을 기반으로 계산되는 다양한 분류 평가지표들은 모델의 성능을 수치적으로 나타냅니다.

-   **정확도 (Accuracy)**: 전체 예측 중 올바르게 예측한 비율. `(TP + TN) / (TP + TN + FP + FN)`
    -   가장 직관적인 지표이지만, 클래스 불균형이 심한 데이터셋에서는 오해의 소지가 있습니다.
-   **정밀도 (Precision)**: 모델이 '양성'으로 예측한 것 중에서 실제로 '양성'인 비율. `TP / (TP + FP)`
    -   거짓 양성(False Positive)을 줄이는 것이 중요할 때 사용합니다. (예: 스팸 메일 분류, 오진율 감소)
-   **재현율 (Recall, Sensitivity)**: 실제 '양성'인 것 중에서 모델이 올바르게 '양성'으로 예측한 비율. `TP / (TP + FN)`
    -   거짓 음성(False Negative)을 줄이는 것이 중요할 때 사용합니다. (예: 암 진단, 실제 환자를 놓치지 않는 것)
-   **F1-score**: 정밀도와 재현율의 조화평균. `2 * (Precision * Recall) / (Precision + Recall)`
    -   정밀도와 재현율이 모두 중요할 때 사용합니다. 두 지표 중 어느 하나가 극단적으로 낮을 경우 F1-score도 낮아집니다.

```python
# 분류 평가지표 계산
accuracy = accuracy_score(y_test, y_pred_lr)
precision = precision_score(y_test, y_pred_lr)
recall = recall_score(y_test, y_pred_lr)
f1 = f1_score(y_test, y_pred_lr)

print("--- 분류 평가지표 ---")
print(f"정확도 (Accuracy): {accuracy:.4f}")
print(f"정밀도 (Precision): {precision:.4f}")
print(f"재현율 (Recall): {recall:.4f}")
print(f"F1-score: {f1:.4f}")

# classification_report를 통한 종합 보고서
print("\n--- 분류 종합 보고서 (Classification Report) ---")
print(classification_report(y_test, y_pred_lr, target_names=['양성 (Benign)', '악성 (Malignant)']))
```

#### 1.3.4 ROC 곡선 및 AUC

ROC (Receiver Operating Characteristic) 곡선은 이진 분류 모델의 성능을 다양한 임계값(Threshold)에서 시각적으로 평가하는 도구입니다. AUC (Area Under the Curve)는 ROC 곡선 아래 면적을 의미하며, 모델의 전반적인 분류 성능을 하나의 수치로 요약합니다.

-   **ROC 곡선**: 모델의 True Positive Rate (TPR, 재현율)과 False Positive Rate (FPR) 간의 관계를 다양한 임계값에서 보여줍니다. 곡선이 왼쪽 상단에 가까울수록 좋은 모델입니다.
-   **AUC (Area Under Curve)**: ROC 곡선 아래 면적. 0.5(무작위 분류)부터 1.0(완벽한 분류) 사이의 값을 가집니다. AUC 값이 1에 가까울수록 모델이 양성 클래스와 음성 클래스를 잘 구분한다는 의미입니다. 클래스 불균형 데이터셋에서 모델의 성능을 평가하는 데 특히 유용합니다.

```python
from sklearn.metrics import roc_curve, roc_auc_score

# ROC 곡선 데이터 계산
fpr, tpr, thresholds = roc_curve(y_test, y_proba_lr)

# AUC 점수 계산
roc_auc = roc_auc_score(y_test, y_proba_lr)

print(f"--- ROC AUC 점수: {roc_auc:.4f} ---")

# ROC 곡선 시각화
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)') # 대각선 (무작위 분류기)
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve for Breast Cancer Prediction')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()
```

### 1.4 회귀 모델 평가 (캘리포니아 주택가격)

캘리포니아 주택가격 데이터셋을 사용하여 회귀 모델(Linear Regression)을 학습하고, 주요 회귀 평가지표를 통해 모델의 성능을 분석합니다.

#### 1.4.1 회귀 평가지표

회귀 모델의 성능을 평가하는 주요 지표들은 예측값과 실제값 간의 오차를 기반으로 합니다.

-   **MAE (Mean Absolute Error)**: 평균 절대 오차. 예측값과 실제값 차이의 절댓값 평균. `(1/n) * sum(|y_true - y_pred|)`
    -   오차의 크기를 직관적으로 이해하기 쉽습니다. 이상치에 덜 민감합니다.
-   **MSE (Mean Squared Error)**: 평균 제곱 오차. 예측값과 실제값 차이의 제곱 평균. `(1/n) * sum((y_true - y_pred)^2)`
    -   오차가 클수록 더 큰 페널티를 부여합니다. 이상치에 민감합니다.
-   **RMSE (Root Mean Squared Error)**: 평균 제곱근 오차. MSE에 제곱근을 취한 값. `sqrt(MSE)`
    -   오차의 단위를 실제 타겟 변수와 동일하게 만들어 해석이 용이합니다.
-   **R² (R-squared, 결정계수)**: 모델이 분산을 얼마나 잘 설명하는지 나타내는 지표. 0과 1 사이의 값을 가지며, 1에 가까울수록 모델의 설명력이 높습니다. `1 - (MSE / Variance(y_true))`
    -   모델의 예측이 실제값의 변동성을 얼마나 잘 설명하는지 나타냅니다.

```python
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. 캘리포니아 주택가격 데이터셋 로드
housing = fetch_california_housing()
X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)
y_housing = housing.target

# 2. 데이터 분할
X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_housing, y_housing, test_size=0.3, random_state=42)

# 3. 특성 스케일링
scaler_h = StandardScaler()
X_train_scaled_h = scaler_h.fit_transform(X_train_h)
X_test_scaled_h = scaler_h.transform(X_test_h)

# 4. 선형 회귀 모델 학습
model_lr_h = LinearRegression()
model_lr_h.fit(X_train_scaled_h, y_train_h)

# 5. 예측 수행
y_pred_lr_h = model_lr_h.predict(X_test_scaled_h)

# 6. 회귀 평가지표 계산
mae = mean_absolute_error(y_test_h, y_pred_lr_h)
mse = mean_squared_error(y_test_h, y_pred_lr_h)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_h, y_pred_lr_h)

print("--- 회귀 모델 평가지표 (캘리포니아 주택가격) ---")
print(f"MAE (Mean Absolute Error): {mae:.4f}")
print(f"MSE (Mean Squared Error): {mse:.4f}")
print(f"RMSE (Root Mean Squared Error): {rmse:.4f}")
print(f"R² (R-squared): {r2:.4f}")
```

#### 1.4.2 시각화 기법

회귀 모델의 성능을 시각적으로 평가하는 것은 수치 지표만으로는 파악하기 어려운 모델의 특성이나 오차 패턴을 이해하는 데 도움을 줍니다.

-   **실제값 vs 예측값 산점도 (Actual vs Predicted Plot)**:
    -   X축에 실제값, Y축에 예측값을 놓고 산점도를 그립니다. 점들이 대각선(y=x)에 가까울수록 모델의 예측 성능이 좋다는 것을 의미합니다.
    ```python
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_test_h, y=y_pred_lr_h, alpha=0.6)
    plt.plot([y_test_h.min(), y_test_h.max()], [y_test_h.min(), y_test_h.max()], 'r--', lw=2) # y=x 대각선
    plt.xlabel('실제 주택 가격')
    plt.ylabel('예측 주택 가격')
    plt.title('실제값 vs 예측값 산점도')
    plt.grid(True, alpha=0.3)
    plt.show()
    ```
-   **잔차 분석 (Residual Plot)**:
    -   잔차(Residual)는 실제값과 예측값의 차이(`y_true - y_pred`)입니다. 잔차 플롯은 X축에 예측값, Y축에 잔차를 놓고 그립니다. 이상적인 잔차 플롯은 잔차들이 0을 중심으로 무작위로 분포하며, 특정 패턴을 보이지 않아야 합니다. 패턴이 보인다면 모델이 데이터의 특정 부분을 잘 설명하지 못하고 있음을 의미합니다.
    ```python
    residuals = y_test_h - y_pred_lr_h
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_pred_lr_h, y=residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--', lw=2) # 잔차 0 기준선
    plt.xlabel('예측 주택 가격')
    plt.ylabel('잔차 (실제값 - 예측값)')
    plt.title('잔차 플롯')
    plt.grid(True, alpha=0.3)
    plt.show()
    ```
-   **예측값 분포 히스토그램 (Histogram of Predicted Values)**:
    -   예측값의 분포를 히스토그램으로 시각화하여 예측값이 실제값의 분포와 얼마나 유사한지, 또는 특정 값으로 편향되어 있는지 등을 파악할 수 있습니다.
    ```python
    plt.figure(figsize=(8, 6))
    sns.histplot(y_pred_lr_h, kde=True, color='skyblue', label='예측값')
    sns.histplot(y_test_h, kde=True, color='orange', alpha=0.0, label='실제값') # 실제값 분포도 함께 표시
    plt.xlabel('주택 가격')
    plt.ylabel('빈도')
    plt.title('예측값 및 실제값 분포 히스토그램')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()
    ```

## 2. Matplotlib & Seaborn 데이터 시각화 완전 정복

데이터 시각화는 데이터의 패턴, 추세, 이상치 등을 직관적으로 파악하고, 머신러닝 모델의 결과를 효과적으로 전달하는 데 필수적인 과정입니다. Matplotlib은 파이썬의 가장 기본적인 시각화 라이브러리이며, Seaborn은 Matplotlib을 기반으로 통계 그래프를 더 쉽고 아름답게 그릴 수 있도록 돕는 라이브러리입니다.

### 2.1 환경 설정 및 기초

#### 2.1.1 라이브러리 Import 및 한글 폰트 설정

데이터 시각화를 시작하기 전에 필요한 라이브러리를 임포트하고, 한글 텍스트가 깨지지 않도록 폰트 설정을 해줍니다. 특히 Windows 환경에서는 특정 폰트 경로를 지정해야 할 수 있습니다.

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib import font_manager, rc
import seaborn as sns

# 한글 폰트 설정 (Windows 예시)
# 자신의 시스템에 설치된 한글 폰트 경로를 확인하여 설정해야 합니다.
# 예: Malgun Gothic, NanumGothic 등
font_name = font_manager.FontProperties(fname="c:/Windows/Fonts/malgun.ttf").get_name() # Malgun Gothic 폰트 경로 예시
rc('font', family=font_name)

# 마이너스 부호 깨짐 방지
plt.rcParams['axes.unicode_minus'] = False

print("Matplotlib 및 Seaborn 환경 설정 완료.")
```

#### 2.1.2 Matplotlib 기본 구조

Matplotlib으로 그래프를 그릴 때 이해해야 할 핵심 구성 요소는 `Figure`, `Axes`, `Axis`, `Artist`입니다.

-   **Figure (그림)**: 전체 그래프가 그려지는 도화지 역할을 합니다. 하나 이상의 `Axes` 객체를 포함할 수 있습니다. `plt.figure()`로 생성합니다.
-   **Axes (축)**: 실제 그래프(플롯)가 그려지는 영역입니다. x축, y축, 제목, 레이블 등을 포함합니다. `Figure` 안에 여러 개의 `Axes`가 있을 수 있습니다. `fig.add_subplot()` 또는 `plt.subplots()`로 생성합니다.
-   **Axis (축)**: `Axes` 내의 개별 축(x축, y축)을 의미합니다. 눈금(ticks)과 눈금 레이블(tick labels)을 관리합니다.
-   **Artist (예술가)**: `Figure` 내의 모든 요소를 통칭합니다. 텍스트, 선, 점, 이미지 등 그래프를 구성하는 모든 시각적 요소들이 `Artist` 객체입니다.

```python
# Matplotlib 기본 구조 예시
fig = plt.figure(figsize=(10, 6)) # Figure 객체 생성
ax = fig.add_subplot(1, 1, 1)    # Axes 객체 생성 (1행 1열 중 첫 번째)

# Axes에 데이터 플롯
ax.plot([1, 2, 3, 4], [1, 4, 2, 3])

# Axes에 제목, 레이블 설정
ax.set_title('Axes Title')
ax.set_xlabel('X-axis Label')
ax.set_ylabel('Y-axis Label')

plt.show()
```

### 2.2 기본 차트 생성

Matplotlib과 Seaborn을 사용하여 다양한 종류의 기본 차트를 생성하는 방법을 학습합니다. 데이터의 특성과 전달하고자 하는 메시지에 따라 적절한 차트 유형을 선택하는 것이 중요합니다.

#### 2.2.1 라인 차트 (Line Chart)

라인 차트는 시간의 흐름에 따른 데이터의 변화나 연속적인 데이터의 추세를 보여줄 때 유용합니다. `plt.plot()` 함수를 사용하여 그립니다.

```python
# 데이터 생성
x = np.linspace(0, 10, 100) # 0부터 10까지 100개의 등간격 숫자 생성
y = np.sin(x) # x에 대한 sin 값

plt.figure(figsize=(10, 6)) # Figure 크기 설정
plt.plot(x, y, label='Sine Wave', color='skyblue', linestyle='-', marker='o', markersize=4, linewidth=2)

plt.title('시간에 따른 데이터 변화 (사인 곡선)', fontsize=16, fontweight='bold')
plt.xlabel('시간 (단위)', fontsize=12)
plt.ylabel('값', fontsize=12)
plt.legend(fontsize=11) # 범례 표시
plt.grid(True, alpha=0.6, linestyle='--') # 격자선 표시
plt.show()
```

#### 2.2.2 데이터 타입별 처리

데이터 시각화 시, 입력 데이터의 타입(List, NumPy Array, Pandas Series/DataFrame)에 따라 처리 방식이 달라질 수 있습니다. Matplotlib과 NumPy는 벡터화된 연산을 지원하여 효율적인 데이터 처리를 가능하게 합니다.

-   **List vs NumPy 벡터 연산**: 
    -   Python의 `List`는 각 요소를 개별적으로 처리해야 하므로 대규모 데이터에서는 비효율적입니다.
    -   `NumPy Array`는 벡터화된 연산을 지원하여 배열 전체에 대한 연산을 빠르게 수행할 수 있습니다. 이는 대규모 수치 데이터 처리 및 시각화에 필수적입니다.
-   **List comprehension을 활용한 데이터 변환**: `List` 데이터를 효율적으로 변환하거나 필터링할 때 유용합니다.
-   **NumPy array의 벡터화 연산**: `NumPy` 배열은 수학적 연산을 배열의 모든 요소에 한 번에 적용할 수 있어 코드를 간결하고 빠르게 만듭니다.

```python
# 데이터 생성 (List vs NumPy Array)
list_data = [1, 2, 3, 4, 5]
numpy_data = np.array([1, 2, 3, 4, 5])

# List comprehension을 활용한 변환
list_squared = [x**2 for x in list_data]
print(f"List comprehension 결과: {list_squared}")

# NumPy array의 벡터화 연산
numpy_squared = numpy_data**2
print(f"NumPy 벡터화 연산 결과: {numpy_squared}")

# Pandas Series/DataFrame 사용 예시
df_example = pd.DataFrame({'A': [10, 20, 30], 'B': [1, 2, 3]})
plt.figure(figsize=(8, 5))
plt.bar(df_example['A'], df_example['B'])
plt.title('Pandas DataFrame을 이용한 막대 그래프')
plt.xlabel('값 A')
plt.ylabel('값 B')
plt.show()
```

### 2.3 고급 시각화 기법

Matplotlib과 Seaborn은 기본적인 차트 외에도 다양한 고급 시각화 기능을 제공하여 데이터의 복잡한 패턴을 효과적으로 표현할 수 있습니다.

#### 2.3.1 Seaborn 스타일 적용

Seaborn은 Matplotlib 위에 구축되어 있어, Matplotlib의 기본 스타일을 변경하여 더 미려하고 통계적인 그래프를 쉽게 생성할 수 있습니다. `sns.set_style()` 함수를 사용하여 다양한 스타일을 적용할 수 있습니다.

-   **`darkgrid`**: 어두운 배경에 격자선이 있는 스타일 (기본값)
-   **`whitegrid`**: 밝은 배경에 격자선이 있는 스타일
-   **`dark`**: 어두운 배경 (격자선 없음)
-   **`white`**: 밝은 배경 (격자선 없음)
-   **`ticks`**: 최소한의 스타일 (축 눈금만 표시)

```python
# 데이터 생성
x = np.linspace(0, 10, 100)
y_sin = np.sin(x)
y_cos = np.cos(x)

# darkgrid 스타일 적용
sns.set_style('darkgrid')
plt.figure(figsize=(10, 6))
plt.plot(x, y_sin, label='sin(x)')
plt.plot(x, y_cos, label='cos(x)')
plt.title('Seaborn Darkgrid Style')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend()
plt.show()

# whitegrid 스타일 적용
sns.set_style('whitegrid')
plt.figure(figsize=(10, 6))
plt.plot(x, y_sin, label='sin(x)')
plt.plot(x, y_cos, label='cos(x)')
plt.title('Seaborn Whitegrid Style')
plt.xlabel('X-axis')
plt.ylabel('Y-axis')
plt.legend()
plt.show()

# 다른 스타일도 유사하게 적용 가능
# sns.set_style('dark')
# sns.set_style('white')
# sns.set_style('ticks')
```

#### 2.3.2 다중 함수 비교 차트 (서브플롯 활용)

여러 개의 그래프를 한 Figure 안에 배치하여 비교 분석할 때 서브플롯(Subplot)을 사용합니다. `plt.subplot()` 또는 `plt.subplots()` 함수를 활용할 수 있습니다.

```python
# 데이터 생성
x = np.linspace(-5, 5, 100)

plt.figure(figsize=(12, 10))

# 첫 번째 서브플롯 (2행 1열 중 첫 번째)
plt.subplot(2, 1, 1) 
plt.plot(x, x, label='선형 함수 (y = x)', color='#2E8B57', linewidth=3)
plt.plot(x, x**2, label='2차 함수 (y = x²)', color='#4169E1', linewidth=3)
plt.plot(x, x**3, label='3차 함수 (y = x³)', color='#DC143C', linewidth=3)
plt.title('다양한 함수 비교', fontsize=15)
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True, alpha=0.6)

# 두 번째 서브플롯 (2행 1열 중 두 번째)
plt.subplot(2, 1, 2)
plt.plot(x, np.exp(x), label='지수 함수 (y = e^x)', color='#FF8C00', linewidth=3)
plt.plot(x, np.log(np.abs(x) + 1), label='로그 함수 (y = log(|x|+1))', color='#8A2BE2', linewidth=3)
plt.title('지수 및 로그 함수', fontsize=15)
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True, alpha=0.6)

plt.tight_layout() # 서브플롯 간의 간격 자동 조절
plt.show()
```

### 2.4 통계적 시각화

Seaborn은 통계적 관계를 시각화하는 데 특화된 라이브러리로, Matplotlib보다 더 쉽고 간결하게 복잡한 통계 그래프를 그릴 수 있습니다.

#### 2.4.1 히스토그램 (Histogram)

히스토그램은 데이터의 분포를 막대 그래프 형태로 보여주는 차트입니다. 데이터가 어떤 값 범위에 얼마나 많이 분포하는지 파악하는 데 유용합니다.

-   **데이터 분포**: 데이터의 모양, 중심, 퍼짐 정도를 한눈에 파악할 수 있습니다.
-   **bin 개수**: `bins` 매개변수를 조절하여 막대의 개수(구간의 수)를 변경할 수 있습니다. `bins`가 너무 적으면 정보 손실이, 너무 많으면 노이즈가 많아질 수 있습니다.
-   **밀도 히스토그램 vs 빈도 히스토그램**: 
    -   `sns.histplot(kde=True)`: 막대의 높이가 빈도수가 아닌 밀도(확률)를 나타내며, KDE(Kernel Density Estimate) 곡선을 함께 그려 데이터 분포를 부드럽게 보여줍니다.
    -   `sns.histplot()`: 막대의 높이가 빈도수를 나타냅니다.

```python
# 데이터 생성 (정규 분포를 따르는 무작위 데이터)
data_hist = np.random.randn(1000)

plt.figure(figsize=(12, 6))

# 빈도 히스토그램
plt.subplot(1, 2, 1)
sns.histplot(data_hist, bins=30, color='skyblue', edgecolor='black')
plt.title('빈도 히스토그램')
plt.xlabel('값')
plt.ylabel('빈도')
plt.grid(True, alpha=0.3)

# 밀도 히스토그램 (KDE 곡선 포함)
plt.subplot(1, 2, 2)
sns.histplot(data_hist, bins=30, kde=True, color='lightcoral', edgecolor='black')
plt.title('밀도 히스토그램 (KDE)')
plt.xlabel('값')
plt.ylabel('밀도')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

#### 2.4.2 확률분포 시각화

데이터의 확률 분포를 시각화하여 데이터가 어떤 통계적 분포를 따르는지, 또는 여러 분포를 비교 분석할 수 있습니다.

-   **정규분포**: `scipy.stats.norm`을 사용하여 정규 분포의 확률 밀도 함수(PDF)를 그릴 수 있습니다.
-   **커널 밀도 추정 (KDE)**: 데이터 포인트의 밀도를 추정하여 부드러운 곡선으로 분포를 나타냅니다. `sns.kdeplot()` 함수를 사용합니다.
-   **다양한 분포 비교 분석**: 여러 데이터셋이나 특성들의 분포를 한 그래프에 겹쳐 그려 비교할 수 있습니다.

```python
from scipy.stats import norm

# 데이터 생성 (다양한 분포)
data_norm1 = np.random.normal(loc=0, scale=1, size=1000) # 표준 정규 분포
data_norm2 = np.random.normal(loc=2, scale=0.5, size=1000) # 평균 2, 표준편차 0.5

plt.figure(figsize=(10, 6))

sns.kdeplot(data_norm1, fill=True, color='blue', label='정규 분포 1')
sns.kdeplot(data_norm2, fill=True, color='green', label='정규 분포 2')

# 정규 분포 PDF 곡선 추가 (참고용)
x_pdf = np.linspace(-4, 5, 100)
plt.plot(x_pdf, norm.pdf(x_pdf, loc=0, scale=1), color='darkblue', linestyle='--', label='정규 분포 1 (PDF)')
plt.plot(x_pdf, norm.pdf(x_pdf, loc=2, scale=0.5), color='darkgreen', linestyle='--', label='정규 분포 2 (PDF)')

plt.title('확률 분포 시각화 (KDE 및 PDF)')
plt.xlabel('값')
plt.ylabel('밀도')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

### 2.5 실무 응용

Matplotlib과 Seaborn의 다양한 기능을 조합하여 실무에서 필요한 복잡한 시각화를 구현할 수 있습니다. 다음은 몇 가지 실무 응용 예시입니다.

-   **서브플롯 (Subplots)을 활용한 종합 분석**: `plt.subplots()`를 사용하여 여러 개의 관련 그래프를 한 Figure 안에 배치함으로써, 다양한 관점에서 데이터를 분석하고 비교할 수 있습니다. 예를 들어, 데이터의 분포, 상관관계, 시계열 변화 등을 동시에 보여줄 수 있습니다.

    ```python
    # Iris 데이터셋 로드 (예시)
    iris = sns.load_dataset('iris')

    plt.figure(figsize=(15, 5))

    # 첫 번째 서브플롯: 꽃잎 길이 분포
    plt.subplot(1, 3, 1) # 1행 3열 중 첫 번째
    sns.histplot(iris['petal_length'], kde=True, color='skyblue')
    plt.title('꽃잎 길이 분포')

    # 두 번째 서브플롯: 꽃잎 길이와 너비 산점도
    plt.subplot(1, 3, 2) # 1행 3열 중 두 번째
    sns.scatterplot(x='petal_length', y='petal_width', hue='species', data=iris)
    plt.title('꽃잎 길이 vs 너비')

    # 세 번째 서브플롯: 종별 꽃잎 길이 박스플롯
    plt.subplot(1, 3, 3) # 1행 3열 중 세 번째
    sns.boxplot(x='species', y='petal_length', data=iris)
    plt.title('종별 꽃잎 길이')

    plt.tight_layout() # 서브플롯 간 간격 자동 조절
    plt.show()
    ```

-   **함수별 변화율(기울기) 비교**: 미분 개념을 활용하여 함수의 변화율을 계산하고 시각화함으로써, 데이터의 동적인 특성을 분석할 수 있습니다. 예를 들어, 주식 가격의 변화율이나 인구 증가율 등을 시각화할 때 유용합니다.

    ```python
    # 데이터 생성
    x = np.linspace(0.1, 10, 100)
    y_log = np.log(x)
    y_sqrt = np.sqrt(x)

    # 변화율 (미분 근사) 계산
    dy_log = np.diff(y_log) / np.diff(x)
    dy_sqrt = np.diff(y_sqrt) / np.diff(x)

    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.plot(x, y_log, label='y = log(x)')
    plt.plot(x, y_sqrt, label='y = sqrt(x)')
    plt.title('함수 원본')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.plot(x[:-1], dy_log, label='log(x) 변화율', color='red')
    plt.plot(x[:-1], dy_sqrt, label='sqrt(x) 변화율', color='blue')
    plt.title('함수별 변화율')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()
    ```

-   **고급 색상 팔레트 활용**: Seaborn의 다양한 색상 팔레트(`palette`)를 사용하여 그래프의 미적 품질을 높이고, 데이터의 특정 속성(예: 범주, 강도)을 효과적으로 표현할 수 있습니다. `cmap` (컬러맵)을 사용하여 연속적인 색상 변화를 나타낼 수도 있습니다.

    ```python
    # Iris 데이터셋 로드
    iris = sns.load_dataset('iris')

    plt.figure(figsize=(10, 6))
    # 'viridis' 컬러맵을 사용하여 꽃잎 길이와 너비 산점도 시각화
    sns.scatterplot(x='petal_length', y='petal_width', hue='species', size='sepal_length', 
                    palette='viridis', sizes=(20, 200), data=iris)
    plt.title('Iris 데이터셋: 꽃잎 길이/너비 및 종별 시각화 (viridis 팔레트)')
    plt.xlabel('꽃잎 길이')
    plt.ylabel('꽃잎 너비')
    plt.legend(title='종')
    plt.grid(True, alpha=0.3)
    plt.show()
    ```

## 3. 자연어 처리 및 텍스트 분석 종합 실습

자연어 처리(Natural Language Processing, NLP)는 인간의 언어를 컴퓨터가 이해하고 처리할 수 있도록 하는 인공지능의 한 분야입니다. 이 섹션에서는 텍스트 데이터를 머신러닝 모델이 학습할 수 있는 형태로 변환하는 방법과, 이를 활용하여 감정 분석 모델을 구축하는 과정을 종합적으로 실습합니다.

### 3.1 목표 및 개요

-   **텍스트 전처리 및 벡터화** 기법 이해: 비정형 텍스트 데이터를 정제하고, 단어 또는 문서의 특징을 추출하여 수치형 벡터로 변환하는 다양한 방법을 학습합니다.
-   **한글과 영어 텍스트 분석** 비교: 언어별 특성(예: 한글의 형태소 분석)을 고려한 텍스트 처리 방법을 이해하고, 영어 텍스트 처리와의 차이점을 파악합니다.
-   **감정 분석 모델** 구축 및 평가: 텍스트 데이터를 기반으로 긍정/부정 감정을 분류하는 머신러닝 모델을 구축하고, 그 성능을 평가합니다.
-   **인터랙티브 시각화**를 통한 결과 표현: Plotly와 같은 라이브러리를 활용하여 텍스트 분석 결과를 동적이고 상호작용 가능한 형태로 시각화합니다.

### 3.2 텍스트 분석 기초 이론

텍스트 분석은 컴퓨터가 자연어를 이해하고 처리할 수 있도록 하는 과정입니다. 이는 비정형 텍스트 데이터를 머신러닝 모델이 학습할 수 있는 수치형 데이터로 변환하는 것이 핵심입니다.

#### 3.2.1 텍스트 분석의 핵심 개념

-   **비정형 데이터**: 텍스트는 정해진 구조가 없는 비정형 데이터이므로, 이를 정형화된 형태로 변환하는 과정이 필요합니다.
-   **특성 추출**: 텍스트에서 의미 있는 정보(단어, 구문, 문맥 등)를 추출하여 모델이 학습할 수 있는 특성(Feature)으로 만듭니다.

#### 3.2.2 주요 단계

1.  **텍스트 수집**: 분석하고자 하는 텍스트 데이터를 수집합니다. (예: 뉴스 기사, 소셜 미디어 게시물, 영화 리뷰)
2.  **텍스트 전처리 (Text Preprocessing)**: 수집된 텍스트에서 불필요한 요소(특수문자, 숫자, HTML 태그 등)를 제거하고, 텍스트를 정규화(소문자 변환, 오탈자 수정 등)하는 과정입니다.
3.  **토큰화 (Tokenization)**: 텍스트를 의미 있는 최소 단위(단어, 형태소 등)로 분리합니다. (예: 문장을 단어로, 단어를 형태소로)
4.  **어휘사전 구축 (Vocabulary Building)**: 토큰화된 단어들에 고유한 번호(인덱스)를 할당하여 어휘사전(Vocabulary)을 만듭니다.
5.  **벡터화 (Vectorization)**: 텍스트를 수치형 벡터로 변환합니다. 이는 컴퓨터가 텍스트를 이해하고 계산할 수 있도록 하는 핵심 단계입니다.
6.  **모델 학습 (Model Training)**: 벡터화된 텍스트 데이터를 사용하여 머신러닝 또는 딥러닝 알고리즘을 학습시킵니다.
7.  **모델 평가 및 해석**: 학습된 모델의 성능을 평가하고, 예측 결과를 해석합니다.

### 3.3 벡터화 방법

텍스트 데이터를 머신러닝 모델이 이해할 수 있는 수치형 벡터로 변환하는 과정을 **벡터화(Vectorization)**라고 합니다. 이는 자연어 처리의 핵심 단계 중 하나입니다.

#### 3.3.1 CountVectorizer (단어 빈도 기반 벡터화)

`CountVectorizer`는 문서 집합에서 단어 토큰의 출현 횟수를 세어 DTM(Document-Term Matrix)을 생성합니다. 각 행은 문서를, 각 열은 단어를 나타내며, 셀 값은 해당 문서에 단어가 나타난 횟수입니다.

-   **특징**:
    -   단어의 단순 빈도수만을 기반으로 합니다.
    -   **불용어(Stopwords) 제거**: `stop_words` 매개변수를 통해 '은', '는', '이', '가'와 같이 자주 등장하지만 의미 없는 단어들을 제거할 수 있습니다.
    -   **N-gram 적용 가능**: `ngram_range` 매개변수를 통해 단어 하나(unigram)뿐만 아니라 두 개(bigram) 또는 그 이상의 단어 묶음(n-gram)을 토큰으로 사용할 수 있습니다. 이는 단어의 순서 정보를 일부 반영할 수 있게 합니다.

```python
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# 예시 텍스트 데이터
sample_texts = [
    "나는 사과를 좋아한다",
    "나는 바나나를 좋아한다",
    "사과와 바나나는 맛있다"
]

# CountVectorizer 객체 생성
# stop_words='english': 영어 불용어 제거 (한글은 별도 리스트 필요)
# ngram_range=(1, 2): unigram (단어 하나)과 bigram (단어 두 개 묶음) 모두 사용
vect = CountVectorizer(stop_words=None, ngram_range=(1, 2))

# 텍스트 데이터에 fit (어휘사전 구축) 및 transform (벡터화) 수행
bag_of_words = vect.fit_transform(sample_texts)

# 생성된 어휘사전 (단어 -> 인덱스 매핑) 확인
print("--- CountVectorizer 어휘사전 ---")
print(vect.vocabulary_)

# 벡터화된 결과 (희소 행렬) 확인
print("\n--- 벡터화된 결과 (희소 행렬) ---")
print(bag_of_words)

# 희소 행렬을 DataFrame으로 변환하여 확인
df_bow = pd.DataFrame(bag_of_words.toarray(), columns=vect.get_feature_names_out())
print("\n--- 벡터화된 결과 (DataFrame) ---")
print(df_bow)
```

#### 3.3.2 TF-IDF (Term Frequency-Inverse Document Frequency)

TF-IDF는 단어의 중요도를 측정하는 통계적 가중치입니다. 단순히 단어의 빈도수만을 고려하는 `CountVectorizer`의 한계를 보완하여, 특정 문서에서는 자주 나타나지만 전체 문서에서는 드물게 나타나는 단어에 더 높은 가중치를 부여합니다. 이는 문서의 특징을 더 잘 나타내는 단어를 식별하는 데 유용합니다.

-   **TF (Term Frequency)**: 특정 문서 내에서 단어가 나타나는 빈도수. 단어가 문서에 많이 등장할수록 중요하다고 판단합니다.
-   **IDF (Inverse Document Frequency)**: 전체 문서 집합에서 단어가 나타나는 빈도수의 역수. 단어가 여러 문서에 걸쳐 자주 등장할수록 중요도가 낮아지고, 특정 문서에만 등장할수록 중요도가 높아집니다.
-   **TF-IDF = TF × IDF**: TF와 IDF를 곱하여 최종 단어 중요도를 계산합니다.

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# 예시 텍스트 데이터 (CountVectorizer와 동일)
sample_texts = [
    "나는 사과를 좋아한다",
    "나는 바나나를 좋아한다",
    "사과와 바나나는 맛있다"
]

# TfidfVectorizer 객체 생성
tfidf = TfidfVectorizer()

# 텍스트 데이터에 fit (어휘사전 구축 및 IDF 계산) 및 transform (TF-IDF 벡터화) 수행
tfidf_matrix = tfidf.fit_transform(sample_texts)

# 생성된 어휘사전 확인
print("--- TF-IDF 어휘사전 ---")
print(tfidf.vocabulary_)

# 벡터화된 결과 (희소 행렬) 확인
print("\n--- 벡터화된 결과 (희소 행렬) ---")
print(tfidf_matrix)

# 희소 행렬을 DataFrame으로 변환하여 확인
df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())
print("\n--- 벡터화된 결과 (DataFrame) ---")
print(df_tfidf)

# 각 단어의 IDF 값 확인
print("\n--- 각 단어의 IDF 값 ---")
for word, idx in tfidf.vocabulary_.items():
    print(f"{word}: {tfidf.idf_[idx]:.4f}")
```

### 3.4 한글 텍스트 분석

영어와 달리 한글은 띄어쓰기만으로는 단어의 의미를 파악하기 어렵습니다. '은', '는', '이', '가'와 같은 조사가 단어에 붙어 의미를 형성하기 때문에, 한글 텍스트를 분석하기 위해서는 **형태소 분석(Morphological Analysis)** 과정이 필수적입니다. 형태소 분석은 단어를 더 이상 쪼갤 수 없는 의미 있는 최소 단위인 형태소로 분리하고, 각 형태소의 품사(명사, 동사, 조사 등)를 태깅하는 과정입니다.

#### 3.4.1 KoNLPy를 이용한 형태소 분석

`KoNLPy`는 파이썬에서 한글 자연어 처리를 위한 라이브러리로, 다양한 형태소 분석기(Okt, Kkma, Komoran, Hannanum 등)를 통합하여 제공합니다. 여기서는 `Okt` (Open Korean Text) 형태소 분석기를 사용한 예시를 보여줍니다.

```python
from konlpy.tag import Okt

# Okt 형태소 분석기 객체 생성
okt = Okt() # 형태소 분석기 객체는 한 번만 생성

sample_korean = "안녕하세요, 머신러닝은 정말 재미있습니다. 만나서 반갑습니다."

print(f"원본 텍스트: {sample_korean}")

# 1. 형태소 분리 (morphs): 텍스트를 형태소 단위로 분리
morphs = okt.morphs(sample_korean)
print(f"\n형태소 분리 (morphs): {morphs}")

# 2. 명사 추출 (nouns): 텍스트에서 명사만 추출
nouns = okt.nouns(sample_korean)
print(f"\n명사 추출 (nouns): {nouns}")

# 3. 구문 추출 (phrases): 텍스트에서 의미 있는 구문(명사구) 추출
phrases = okt.phrases(sample_korean)
print(f"\n구문 추출 (phrases): {phrases}")

# 4. 품사 태깅 (pos): 형태소와 품사 태그를 튜플 형태로 반환
pos_tags = okt.pos(sample_korean)
print(f"\n품사 태깅 (pos): {pos_tags}")
```

#### 3.4.2 한글 토크나이저 함수

`CountVectorizer`나 `TfidfVectorizer`와 같은 Scikit-learn의 벡터화 도구에 한글 텍스트를 적용하기 위해서는 `tokenizer` 매개변수에 형태소 분석기를 연결해야 합니다. 이를 위해 `KoNLPy`의 형태소 분석 기능을 활용하는 커스텀 토크나이저 함수를 정의할 수 있습니다.

```python
from konlpy.tag import Okt
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

okt = Okt() # 형태소 분석기 객체는 한 번만 생성

# 한글 불용어 리스트 (예시)
stop_words_korean = ['은', '는', '이', '가', '을', '를', '에', '에서', '와', '과', '하다', '이다', '되다', '입니다', '습니다', '고', '도', '만', '지만', '그리고', '그래서', '그러나', '하지만', '즉', '따라서', '등', '같은', '수', '것', '저', '그', '이', '때', '때문', '통해', '대한', '위한', '있습니다', '있습니다']

def korean_tokenizer(text):
    """한글 텍스트를 형태소 단위로 토큰화하여 반환합니다.
    품사 태깅 없이 모든 형태소를 사용합니다.
    """
    return okt.morphs(text)

def korean_tokenizer_with_stopwords(text):
    """한글 텍스트를 형태소 단위로 토큰화하고 불용어를 제거하여 반환합니다.
    """
    words = okt.morphs(text)
    return [word for word in words if word not in stop_words_korean]

# 예시 텍스트 데이터
sample_korean_texts = [
    "머신러닝은 정말 재미있는 분야입니다. 파이썬으로 쉽게 배울 수 있습니다.",
    "자연어 처리는 어렵지만 흥미로운 기술입니다. 한글 분석은 KoNLPy를 통해 가능합니다.",
    "데이터 과학은 미래의 핵심 역량입니다. 꾸준히 학습해야 합니다."
]

print("--- 불용어 제거 없는 한글 CountVectorizer ---")
# tokenizer 매개변수에 커스텀 토크나이저 함수 전달
vect_korean = CountVectorizer(tokenizer=korean_tokenizer)
bow_korean = vect_korean.fit_transform(sample_korean_texts)

df_bow_korean = pd.DataFrame(bow_korean.toarray(), columns=vect_korean.get_feature_names_out())
print(df_bow_korean)

print("\n--- 불용어 제거 포함 한글 CountVectorizer ---")
vect_korean_sw = CountVectorizer(tokenizer=korean_tokenizer_with_stopwords)
bow_korean_sw = vect_korean_sw.fit_transform(sample_korean_texts)

df_bow_korean_sw = pd.DataFrame(bow_korean_sw.toarray(), columns=vect_korean_sw.get_feature_names_out())
print(df_bow_korean_sw)
```


### 3.5 감정 분석 모델 구축

감정 분석(Sentiment Analysis)은 텍스트에 담긴 주관적인 의견, 감정, 태도 등을 파악하여 긍정, 부정, 중립 등으로 분류하는 자연어 처리의 한 분야입니다. 여기서는 영화 리뷰 데이터를 사용하여 감정 분석 모델을 구축하는 과정을 실습합니다.

#### 3.5.1 데이터셋

감정 분석 모델 구축을 위해 다음과 같은 영화 리뷰 데이터셋을 활용할 수 있습니다.

-   **네이버 영화 리뷰**: 한글 감정 분석 (긍정/부정)에 주로 사용되는 데이터셋입니다. (실제 데이터 로드는 복잡하므로, 여기서는 가상 데이터를 사용합니다.)
-   **IMDB 영화 리뷰**: 영어 감정 분석 (긍정/부정)에 널리 사용되는 데이터셋입니다.

#### 3.5.2 모델 학습 과정

감정 분석 모델은 일반적으로 다음과 같은 단계로 구축됩니다.

1.  **텍스트 전처리**: 
    -   불필요한 문자(특수문자, 숫자 등) 제거
    -   대소문자 통일 (영어의 경우)
    -   **토큰화**: 텍스트를 단어 또는 형태소 단위로 분리
    -   **불용어 제거**: 의미 없는 단어(조사, 관사 등) 제거
2.  **벡터화**: 전처리된 텍스트를 머신러닝 모델이 이해할 수 있는 수치형 벡터로 변환합니다. `CountVectorizer` 또는 `TF-IDF`가 주로 사용됩니다.
3.  **모델 학습**: 벡터화된 데이터를 사용하여 로지스틱 회귀(Logistic Regression), SVM, 나이브 베이즈(Naive Bayes) 등 분류 알고리즘을 학습시킵니다.
4.  **모델 평가**: 학습된 모델의 성능을 정확도, 분류 리포트(Precision, Recall, F1-score), 혼동행렬 등을 통해 평가합니다.

#### 실습: 감정 분석 모델 구축 (가상 데이터)

```python
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import re # 정규표현식 라이브러리

# 1. 가상 영화 리뷰 데이터셋 생성
# 실제 데이터셋은 CSV 파일 등으로 로드됩니다.
reviews = [
    ("이 영화 정말 최고예요! 강력 추천합니다.", 1), # 긍정
    ("시간 낭비였어요. 너무 지루하고 재미없네요.", 0), # 부정
    ("배우들 연기가 인상 깊었습니다. 스토리는 평범했어요.", 1), # 긍정
    ("최악의 영화. 다시는 안 볼 겁니다.", 0), # 부정
    ("기대 이상으로 좋았습니다. 여운이 남네요.", 1), # 긍정
    ("그냥 그랬어요. 특별히 좋지도 나쁘지도 않네요.", 0), # 중립 (여기서는 부정으로 간주)
    ("정말 감동적인 영화였습니다. 눈물이 났어요.", 1) # 긍정
]

df_reviews = pd.DataFrame(reviews, columns=['text', 'sentiment'])

print("--- 가상 영화 리뷰 데이터셋 ---")
print(df_reviews)

# 2. 텍스트 전처리 함수 정의
def preprocess_text(text):
    # 한글 외 문자 제거 (정규표현식 사용)
    text = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣\s]', '', text) 
    return text

df_reviews['cleaned_text'] = df_reviews['text'].apply(preprocess_text)

# 3. 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(df_reviews['cleaned_text'], df_reviews['sentiment'], test_size=0.3, random_state=42, stratify=df_reviews['sentiment'])

# 4. 벡터화 (CountVectorizer 사용)
# 한글은 띄어쓰기 기준으로 토큰화되므로, 형태소 분석기를 사용하면 더 정확합니다.
# 여기서는 간단한 예시를 위해 기본 토크나이저 사용
vectorizer = CountVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print(f"\n훈련 데이터 벡터 형태: {X_train_vec.shape}")
print(f"테스트 데이터 벡터 형태: {X_test_vec.shape}")

# 5. 모델 학습 (로지스틱 회귀)
sentiment_model = LogisticRegression(max_iter=1000, random_state=42)
sentiment_model.fit(X_train_vec, y_train)

# 6. 모델 평가
y_pred_sentiment = sentiment_model.predict(X_test_vec)

print("\n--- 감정 분석 모델 성능 평가 ---")
print(f"정확도: {accuracy_score(y_test, y_pred_sentiment):.4f}")
print("\n분류 리포트:\n", classification_report(y_test, y_pred_sentiment))
print("\n혼동 행렬:\n", confusion_matrix(y_test, y_pred_sentiment))
```

### 3.6 인터랙티브 시각화

인터랙티브 시각화는 정적인 이미지와 달리 사용자가 그래프와 상호작용(확대/축소, 이동, 정보 확인 등)할 수 있게 하여 데이터 분석을 더욱 풍부하고 효과적으로 만듭니다. 특히 텍스트 분석 결과와 같이 복잡한 데이터를 탐색하고 이해하는 데 큰 도움을 줍니다. `Plotly`는 파이썬에서 인터랙티브 그래프를 쉽게 생성할 수 있는 강력한 라이브러리입니다.

#### 3.6.1 Plotly 활용

`Plotly Express`는 `Plotly`의 고수준 API로, 몇 줄의 코드로도 복잡한 인터랙티브 그래프를 생성할 수 있습니다. 여기서는 감정 분석 결과의 분포를 인터랙티브 산점도로 시각화하는 예시를 보여줍니다.

```python
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
import numpy as np

# 가상 데이터 생성 (감정 분석 결과 시각화용)
# 실제 데이터는 감정 분석 모델의 예측 확률이나 특성 벡터를 차원 축소한 결과일 수 있습니다.
np.random.seed(42)
df_plot = pd.DataFrame({
    'x': np.random.rand(100),
    'y': np.random.rand(100),
    'sentiment': np.random.choice(['긍정', '부정', '중립'], 100),
    'review_text': [f'리뷰 {i}: 이 영화는 {np.random.choice(['정말 좋아요', '별로예요', '그냥 그래요'])} ' for i in range(100)]
})

# 인터랙티브 산점도 생성
# x, y: 데이터 포인트의 위치
# color: 감정(sentiment)에 따라 색상 구분
# hover_name: 마우스를 올렸을 때 표시될 이름 (여기서는 리뷰 텍스트)
# title: 그래프 제목
fig = px.scatter(df_plot, x='x', y='y', color='sentiment',
                 hover_name='review_text', title='감정 분석 결과 인터랙티브 산점도')

fig.show()
```

#### 3.6.2 시각화 종류 (텍스트 분석 관련)

-   **워드 클라우드 (Word Cloud)**: 텍스트 데이터에서 단어의 빈도수를 시각적으로 표현하는 방법입니다. 자주 등장하는 단어일수록 더 크게 표시되어, 텍스트의 주요 키워드를 한눈에 파악할 수 있습니다. (예: `wordcloud` 라이브러리)
-   **감정 분포 차트**: 긍정, 부정, 중립 감정의 비율을 파이 차트나 막대 그래프로 시각화하여 전체적인 감정 분포를 보여줍니다.
-   **모델 성능 비교 (인터랙티브)**: 여러 텍스트 벡터화 방법(CountVectorizer, TF-IDF)이나 다른 머신러닝 모델들의 성능(정확도, F1-score 등)을 인터랙티브 막대 그래프나 라인 차트로 비교하여 보여줄 수 있습니다.
-   **단어 임베딩 시각화**: Word2Vec, GloVe, FastText 등 단어 임베딩 모델을 통해 생성된 단어 벡터들을 PCA나 t-SNE로 차원 축소한 후, 인터랙티브 산점도로 시각화하여 단어 간의 의미적 유사성을 탐색할 수 있습니다.

## 📋 핵심 요약

본 문서는 머신러닝 모델 평가, 데이터 시각화, 그리고 자연어 처리의 핵심 개념과 실전 기법들을 다루었습니다. 각 분야의 주요 내용을 요약하면 다음과 같습니다.

### 머신러닝 모델 평가

머신러닝 모델의 성능을 객관적으로 측정하고 이해하는 것은 모델 개발의 핵심입니다. 문제 유형에 따라 적절한 평가 지표를 선택하고 해석하는 능력이 중요합니다.

-   **이진 분류 모델 평가**: 
    -   **혼동행렬 (Confusion Matrix)**: 모델의 예측과 실제 값 간의 관계(TN, FP, FN, TP)를 시각적으로 보여주어 오류 유형을 파악하는 데 유용합니다.
    -   **주요 평가지표**: 
        -   **정확도 (Accuracy)**: 전체 예측 중 올바른 예측 비율. (클래스 불균형 시 주의)
        -   **정밀도 (Precision)**: 양성 예측 중 실제 양성 비율. (FP 감소 중요 시)
        -   **재현율 (Recall)**: 실제 양성 중 예측한 양성 비율. (FN 감소 중요 시)
        -   **F1-score**: 정밀도와 재현율의 조화평균. (두 지표 모두 중요 시)
    -   **ROC 곡선 및 AUC**: 다양한 임계값에서의 모델 성능(TPR vs FPR)을 시각화하고, AUC 값으로 모델의 전반적인 분류 능력을 평가합니다. (클래스 불균형 시 특히 유용)

-   **회귀 모델 평가**: 
    -   **주요 평가지표**: 
        -   **MAE (Mean Absolute Error)**: 평균 절대 오차. (직관적, 이상치에 덜 민감)
        -   **MSE (Mean Squared Error)**: 평균 제곱 오차. (오차에 큰 페널티, 이상치에 민감)
        -   **RMSE (Root Mean Squared Error)**: MSE의 제곱근. (오차 단위 일치, 해석 용이)
        -   **R² (R-squared, 결정계수)**: 모델의 설명력. (1에 가까울수록 좋음)
    -   **시각화**: 
        -   **실제값 vs 예측값 산점도**: 모델의 예측 경향과 오차 패턴을 시각적으로 확인합니다.
        -   **잔차 분석 (Residual Plot)**: 모델의 오차 분포를 분석하여 모델의 한계나 개선점을 파악합니다.

### 데이터 시각화

데이터 시각화는 데이터의 이해를 돕고, 분석 결과를 효과적으로 전달하는 강력한 도구입니다. Matplotlib과 Seaborn은 파이썬에서 가장 널리 사용되는 시각화 라이브러리입니다.

-   **Matplotlib**: 
    -   **기본 차트 생성**: 라인, 막대, 산점도 등 다양한 기본 차트를 그릴 수 있습니다.
    -   **서브플롯 활용**: `plt.subplot()` 또는 `plt.subplots()`를 사용하여 여러 그래프를 한 Figure 안에 배치하여 비교 분석합니다.
    -   **세부 설정**: 제목, 축 레이블, 범례, 격자선 등 그래프의 모든 요소를 세밀하게 제어할 수 있습니다.
-   **Seaborn**: 
    -   **통계적 시각화**: 히스토그램, KDE 플롯, 박스플롯, 바이올린 플롯 등 통계적 관계를 시각화하는 데 특화되어 있습니다.
    -   **고급 스타일링**: `sns.set_style()`을 통해 Matplotlib의 기본 스타일을 변경하여 더 미려하고 전문적인 그래프를 쉽게 생성할 수 있습니다.
    -   **한글 폰트 설정**: 한글 텍스트가 깨지지 않도록 `matplotlib.rc`를 사용하여 폰트를 설정하는 것이 중요합니다.

### 자연어 처리 (NLP)

자연어 처리는 텍스트 데이터를 컴퓨터가 이해하고 처리할 수 있도록 하는 분야입니다. 텍스트를 수치형으로 변환하는 벡터화 과정이 핵심입니다.

-   **텍스트 벡터화**: 
    -   **CountVectorizer**: 문서 내 단어의 단순 빈도수를 기반으로 텍스트를 벡터로 변환합니다. 불용어 제거 및 N-gram 적용이 가능합니다.
    -   **TF-IDF (Term Frequency-Inverse Document Frequency)**: 단어의 빈도와 문서 내 희소성을 고려하여 단어의 중요도를 가중치로 부여합니다. 특정 문서의 특징을 잘 나타내는 단어를 식별하는 데 유용합니다.
-   **한글 처리**: 
    -   **KoNLPy**: 파이썬에서 한글 자연어 처리를 위한 라이브러리로, 형태소 분석기(Okt, Kkma 등)를 제공합니다.
    -   **형태소 분석**: 한글 텍스트를 의미 있는 최소 단위인 형태소로 분리하고 품사를 태깅하여 정확한 텍스트 분석을 가능하게 합니다.
-   **감정 분석**: 텍스트 데이터를 기반으로 긍정/부정 감정을 분류하는 모델을 구축하고 평가합니다. 텍스트 전처리, 벡터화, 분류 모델 학습, 성능 평가의 과정을 거칩니다.
-   **인터랙티브 시각화**: `Plotly`와 같은 라이브러리를 활용하여 텍스트 분석 결과를 동적이고 상호작용 가능한 형태로 시각화하여 데이터 탐색 및 결과 전달의 효율성을 높입니다.

---

[⏮️ 이전 문서](./0715_ML정리.md) | [다음 문서 ⏭️](../../07_Deep_Learning/01_organize/0717_DL정리.md)
