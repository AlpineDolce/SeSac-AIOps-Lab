<h2>머신러닝 핵심 개념: 지도/비지도 학습, 스케일링, SVM, 이상치 처리 및 실전 적용</h2>
<h4>|&nbsp;&nbsp;작성자: Alpine_Dolce&nbsp;&nbsp;|&nbsp;&nbsp;날짜: 2025-07-09&nbsp;&nbsp;|</h4>

<h2>문서 목표</h2>
본 문서는 지도학습과 비지도학습의 기본 개념부터 데이터 스케일링 기법, 서포트 벡터 머신(SVM)의 원리, 이상치 탐지 및 처리 방법, 그리고 실제 데이터셋을 활용한 전처리 종합 실습까지 머신러닝의 핵심적인 내용들을 다룹니다. 또한, 모델 성능 분석과 실무 적용을 위한 가이드라인을 제시하여 머신러닝 프로젝트의 전반적인 이해를 돕습니다.

<h2>목차</h2>

- [1. 지도학습 vs 비지도학습](#1-지도학습-vs-비지도학습)
  - [1.1 지도학습 (Supervised Learning)](#11-지도학습-supervised-learning)
  - [1.2 비지도학습 (Unsupervised Learning)](#12-비지도학습-unsupervised-learning)
  - [1.3 분류 vs 군집의 차이점](#13-분류-vs-군집의-차이점)
- [2. 데이터 스케일링](#2-데이터-스케일링)
  - [2.1 스케일링의 필요성](#21-스케일링의-필요성)
  - [2.2 스케일링 종류](#22-스케일링-종류)
    - [1. StandardScaler (표준화)](#1-standardscaler-표준화)
    - [2. RobustScaler (로버스트 스케일링)](#2-robustscaler-로버스트-스케일링)
    - [3. MinMaxScaler (최소-최대 정규화)](#3-minmaxscaler-최소-최대-정규화)
    - [4. Normalizer (정규화)](#4-normalizer-정규화)
- [3. 서포트벡터머신(SVM)](#3-서포트벡터머신svm)
  - [3.1 SVM의 핵심 개념](#31-svm의-핵심-개념)
  - [3.2 SVM 분류 예시 (스케일링의 중요성)](#32-svm-분류-예시-스케일링의-중요성)
    - [유방암 데이터셋 실험 결과](#유방암-데이터셋-실험-결과)
  - [3.3 주요 발견사항](#33-주요-발견사항)
  - [3.4 수렴 문제 해결 방법](#34-수렴-문제-해결-방법)
- [4. 이상치 탐지 및 처리](#4-이상치-탐지-및-처리)
  - [4.1 IQR 방법을 이용한 이상치 탐지](#41-iqr-방법을-이용한-이상치-탐지)
    - [계산 공식](#계산-공식)
    - [실습 예제 결과](#실습-예제-결과)
  - [4.2 이상치 처리 전략](#42-이상치-처리-전략)

---

## 1. 지도학습 vs 비지도학습

### 1.1 지도학습 (Supervised Learning)
- **정의**: 출력결과(정답)를 알고 있을 때 사용하는 학습 방식
- **특징**: 레이블이 있는 데이터로 학습
- **사이킷런 패턴**: `fit()` → `predict()`
- **예시**: 분류, 회귀

### 1.2 비지도학습 (Unsupervised Learning)
- **정의**: 결과를 모르는 상태에서 패턴을 찾는 학습 방식
- **특징**: 라벨링이 없는 데이터 사용
- **사이킷런 패턴**: `fit()` → `transform()`
- **활용**: 지도학습 전단계 데이터 분석용으로 많이 사용
- **예시**: 군집화, 차원축소, 연관규칙분석

### 1.3 분류 vs 군집의 차이점

-   **분류 (Classification)**: 미리 정의된 레이블(클래스)이 있는 데이터를 학습하여 새로운 데이터가 어떤 레이블에 속할지 예측하는 지도 학습 기법입니다. 모델은 입력 특성과 레이블 간의 관계를 학습합니다.
    -   **예시**: "이 이미지는 기린일 확률이 0.7, 병아리일 확률이 0.3입니다." (정답이 '기린' 또는 '병아리'로 명확히 정해져 있음)
-   **군집 (Clustering)**: 레이블이 없는 데이터를 사용하여 데이터 내에 존재하는 자연스러운 그룹(클러스터)을 찾아내는 비지도 학습 기법입니다. 데이터 포인트 간의 유사성을 기반으로 그룹을 형성합니다.
    -   **예시**: "이 데이터는 클래스1에 속할 확률이 0.7, 클래스2에 속할 확률이 0.3입니다." (클래스1, 클래스2가 무엇인지는 사전에 정의되지 않고, 데이터로부터 발견된 그룹임)

## 2. 데이터 스케일링

### 2.1 스케일링의 필요성
- 서로 다른 단위와 범위를 가진 특성들을 통일
- 모델 성능 향상과 학습 안정성 확보
- 특히 거리 기반 알고리즘에서 필수적

### 2.2 스케일링 종류

#### 1. StandardScaler (표준화)
```python
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
scaled_data = ss.fit_transform(data)
```
- **특징**: 데이터의 평균을 0, 표준편차를 1로 변환하여 표준 정규 분포와 유사하게 만듭니다.
- **사용 시기**: 
  - 데이터가 정규 분포를 따른다고 가정할 때 (또는 정규 분포에 가깝게 만들고 싶을 때)
  - 선형 모델(로지스틱 회귀, SVM), 신경망 등 **거리 기반 또는 가중치 기반 모델**에서 특성들의 스케일 차이가 모델 성능에 큰 영향을 미칠 때 사용합니다.
- **단점**: 이상치(Outlier)에 민감하게 반응하여 변환된 값의 범위가 크게 왜곡될 수 있습니다.

#### 2. RobustScaler (로버스트 스케일링)
```python
from sklearn.preprocessing import RobustScaler
rb = RobustScaler()
scaled_data = rb.fit_transform(data)
```
- **특징**: 데이터의 중앙값(Median)과 사분위 범위(IQR: Interquartile Range)를 사용하여 스케일을 조정합니다.
- **사용 시기**: 데이터에 **이상치(Outlier)가 많을 때** 매우 효과적입니다. 이상치의 영향을 최소화하면서 데이터를 스케일링해야 할 경우에 사용합니다.
- **장점**: `StandardScaler`나 `MinMaxScaler`와 달리 이상치에 강건(robust)하여 데이터 분포를 왜곡시키지 않습니다.

#### 3. MinMaxScaler (최소-최대 정규화)
```python
from sklearn.preprocessing import MinMaxScaler
mm = MinMaxScaler()
scaled_data = mm.fit_transform(data)
```
- **특징**: 특성값의 최솟값을 0, 최댓값을 1로 하여 모든 특성값을 0과 1 사이의 범위로 변환합니다.
- **사용 시기**: 
  - 특성값의 범위가 명확히 0~1 사이에 와야 할 때 (예: 이미지 픽셀 데이터, 특정 신경망의 입력)
  - 데이터의 분포가 균일하고 이상치가 적을 때 사용합니다.
- **단점**: 이상치에 매우 민감하여 이상치가 존재할 경우 데이터가 좁은 범위에 몰릴 수 있습니다.

#### 4. Normalizer (정규화)
```python
from sklearn.preprocessing import Normalizer
nm = Normalizer()
scaled_data = nm.fit_transform(data)
```
- **특징**: 각 샘플(행)의 유클리드 노름(L2 Norm)을 1로 만들어 벡터의 크기를 정규화합니다. 즉, 각 데이터 포인트(벡터)를 원점으로부터의 거리가 1인 단위 벡터로 변환합니다.
- **사용 시기**: 
  - 주로 텍스트 분류나 클러스터링(군집분석)에서 문서의 길이에 따른 영향을 제거하고 단어의 상대적 중요도를 반영할 때 유용합니다.
  - 코사인 유사도(Cosine Similarity)와 함께 사용될 때 효과적입니다.

## 3. 서포트벡터머신(SVM)

### 3.1 SVM의 핵심 개념
- **기본 아이디어**: 평면에 선을 그어 데이터를 분류
- **고차원 매핑**: 평면에서 분리가 어려운 경우 고차원 공간으로 변환
- **마진 최대화**: 클래스 간 경계를 최대한 멀리 설정

### 3.2 SVM 분류 예시 (스케일링의 중요성)

SVM은 특성들의 스케일에 매우 민감하게 반응하는 모델입니다. 아래 예시는 스케일링 적용 전후의 SVM 모델 성능 차이를 보여줍니다.

```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_breast_cancer
from sklearn.metrics import accuracy_score

# 유방암 데이터셋 로드
cancer = load_breast_cancer()
X, y = cancer.data, cancer.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 1. 스케일링 없이 SVM 학습
svm_no_scale = SVC(random_state=0)
svm_no_scale.fit(X_train, y_train)
print(f"스케일링 없음 - 훈련 정확도: {svm_no_scale.score(X_train, y_train):.3f}")
print(f"스케일링 없음 - 테스트 정확도: {svm_no_scale.score(X_test, y_test):.3f}")

# 2. StandardScaler 적용 후 SVM 학습
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

svm_scaled = SVC(random_state=0)
svm_scaled.fit(X_train_scaled, y_train)
print(f"스케일링 적용 - 훈련 정확도: {svm_scaled.score(X_train_scaled, y_train):.3f}")
print(f"스케일링 적용 - 테스트 정확도: {svm_scaled.score(X_test_scaled, y_test):.3f}")
```

#### 유방암 데이터셋 실험 결과
| 모델 | 스케일링 | 훈련 정확도 | 테스트 정확도 |
|------|----------|-------------|---------------|
| 로지스틱 회귀 | 없음 | 94.6% | 94.4% |
| 로지스틱 회귀 | 적용 | 99.1% | 96.5% |
| SVM | 없음 | 90.4% | 93.7% |
| SVM | 적용 | 99.1% | 96.5% |

### 3.3 주요 발견사항
1. **SVM은 스케일링에 매우 민감**: 스케일링을 적용하지 않으면 성능이 크게 저하될 수 있습니다.
2. **로지스틱 회귀는 상대적으로 덜 민감하지만 수렴 속도 개선**: 로지스틱 회귀도 스케일링을 통해 학습 효율성과 성능을 높일 수 있습니다.
3. **스케일링 적용 시 두 모델 모두 성능 향상**: 특히 거리 기반 모델이나 경사하강법 기반 모델에서 스케일링은 필수적인 전처리 과정입니다.

### 3.4 수렴 문제 해결 방법
```python
# 방법 1: max_iter 증가
LogisticRegression(max_iter=1000)

# 방법 2: 데이터 스케일링 (권장)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

## 4. 이상치 탐지 및 처리

### 4.1 IQR 방법을 이용한 이상치 탐지

#### 계산 공식
- **Q1**: 25% 분위수
- **Q3**: 75% 분위수  
- **IQR**: Q3 - Q1
- **하한 경계**: Q1 - 1.5 × IQR
- **상한 경계**: Q3 + 1.5 × IQR

#### 실습 예제 결과

```python
import pandas as pd
import numpy as np

# 가상 데이터 생성
data = pd.Series([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])

# Q1, Q3 계산
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1

# 이상치 경계 계산
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

print(f"Q1: {Q1}")
print(f"Q3: {Q3}")
print(f"IQR: {IQR}")
print(f"하한 경계: {lower_bound}")
print(f"상한 경계: {upper_bound}")

# 이상치 탐지
outliers = data[(data < lower_bound) | (data > upper_bound)]
print(f"탐지된 이상치: {outliers.tolist()}")

# 이상치 제거 (예시)
data_cleaned = data[(data >= lower_bound) & (data <= upper_bound)]
print(f"이상치 제거 후 데이터 (일부): {data_cleaned.tolist()}")
print(f"처리 전: 평균 {data.mean():.2f}, 표준편차 {data.std():.2f}")
print(f"처리 후: 평균 {data_cleaned.mean():.2f}, 표준편차 {data_cleaned.std():.2f}")
```

### 4.2 이상치 처리 전략
1. **제거**: 완전히 삭제
2. **대체**: 경계값으로 변경 (Winsorizing)
3. **변환**: 로그 변환 등을 통한 완화


---

[⏮️ 이전 문서](./0708_ML정리.md) | [다음 문서 ⏭️](./0710_ML정리.md)