<h2>머신러닝 기초: 데이터 전처리 (결측치, 중복, 정규화, 인코딩)</h2>
<h4>|&nbsp;&nbsp;작성자: Alpine_Dolce&nbsp;&nbsp;|&nbsp;&nbsp;날짜: 2025-07-03&nbsp;&nbsp;|</h4>


<h2>문서 목표</h2>
이 문서는 부트캠프에서 학습한 머신러닝 데이터 전처리 기법에 대한 핵심 이론과 실습 예제를 정리한 자료입니다. 누락 데이터 및 중복 데이터 처리, 데이터 정규화, 타입 변환, 구간 나누기, 원핫 인코딩 등 다양한 전처리 방법을 상세히 다룹니다. 본 문서를 통해 데이터 전처리에 대한 이해를 높이고, 실제 Machine Learning(ML) 모델의 성능 향상에 기여하는 데 도움이 되기를 바랍니다.

<h2>목차</h2>

- [1. Scikit-learn (사이킷런): 머신러닝 핵심 라이브러리](#1-scikit-learn-사이킷런-머신러닝-핵심-라이브러리)
  - [1.1. Scikit-learn 소개](#11-scikit-learn-소개)
  - [1.2. Scikit-learn 라이브러리란?](#12-scikit-learn-라이브러리란)
  - [1.3. Scikit-learn의 주요 구성요소](#13-scikit-learn의-주요-구성요소)
  - [1.4. Scikit-learn 설치 및 환경 설정](#14-scikit-learn-설치-및-환경-설정)
  - [1.5. Scikit-learn의 머신러닝 워크플로우](#15-scikit-learn의-머신러닝-워크플로우)
- [2. 누락 데이터 처리](#2-누락-데이터-처리)
  - [2.1. 누락 데이터의 영향](#21-누락-데이터의-영향)
  - [2.2. 누락 데이터 확인 함수](#22-누락-데이터-확인-함수)
  - [2.3. 누락 데이터 처리 방법](#23-누락-데이터-처리-방법)
    - [1. 삭제 방법 (`dropna()`)](#1-삭제-방법-dropna)
    - [2. 대체 방법 (`fillna()`)](#2-대체-방법-fillna)
- [3. 중복 데이터 처리](#3-중복-데이터-처리)
  - [3.1. 중복 데이터의 문제](#31-중복-데이터의-문제)
  - [3.2. 중복 데이터 확인 및 제거](#32-중복-데이터-확인-및-제거)
- [4. 데이터 정규화 (Normalization \& Scaling)](#4-데이터-정규화-normalization--scaling)
  - [4.1. 정규화의 필요성](#41-정규화의-필요성)
  - [4.2. 정규화 공식 및 방법](#42-정규화-공식-및-방법)
  - [4.3. 정규화 및 단위 변환 예제](#43-정규화-및-단위-변환-예제)
- [5. 데이터 타입 변환](#5-데이터-타입-변환)
  - [5.1. 타입 변환의 필요성](#51-타입-변환의-필요성)
  - [5.2. 타입 변환 예제](#52-타입-변환-예제)
  - [5.3. 주요 타입 변환 함수](#53-주요-타입-변환-함수)
- [6. 구간 나누기 (Binning)](#6-구간-나누기-binning)
  - [6.1. 구간 나누기의 필요성](#61-구간-나누기의-필요성)
  - [6.2. 구간 나누기 예제](#62-구간-나누기-예제)
  - [6.3. 구간 나누기 함수](#63-구간-나누기-함수)
- [7. 원핫 인코딩 (One-Hot Encoding)](#7-원핫-인코딩-one-hot-encoding)
  - [7.1. 원핫 인코딩의 필요성](#71-원핫-인코딩의-필요성)
  - [7.2. 원핫 인코딩 개념](#72-원핫-인코딩-개념)
  - [7.3. 원핫 인코딩 구현](#73-원핫-인코딩-구현)
  - [7.4. 원핫 인코딩 과정 및 복원](#74-원핫-인코딩-과정-및-복원)
- [8. 실전 예제: Iris 데이터셋 종합 처리](#8-실전-예제-iris-데이터셋-종합-처리)
- [9. 핵심 요약](#9-핵심-요약)
  - [9.1. 데이터 전처리 절차 요약](#91-데이터-전처리-절차-요약)
  - [9.2. 데이터 변환 기법 요약](#92-데이터-변환-기법-요약)
  - [9.3. 데이터 타입 분류 및 주요 주의사항](#93-데이터-타입-분류-및-주요-주의사항)

---

## 1. Scikit-learn (사이킷런): 머신러닝 핵심 라이브러리

### 1.1. Scikit-learn 소개

Scikit-learn (사이킷런)은 파이썬으로 머신러닝을 수행하는 데 가장 널리 사용되는 오픈소스 라이브러리입니다. 2007년 David Cournapeau에 의해 처음 개발되었으며, 이후 활발한 커뮤니티 기여를 통해 지속적으로 발전해왔습니다.

### 1.2. Scikit-learn 라이브러리란?

Scikit-learn은 다양한 머신러닝 알고리즘과 유틸리티를 효율적으로 사용할 수 있도록 설계된 파이썬 라이브러리입니다. 다음과 같은 핵심적인 특징을 가집니다:

1.  **일관된 API**: 모든 모델과 변환기가 `fit()`, `transform()`, `predict()`와 같은 일관된 메서드 이름을 사용하여 직관적이고 사용하기 쉽습니다. 이는 다른 모델이나 전처리 기법으로 쉽게 교체하며 실험할 수 있게 합니다.
2.  **다양한 알고리즘 지원**: 분류(Classification), 회귀(Regression), 클러스터링(Clustering), 차원 축소(Dimensionality Reduction) 등 지도 학습 및 비지도 학습의 광범위한 알고리즘을 제공합니다.
3.  **데이터 전처리 및 특성 공학 도구**: 데이터 스케일링, 인코딩, 결측치 처리, 특성 선택 등 머신러닝 워크플로우의 핵심인 데이터 전처리 및 특성 공학 기능을 강력하게 지원합니다.
4.  **모델 선택 및 평가 도구**: 교차 검증(Cross-validation), 하이퍼파라미터 튜닝(GridSearchCV, RandomizedSearchCV), 다양한 성능 평가 지표 등을 제공하여 모델의 성능을 객관적으로 검증하고 최적화할 수 있습니다.
5.  **활발한 커뮤니티 및 문서**: 풍부한 예제와 잘 정리된 문서를 통해 학습 및 문제 해결에 용이하며, 지속적인 업데이트와 개선이 이루어지고 있습니다.

### 1.3. Scikit-learn의 주요 구성요소

Scikit-learn은 기능별로 다양한 모듈로 구성되어 있으며, 주요 모듈은 다음과 같습니다:

*   **`sklearn.base`**: 모든 Scikit-learn 추정기(Estimator)의 기본 클래스를 정의합니다. `fit`, `transform`, `predict` 등의 메서드가 여기서 정의됩니다.
*   **`sklearn.preprocessing`**: 데이터 스케일링(MinMaxScaler, StandardScaler, RobustScaler), 범주형 인코딩(OneHotEncoder, LabelEncoder), 다항 특성 생성(PolynomialFeatures) 등 다양한 전처리 기능을 제공합니다.
*   **`sklearn.impute`**: 결측치 처리(SimpleImputer, KNNImputer) 기능을 제공합니다.
*   **`sklearn.feature_selection`**: 특성 선택(SelectKBest, RFE) 기능을 제공하여 모델 학습에 가장 중요한 특성을 선별할 수 있게 합니다.
*   **`sklearn.feature_selection`**: 특성 선택(SelectKBest, RFE) 기능을 제공하여 모델 학습에 가장 중요한 특성을 선별할 수 있게 합니다.
*   **`sklearn.model_selection`**: 데이터셋 분할(train_test_split), 교차 검증(KFold, StratifiedKFold), 하이퍼파라미터 튜닝(GridSearchCV, RandomizedSearchCV) 등 모델 선택 및 평가에 필요한 도구를 제공합니다.
*   **`sklearn.metrics`**: 분류, 회귀, 클러스터링 등 다양한 문제 유형에 대한 성능 평가 지표(accuracy_score, precision_score, recall_score, mean_squared_error, r2_score 등)를 제공합니다.
*   **`sklearn.linear_model`**: 선형 회귀, 로지스틱 회귀, Ridge, Lasso 등 선형 모델을 포함합니다.
*   **`sklearn.tree`**: 의사결정트리 분류기 및 회귀기를 제공합니다.
*   **`sklearn.ensemble`**: 랜덤 포레스트, Gradient Boosting 등 앙상블 모델을 제공합니다.
*   **`sklearn.svm`**: 서포트 벡터 머신(SVM) 분류기 및 회귀기를 제공합니다.
*   **`sklearn.neighbors`**: K-최근접 이웃(KNN) 분류기 및 회귀기를 제공합니다.
*   **`sklearn.cluster`**: K-Means, DBSCAN 등 클러스터링 알고리즘을 제공합니다.
*   **`sklearn.decomposition`**: PCA, NMF 등 차원 축소 알고리즘을 제공합니다.

### 1.4. Scikit-learn 설치 및 환경 설정

Scikit-learn은 파이썬 패키지 관리자인 `pip`를 사용하여 쉽게 설치할 수 있습니다. 설치 전에 `numpy`와 `scipy`가 미리 설치되어 있어야 합니다.

```bash
pip install numpy scipy scikit-learn
```

아나콘다(Anaconda) 환경을 사용한다면, 다음 명령어를 통해 설치할 수 있습니다.

```bash
conda install scikit-learn
```

설치 후에는 파이썬 스크립트나 Jupyter Notebook에서 `import sklearn`을 통해 라이브러리를 사용할 수 있습니다.

### 1.5. Scikit-learn의 머신러닝 워크플로우

Scikit-learn은 머신러닝 프로젝트의 일반적인 워크플로우를 효율적으로 지원합니다.

1.  **데이터 로딩 및 탐색**: Pandas와 NumPy를 사용하여 데이터를 불러오고 기본적인 통계 및 시각화를 통해 데이터를 이해합니다.
2.  **데이터 전처리**: `sklearn.preprocessing`, `sklearn.impute` 모듈의 변환기(Transformer)를 사용하여 결측치 처리, 스케일링, 인코딩 등을 수행합니다.
    *   **변환기(Transformer)의 `fit()` 및 `transform()` 메서드**:
        *   `fit()`: 훈련 데이터로부터 변환에 필요한 파라미터(예: MinMaxScaler의 최솟값/최댓값, StandardScaler의 평균/표준편차)를 학습합니다.
        *   `transform()`: 학습된 파라미터를 사용하여 데이터를 변환합니다.
        *   `fit_transform()`: `fit()`과 `transform()`을 한 번에 수행합니다.
3.  **데이터 분할**: `sklearn.model_selection.train_test_split`을 사용하여 데이터를 훈련 세트와 테스트 세트로 나눕니다.
4.  **모델 선택 및 학습**: `sklearn`의 다양한 알고리즘(Estimator) 중 문제 유형에 맞는 모델을 선택하고, 훈련 세트에 대해 `fit()` 메서드를 호출하여 모델을 학습시킵니다.
5.  **예측**: 학습된 모델의 `predict()` 메서드를 사용하여 새로운 데이터에 대한 예측을 수행합니다. 분류 모델의 경우 `predict_proba()`를 통해 확률을 얻을 수도 있습니다.
6.  **모델 평가**: `sklearn.metrics` 모듈의 다양한 평가 지표를 사용하여 모델의 성능을 객관적으로 측정합니다.
7.  **하이퍼파라미터 튜닝**: `sklearn.model_selection`의 `GridSearchCV`나 `RandomizedSearchCV`를 사용하여 모델의 성능을 최적화합니다.

이러한 일관된 워크플로우는 머신러닝 모델 개발 과정을 체계적이고 효율적으로 만들어 줍니다.

## 2. 누락 데이터 처리

### 2.1. 누락 데이터의 영향

1.  **예측 결과 왜곡**: 머신러닝 및 딥러닝 모델은 누락된 데이터가 있을 경우 정확한 학습을 방해하여 예측 결과의 신뢰성을 떨어뜨립니다.
2.  **모델 성능 저하**: 많은 알고리즘들이 누락된 값을 처리하지 못하거나, 잘못된 방식으로 처리할 경우 모델의 성능이 저하될 수 있습니다.
3.  **분석의 어려움**: 누락된 데이터는 통계 분석이나 시각화 과정에서 오류를 발생시키거나 잘못된 결론을 도출하게 할 수 있습니다.

따라서 보다 예측력이 높고 신뢰성 있는 결과를 얻으려면 누락된 데이터에 대한 적절한 처리가 필수적입니다.

### 2.2. 누락 데이터 확인 함수

Pandas는 DataFrame 내의 누락된 데이터를 효율적으로 확인하기 위한 함수들을 제공합니다.

1.  **`isnull()` 함수**: DataFrame 또는 Series의 각 요소가 `NaN` (Not a Number)인지 여부를 불리언(True/False) 값으로 반환합니다. `NaN`은 누락된 데이터를 나타내는 표준 값입니다.

    ```python
    import pandas as pd
    import numpy as np

    # NaN 값은 numpy를 사용하여 직접 입력 가능
    s = pd.Series([1, 2, 3, 4, np.nan, 5, np.nan])
    print("--- Series의 isnull() 결과 ---")
    print(s.isnull())
    # 출력:
    # 0    False
    # 1    False
    # 2    False
    # 3    False
    # 4     True
    # 5    False
    # 6     True
    # dtype: bool
    ```

2.  **`notnull()` 함수**: `isnull()`과 반대로, 요소가 `NaN`이 아닌지 여부를 불리언 값으로 반환합니다.

    ```python
    print("\n--- Series의 notnull() 결과 ---")
    print(s.notnull())
    # 출력:
    # 0     True
    # 1     True
    # 2     True
    # 3     True
    # 4    False
    # 5     True
    # 6    False
    # dtype: bool
    ```

3.  **`sum()` 함수와 조합**: `isnull()` 또는 `notnull()`의 결과에 `sum()` 함수를 사용하면 `True` 값의 개수(즉, `NaN` 값의 개수)를 효율적으로 확인할 수 있습니다. 이는 각 컬럼별 결측치 현황을 파악하는 데 매우 유용합니다.

    ```python
    print("\n--- Series의 NaN 값 개수 확인 ---")
    print(f"NaN 값 개수: {s.isnull().sum()}개")
    # 출력: NaN 값 개수: 2개

    # DataFrame 전체의 컬럼별 NaN 개수 확인
    # (예시를 위해 data.csv를 다시 로드)
    data_df = pd.read_csv("./data/data.csv")
    print("\n--- DataFrame의 컬럼별 NaN 값 개수 확인 ---")
    print(data_df.isnull().sum())
    # 출력 예시:
    # name      0
    # gender    0
    # age       0
    # height    2  # height 컬럼에 2개의 NaN이 있다고 가정
    # weight    1  # weight 컬럼에 1개의 NaN이 있다고 가정
    # dtype: int64
    ```

### 2.3. 누락 데이터 처리 방법

누락된 데이터를 처리하는 방법은 크게 **삭제**와 **대체** 두 가지로 나눌 수 있습니다. 어떤 방법을 선택할지는 데이터의 특성, 누락된 데이터의 양, 그리고 분석 목표에 따라 달라집니다.

#### 1. 삭제 방법 (`dropna()`)

누락된 데이터가 포함된 행이나 열을 제거하는 방법입니다. 데이터의 양이 충분하고 누락된 데이터의 비율이 매우 낮을 때 고려할 수 있습니다.

```python
# 파일명: exam11_1.py
import pandas as pd

# 예시 데이터 로드 (data.csv에 height 컬럼에 NaN이 있다고 가정)
data = pd.read_csv("./data/data.csv")
print("--- 원본 DataFrame 정보 ---")
print("컬럼명:", data.columns)
print("인덱스:", data.index)
print(data.info())

print("\n--- 'height' 필드의 누락 데이터 확인 ---")
# value_counts(dropna=False)는 NaN 값도 포함하여 빈도수를 계산
print(data['height'].value_counts(dropna=False))
print(data['height'].isnull())    # NaN이면 True
print(data['height'].notnull())   # NaN이 아니면 True
print(f"height 필드 NaN 개수: {data['height'].isnull().sum()}개")

print("\n--- 'height' 필드에 NaN이 있는 행 삭제 ---")
print("삭제 전 데이터 개수:", data.shape) # (행, 열) 튜플

# dropna(subset=['height'], how='any', axis=0) 설명:
# - subset=['height']: 'height' 컬럼에서만 NaN을 검사합니다.
# - how='any': subset에 지정된 컬럼 중 하나라도 NaN이 있으면 해당 행을 삭제합니다.
# - axis=0: 행(row)을 기준으로 삭제합니다. (axis=1은 열(column) 삭제)
data_cleaned = data.dropna(subset=['height'], how='any', axis=0)
print("삭제 후 데이터 개수:", data_cleaned.shape)

print("\n--- 인덱스 재설정 (reset_index) ---")
# dropna()는 기존 인덱스를 유지하므로, 인덱스를 0부터 재설정하여 연속성을 확보합니다.
# drop=True는 기존 인덱스를 새로운 컬럼으로 추가하지 않고 버립니다.
data_cleaned = data_cleaned.reset_index(drop=True)
print(data_cleaned)
```

**`dropna()` 함수 주요 매개변수 요약**:

*   `axis`: `0` (기본값)은 행을 삭제, `1`은 열을 삭제합니다.
*   `how`: `'any'` (기본값)는 하나라도 `NaN`이 있으면 삭제, `'all'`은 모든 값이 `NaN`일 때만 삭제합니다.
*   `thresh`: `NaN`이 아닌 값이 최소한 `thresh`개 이상 있는 행/열만 남기고 나머지를 삭제합니다.
*   `subset`: `NaN`을 검사할 특정 컬럼(열)의 리스트를 지정합니다.
*   `inplace`: `True`로 설정하면 원본 DataFrame을 직접 수정하고, `False` (기본값)는 수정된 새로운 DataFrame을 반환합니다.

#### 2. 대체 방법 (`fillna()`)

누락된 데이터가 많아서 삭제 시 데이터 손실이 크거나 분석이 어려운 경우, 누락된 값을 특정 값으로 채워 넣는 대체(Imputation) 방법을 사용합니다. 이는 데이터의 양을 보존하면서 분석을 가능하게 합니다.

```python
# 파일명: exam11_2.py
import pandas as pd

# 예시 데이터 로드 (height와 weight 컬럼에 NaN이 있다고 가정)
data = pd.read_csv("./data/data.csv")

print("--- 대체 전 누락 데이터 확인 ---")
print(f"height 필드 NaN 개수: {data['height'].isnull().sum()}개")
print(f"weight 필드 NaN 개수: {data['weight'].isnull().sum()}개")

# 평균값으로 대체: 숫자형 데이터의 경우 가장 흔하게 사용되는 방법 중 하나입니다.
mean_height = data['height'].mean()
mean_weight = data['weight'].mean()

# fillna() 함수를 사용하여 NaN 값을 평균값으로 채웁니다.
# inplace=True는 원본 DataFrame을 직접 수정합니다.
data['height'].fillna(mean_height, inplace=True)
data['weight'].fillna(mean_weight, inplace=True)

print("\n--- 대체 후 누락 데이터 확인 ---")
print(f"대체 후 height 필드 NaN 개수: {data['height'].isnull().sum()}개")
print(f"대체 후 weight 필드 NaN 개수: {data['weight'].isnull().sum()}개")
print("\n--- 대체 후 DataFrame (일부) ---")
print(data.head())
```

**대체값 선택 기준**:

*   **평균값 (Mean)**: 데이터가 정규 분포를 따르거나 이상치가 적을 때 적합합니다. 데이터의 전체적인 경향을 유지하는 데 도움이 됩니다.
*   **중간값 (Median)**: 데이터에 이상치(Outlier)가 많거나 분포가 한쪽으로 편향된(skewed) 경우에 평균값보다 더 견고한(robust) 대푯값이 됩니다. 이상치의 영향을 덜 받습니다.
*   **최빈값 (Mode)**: 범주형 데이터(Categorical Data)의 누락된 값을 채울 때 가장 적합합니다. 가장 자주 나타나는 범주로 채워 넣습니다.
*   **이전/다음 값 (Forward/Backward Fill)**: 시계열 데이터와 같이 순서가 중요한 데이터에서 이전 값(`ffill` 또는 `pad`)이나 다음 값(`bfill`)으로 채워 넣는 방법입니다.
*   **예측 모델 사용**: 누락된 값을 예측하는 별도의 머신러닝 모델을 구축하여 채워 넣는 고급 방법도 있습니다.

## 3. 중복 데이터 처리

데이터셋에 중복된 행이 존재할 경우 분석 결과의 정확성을 떨어뜨리고 모델 학습에 불필요한 영향을 줄 수 있습니다. 따라서 중복 데이터를 식별하고 제거하는 과정은 데이터 전처리에서 중요합니다.

### 3.1. 중복 데이터의 문제

1.  **분석 결과 왜곡**: 중복된 데이터는 통계적 분석 결과(예: 평균, 분산)를 왜곡시켜 잘못된 결론을 도출하게 할 수 있습니다.
2.  **모델 성능 저하**: 머신러닝 모델이 중복된 데이터를 반복해서 학습하게 되면 과적합(Overfitting)의 위험이 증가하고, 모델의 일반화 성능이 저하될 수 있습니다.
3.  **처리 시간 증가**: 불필요한 중복 데이터는 데이터 처리 및 모델 학습 시간을 증가시켜 비효율을 초래합니다.

### 3.2. 중복 데이터 확인 및 제거

Pandas는 중복 데이터를 확인하고 제거하는 직관적인 함수들을 제공합니다.

```python
# 파일명: exam11_3.py
import pandas as pd

# 예시 데이터: passenger_code가 중복되는 경우가 있음
data = {
    'passenger_code': ['A101', 'A102', 'A103', 'A101', 'A104', 'A101', 'A103'],
    'target': ['광주', '서울', '부산', '광주', '대구', '광주', '부산'],
    'price': [25000, 27000, 45000, 25000, 35000, 27000, 45000]
}

df = pd.DataFrame(data)
print("--- 원본 데이터 ---")
print(df)

print("\n--- 'passenger_code' 컬럼 기준 중복 데이터 확인 (duplicated()) ---")
# duplicated()는 첫 번째 등장하는 값은 False, 이후 중복되는 값은 True를 반환
duplicated_data = df['passenger_code'].duplicated()
print(duplicated_data)
# 출력:
# 0    False
# 1    False
# 2    False
# 3     True  # A101 중복
# 4    False
# 5     True  # A101 중복
# 6     True  # A103 중복
# Name: passenger_code, dtype: bool

print("\n--- 전체 행이 중복인 경우 제거 (drop_duplicates()) ---")
# 모든 컬럼의 값이 동일한 행만 중복으로 간주하여 제거합니다.
# keep='first' (기본값): 첫 번째 중복만 남기고 나머지는 삭제
df_no_full_duplicates = df.drop_duplicates()
print(df_no_full_duplicates)
# 출력:
#   passenger_code target  price
# 0           A101     광주  25000
# 1           A102     서울  27000
# 2           A103     부산  45000
# 4           A104     대구  35000
# 5           A101     광주  27000  # price가 다르므로 중복 아님

print("\n--- 특정 컬럼('passenger_code') 기준으로 중복 제거 ---")
# subset=['passenger_code']를 사용하여 'passenger_code' 컬럼의 값이 중복되는 행을 제거합니다.
# keep='first' (기본값): 'passenger_code'가 처음 등장하는 행만 남깁니다.
df_unique_passenger = df.drop_duplicates(subset=['passenger_code'])
print(df_unique_passenger)
# 출력:
#   passenger_code target  price
# 0           A101     광주  25000
# 1           A102     서울  27000
# 2           A103     부산  45000
# 4           A104     대구  35000

print("\n--- 두 개 컬럼 조합('passenger_code', 'target')이 중복인 경우 제거 ---")
# subset에 여러 컬럼을 지정하면 해당 컬럼들의 조합이 모두 동일한 경우에만 중복으로 간주합니다.
df_unique_combo = df.drop_duplicates(subset=['passenger_code', 'target'])
print(df_unique_combo)
# 출력:
#   passenger_code target  price
# 0           A101     광주  25000
# 1           A102     서울  27000
# 2           A103     부산  45000
# 4           A104     대구  35000
# 5           A101     광주  27000  # (A101, 광주) 조합은 중복이지만, price가 다르므로 keep='first'에 의해 남을 수 있음. 이 예시에서는 price가 25000인 첫 번째 (A101, 광주)가 남고, 27000인 (A101, 광주)는 중복으로 처리되어 제거됨.
```

**중복 처리 함수 요약**:

*   `duplicated(subset=None, keep='first')`:
    *   DataFrame 또는 Series에서 중복된 값을 불리언 Series로 반환합니다.
    *   `subset`: 중복을 검사할 컬럼(열)의 리스트를 지정합니다. 기본값은 모든 컬럼입니다.
    *   `keep`: `'first'` (기본값)는 첫 번째 중복을 `False`로, 나머지를 `True`로 처리합니다. `'last'`는 마지막 중복을 `False`로, 나머지를 `True`로 처리합니다. `False`는 모든 중복을 `True`로 처리합니다.
*   `drop_duplicates(subset=None, keep='first', inplace=False)`:
    *   DataFrame에서 중복된 행을 제거합니다.
    *   `subset`, `keep`, `inplace` 매개변수는 `duplicated()`와 동일하게 작동합니다.

## 4. 데이터 정규화 (Normalization & Scaling)

데이터 정규화(Normalization) 또는 스케일링(Scaling)은 데이터의 스케일(척도)을 조정하여 모든 특성(Feature)들이 비슷한 범위 내에 있도록 만드는 과정입니다. 이는 머신러닝 모델의 성능에 큰 영향을 미칠 수 있습니다.

### 4.1. 정규화의 필요성

1.  **예측 성능 향상**: 데이터의 단위나 범위가 크게 다를 경우, 특정 특성(예: 값이 큰 특성)이 모델 학습에 지배적인 영향을 미쳐 예측 성능을 저하시킬 수 있습니다. 정규화를 통해 모든 특성들이 공정한 기여를 하도록 만듭니다.
2.  **수렴 속도 향상**: 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘을 사용하는 모델(예: 선형 회귀, 로지스틱 회귀, 신경망)의 경우, 정규화된 데이터를 사용하면 학습 과정의 수렴 속도를 크게 향상시킬 수 있습니다.
3.  **알고리즘의 요구사항**: 거리 기반 알고리즘(예: K-NN, SVM, K-Means)은 특성 간의 거리를 계산하므로, 스케일이 다른 특성들이 존재하면 특정 특성의 영향이 과도하게 반영될 수 있습니다. 정규화는 이러한 문제를 해결합니다.

### 4.2. 정규화 공식 및 방법

가장 일반적인 정규화 방법은 Min-Max 정규화와 Z-score 정규화입니다.

1.  **Min-Max 정규화 (Min-Max Scaling)**:
    *   데이터를 0과 1 사이의 값으로 변환합니다. 특정 범위(예: -1과 1)로 변환할 수도 있습니다.
    *   **공식**: `X_normalized = (X - X_min) / (X_max - X_min)`
    *   **특징**: 데이터의 최솟값과 최댓값을 사용하여 선형적으로 스케일을 조정합니다. 이상치에 민감하게 반응할 수 있습니다.

2.  **Z-score 정규화 (Standardization)**:
    *   데이터를 평균이 0이고 표준편차가 1인 표준 정규 분포 형태로 변환합니다.
    *   **공식**: `X_standardized = (X - μ) / σ` (여기서 `μ`는 평균, `σ`는 표준편차)
    *   **특징**: 데이터의 분포를 정규 분포와 유사하게 만들며, 이상치의 영향을 Min-Max 정규화보다 덜 받습니다. 주로 `StandardScaler` (Scikit-learn)를 통해 구현됩니다.

3.  **Robust 정규화 (Robust Scaling)**:
    *   중간값(Median)과 사분위 범위(IQR: Interquartile Range, Q3 - Q1)를 사용하여 스케일을 조정합니다.
    *   **공식**: `X_robust_scaled = (X - Median) / IQR`
    *   **특징**: 이상치에 매우 강건(robust)합니다. 데이터에 이상치가 많을 때 유용합니다.

### 4.3. 정규화 및 단위 변환 예제

`auto-mpg.csv` 데이터셋을 사용하여 `mpg` (연비) 컬럼을 Min-Max 정규화하고, `MPG` 단위를 `KPL` (Km/Liter)로 환산하는 예제입니다.

```python
# 파일명: exam11_4.py
import pandas as pd

# auto-mpg.csv 파일 로드
data = pd.read_csv('./data/auto-mpg.csv')
print("--- 원본 데이터 정보 ---")
print(data.info())
print(data.head())

# 컬럼명 변경 (가독성을 위해)
data.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 
                'weight', 'acceleration', 'model_year', 'origin', 'car_name']
print("\n--- 컬럼명 변경 후 데이터 (일부) ---")
print(data.head())

print("\n--- 'mpg' 컬럼 Min-Max 정규화 적용 ---")
# Min-Max 정규화 공식 적용
# (원본 값 - 최솟값) / (최댓값 - 최솟값)
data['mpg_normalized'] = (data['mpg'] - data['mpg'].min()) / \
                             (data['mpg'].max() - data['mpg'].min())
print(data[['mpg', 'mpg_normalized']].head())

print("\n--- 'MPG' (Miles Per Gallon)를 'KPL' (Km Per Liter)로 단위 환산 ---")
# 1 마일 = 1.60934 킬로미터
# 1 갤런 = 3.78541 리터
# MPG를 KPL로 변환하는 단위 환산 계수mpg_to_kpl_unit = 1.60934 / 3.78541

# KPL 컬럼 생성 및 소수점 둘째 자리까지 반올림
data['kpl'] = (data['mpg'] * mpg_to_kpl_unit).round(2)
print(data[['mpg', 'kpl']].head())
```

## 5. 데이터 타입 변환

Pandas DataFrame의 컬럼 데이터 타입은 분석의 효율성, 메모리 사용량, 그리고 특정 연산의 가능성에 영향을 미칩니다. 따라서 필요에 따라 데이터 타입을 적절하게 변환하는 것이 중요합니다.

### 5.1. 타입 변환의 필요성

1.  **데이터 정확성**: 숫자형 데이터가 문자열로 잘못 저장된 경우(예: '123'이 아닌 '123a') 이를 숫자형으로 변환해야 올바른 산술 연산을 수행할 수 있습니다.
2.  **메모리 효율성**: 실제로는 작은 정수 범위의 데이터인데 `float64`와 같이 큰 메모리를 차지하는 타입으로 저장되어 있다면, `int8` 등으로 변환하여 메모리 사용량을 최적화할 수 있습니다.
3.  **알고리즘 요구사항**: 머신러닝 알고리즘 중 일부는 특정 데이터 타입(예: 숫자형만 허용)을 요구하거나, 범주형 데이터를 특정 방식으로 인코딩해야 할 수 있습니다.
4.  **범주형 데이터 처리**: 문자열로 된 범주형 데이터를 `category` 타입으로 변환하면 메모리 사용량을 줄이고, Pandas의 범주형 데이터 처리 기능을 활용할 수 있습니다.
5.  **잘못된 데이터 처리**: 데이터 로드 시 특정 기호(예: `?`, `-`)가 숫자형 컬럼에 포함되어 문자열로 인식되는 경우, 이를 `NaN`으로 변환하고 적절히 처리해야 합니다.

### 5.2. 타입 변환 예제

`auto-mpg.csv` 데이터셋에서 `horsepower` 컬럼에 잘못된 데이터(`?`)가 포함되어 있어 문자열로 인식되는 경우를 처리하고, 데이터 타입을 변환하는 예제입니다.

```python
# 파일명: exam11_5.py
import pandas as pd
import numpy as np

# auto-mpg.csv 파일 로드
data = pd.read_csv('./data/auto-mpg.csv')
# 컬럼명 변경 (가독성을 위해)
data.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 
                'weight', 'acceleration', 'model_year', 'origin', 'car_name']

print("--- 초기 데이터 타입 확인 ---")
print(data.dtypes)
print("\n--- 'horsepower' 컬럼의 고유값 확인 (잘못된 데이터 포함 여부) ---")
print(data['horsepower'].unique()) # '?'와 같은 문자열이 포함되어 있을 수 있음

print("\n--- 잘못된 데이터(' ?')를 NaN으로 변환 ---")
# replace(' ?', np.nan)를 사용하여 ' ?' 문자열을 numpy의 NaN으로 대체합니다.
# inplace=True는 원본 DataFrame을 직접 수정합니다.
data['horsepower'].replace(' ?', np.nan, inplace=True)
print(data['horsepower'].head(10)) # 변환 후 상위 10개 데이터 확인

print("\n--- NaN이 포함된 행 삭제 (horsepower 컬럼 기준) ---")
# horsepower 컬럼에 NaN이 있는 행을 삭제합니다.
data.dropna(subset=['horsepower'], axis=0, inplace=True)
print(f"NaN 삭제 후 DataFrame 크기: {data.shape}")

print("\n--- 데이터 타입 변환 전 확인 ---")
print(data.dtypes)

print("\n--- 'horsepower'를 float 타입으로, 'model_year'를 category 타입으로 변환 ---")
# astype() 함수를 사용하여 데이터 타입을 변경합니다.
data['horsepower'] = data['horsepower'].astype('float')
data['model_year'] = data['model_year'].astype('category')

print("\n--- 변환 후 데이터 타입 확인 ---")
print(data.dtypes)
# 출력 예시:
# --- 변환 후 데이터 타입 확인 ---
# mpg             float64
# cylinders         int64
# displacement    float64
# horsepower      float64  # float64로 변경됨
# weight          float64
# acceleration    float64
# model_year     category  # category로 변경됨
# origin            int64
# car_name         object
# dtype: object
```

### 5.3. 주요 타입 변환 함수

*   `astype(dtype)`:
    *   Series 또는 DataFrame의 데이터 타입을 지정된 `dtype`으로 변환합니다.
    *   **예시**: `df['column'].astype('int')`, `df['column'].astype('float')`, `df['column'].astype('str')`
*   `replace(old_value, new_value, inplace=False)`:
    *   Series 또는 DataFrame 내의 특정 값을 다른 값으로 대체합니다.
    *   **예시**: `df['column'].replace('?', np.nan, inplace=True)`
*   `pd.to_numeric(series, errors='coerce')`:
    *   Series를 숫자형으로 강제 변환합니다. 변환할 수 없는 값은 `NaN`으로 처리(`errors='coerce'`)할 수 있어 유용합니다.
*   `'category'` 타입:
    *   Pandas에서 제공하는 특수한 데이터 타입으로, 범주형 데이터를 효율적으로 저장하고 처리할 수 있게 합니다. 문자열 범주보다 메모리 사용량이 적고, 범주형 연산에 최적화되어 있습니다.
    *   **예시**: `df['column'].astype('category')`

## 6. 구간 나누기 (Binning)

구간 나누기(Binning)는 연속형 데이터를 여러 개의 이산적인 구간(bin)으로 나누어 범주형 데이터로 변환하는 전처리 기법입니다. 이는 데이터의 복잡성을 줄이고 특정 패턴을 발견하는 데 도움이 될 수 있습니다.

### 6.1. 구간 나누기의 필요성

1.  **데이터 단순화**: 복잡한 연속형 데이터를 더 적은 수의 범주로 단순화하여 분석을 용이하게 합니다.
2.  **등급 시스템 구축**: 점수나 측정값에 따라 등급(예: A, B, C 등)을 부여하는 시스템을 구축할 때 활용됩니다.
3.  **범주별 분석**: 특정 수치 범위에 속하는 데이터들의 특성을 그룹별로 분석할 때 유용합니다.
4.  **이상치 영향 감소**: 극단적인 이상치의 영향을 줄여 모델의 안정성을 높일 수 있습니다.
5.  **선형성 가정 완화**: 일부 모델이 선형성을 가정할 때, 비선형 관계를 구간 나누기를 통해 선형적으로 근사할 수 있습니다.

### 6.2. 구간 나누기 예제

`auto-mpg.csv` 데이터셋의 `horsepower` (마력) 컬럼을 4개의 구간으로 나누어 등급(`grade`)을 부여하는 예제입니다.

```python
# 파일명: exam11_6.py
import pandas as pd
import numpy as np

# auto-mpg.csv 파일 로드 및 컬럼명 변경
data = pd.read_csv('./data/auto-mpg.csv')
data.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 
                'weight', 'acceleration', 'model_year', 'origin', 'car_name']

# 데이터 전처리 (이전 단계에서 학습한 내용 적용)
# 'horsepower' 컬럼의 ' ?' 값을 NaN으로 변환 후 해당 행 삭제
data['horsepower'].replace(' ?', np.nan, inplace=True)
data.dropna(subset=['horsepower'], axis=0, inplace=True)
# 'horsepower' 컬럼을 숫자형(float)으로 변환
data['horsepower'] = data['horsepower'].astype('float')

print("--- 'horsepower' 컬럼의 분포 확인 (구간 나누기 전) ---")
print(data['horsepower'].describe())

print("\n--- 'horsepower'를 4개 구간으로 나누기 ---")
# np.histogram()을 사용하여 데이터의 분포를 기반으로 구간 경계값(bin_dividers)을 계산합니다.
# bins=4는 데이터를 4개의 동일한 너비의 구간으로 나누라는 의미입니다.
count, bin_dividers = np.histogram(data['horsepower'], bins=4)
print("구간 경계값:", bin_dividers)

# 각 구간에 부여할 라벨명 정의
bin_names = ["D", "C", "B", "A"] # 낮은 값부터 높은 값 순서로 등급 부여

# pd.cut() 함수를 사용하여 'horsepower' 컬럼을 정의된 구간으로 나눕니다.
# x: 구간을 나눌 Series (여기서는 data['horsepower'])
# bins: 구간 경계값 배열 (np.histogram에서 얻은 bin_dividers)
# labels: 각 구간에 부여할 라벨 (bin_names)
# include_lowest=True: 가장 낮은 구간의 경계값(최솟값)을 포함할지 여부. True로 설정하면 첫 번째 구간이 [min, boundary1]이 됩니다.
data["grade"] = pd.cut(x=data['horsepower'], 
                       bins=bin_dividers,
                       labels=bin_names, 
                       include_lowest=True)

print("\n--- 구간 나누기 결과 (power와 grade 컬럼) ---")
print(data[['horsepower', 'grade']].head(10))
print("\n--- 각 등급별 개수 확인 ---")
print(data['grade'].value_counts())
# 출력 예시:
# --- 구간 나누기 결과 (power와 grade 컬럼) ---
#     horsepower grade
# 0        130.0     B
# 1        165.0     A
# 2        150.0     A
# 3        150.0     A
# 4        140.0     B
# 5        198.0     A
# 6        220.0     A
# 7        215.0     A
# 8        225.0     A
# 9        190.0     A
#
# --- 각 등급별 개수 확인 ---
# grade
# A    100
# B     98
# C     95
# D     90
# Name: count, dtype: int64
```

### 6.3. 구간 나누기 함수

*   `numpy.histogram(a, bins)`:
    *   배열 `a`의 값 분포를 기반으로 히스토그램을 계산하고, 각 구간의 빈도수(`count`)와 구간 경계값(`bin_dividers`)을 반환합니다.
    *   `bins`: 구간의 개수(정수) 또는 구간 경계값 배열을 지정합니다.
*   `pandas.cut(x, bins, labels=None, include_lowest=False, right=True)`:
    *   Series `x`의 연속 데이터를 지정된 `bins`를 기준으로 여러 개의 이산적인 구간으로 나눕니다.
    *   `x`: 구간을 나눌 Series 또는 배열.
    *   `bins`: 구간 경계값 배열 또는 구간의 개수(정수). `np.histogram`의 `bin_dividers`를 주로 사용합니다.
    *   `labels`: 각 구간에 부여할 라벨(이름)의 리스트. `bins`의 개수보다 하나 적어야 합니다.
    *   `include_lowest`: 첫 번째 구간의 최솟값을 포함할지 여부. `True`로 설정하면 `[min, boundary1]` 형태가 됩니다.
    *   `right`: 구간의 오른쪽 경계값을 포함할지 여부. `True` (기본값)는 `(a, b]` 형태, `False`는 `[a, b)` 형태가 됩니다.
*   `pandas.qcut(x, q, labels=None)`:
    *   데이터의 분위수(quantile)를 기준으로 데이터를 동일한 개수의 구간으로 나눕니다. 데이터의 분포가 고르지 않을 때 각 구간에 비슷한 수의 데이터가 포함되도록 할 때 유용합니다.

## 7. 원핫 인코딩 (One-Hot Encoding)

원핫 인코딩(One-Hot Encoding)은 머신러닝 모델이 범주형 데이터를 이해하고 처리할 수 있도록 수치형 벡터 형태로 변환하는 가장 일반적인 방법 중 하나입니다.

### 7.1. 원핫 인코딩의 필요성

1.  **수치 데이터만 처리**: 대부분의 머신러닝 및 딥러닝 알고리즘은 숫자형 데이터만을 입력으로 받아들입니다. 따라서 '서울', '부산', '대구'와 같은 범주형 문자열 데이터를 모델에 직접 입력할 수 없습니다.
2.  **순서 정보 제거**: 범주형 데이터에 임의의 숫자(예: 서울=1, 부산=2, 대구=3)를 부여할 경우, 모델이 이 숫자들 사이에 불필요한 순서 관계(예: 부산이 서울보다 2배 크다)가 있다고 오해할 수 있습니다. 원핫 인코딩은 이러한 오해를 방지하고 각 범주를 독립적인 특성으로 표현합니다.

### 7.2. 원핫 인코딩 개념

원핫 인코딩은 각 범주를 고유한 이진(binary) 벡터로 표현합니다. 벡터의 길이는 전체 범주의 개수와 같으며, 해당 범주에 해당하는 위치의 값만 1이고 나머지는 0으로 채워집니다.

**예시**: 4개의 카테고리 (A, B, C, D)가 있을 때

*   **A**: `[1, 0, 0, 0]`
*   **B**: `[0, 1, 0, 0]`
*   **C**: `[0, 0, 1, 0]`
*   **D**: `[0, 0, 0, 1]`

### 7.3. 원핫 인코딩 구현

Scikit-learn 라이브러리의 `OneHotEncoder`를 사용하여 원핫 인코딩을 구현하는 예제입니다. 이전 예제에서 `horsepower`를 기반으로 생성한 `grade` 컬럼을 원핫 인코딩합니다.

```python
# 파일명: exam11_7.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder # Scikit-learn의 OneHotEncoder 임포트

# 이전 예제에서 구간 나누기까지 완료된 데이터 로드 및 전처리
data = pd.read_csv('./data/auto-mpg.csv')
data.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 
                'weight', 'acceleration', 'model_year', 'origin', 'car_name']

# 'horsepower' 컬럼 전처리 (NaN 처리 및 타입 변환)
data['horsepower'].replace(' ?', np.nan, inplace=True)
data.dropna(subset=['horsepower'], axis=0, inplace=True)
data['horsepower'] = data['horsepower'].astype('float')

# 'horsepower'를 4개 구간으로 나누어 'grade' 컬럼 생성
count, bin_dividers = np.histogram(data['horsepower'], bins=4)
bin_names = ["D", "C", "B", "A"]
data["grade"] = pd.cut(x=data['horsepower'], 
                       bins=bin_dividers,
                       labels=bin_names, 
                       include_lowest=True)

print("--- 원핫 인코딩 전 'grade' 컬럼 (일부) ---")
print(data['grade'].head(10))

print("\n--- 원핫 인코딩 수행 ---")
# 1. OneHotEncoder는 2차원 배열을 입력으로 받으므로, 'grade' Series를 reshape(-1, 1)하여 2차원 배열로 변환합니다.
Y_class = np.array(data['grade']).reshape(-1, 1)

# 2. OneHotEncoder 객체 생성
enc = OneHotEncoder()

# 3. fit() 메서드로 인코더를 훈련합니다. (어떤 범주들이 있는지 학습)
enc.fit(Y_class)

# 4. transform() 메서드로 실제 원핫 인코딩을 수행합니다. 결과는 희소 행렬(sparse matrix) 형태입니다.
Y_class_onehot = enc.transform(Y_class)

# 5. toarray() 메서드를 사용하여 희소 행렬을 일반적인 밀집 행렬(dense array)로 변환합니다.
Y_class_onehot = Y_class_onehot.toarray()

print("원핫 인코딩 결과 (처음 10개 행):")
print(Y_class_onehot[:10])
# 출력 예시:
# 원핫 인코딩 결과 (처음 10개 행):
# [[0. 1. 0. 0.]  # B 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]  # A 등급
#  [0. 1. 0. 0.]  # B 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]  # A 등급
#  [1. 0. 0. 0.]] # A 등급

print("\n--- 원핫 인코딩 복원 (argmax() 사용) ---")
# np.argmax(axis=1)는 각 행에서 가장 큰 값(1)의 인덱스를 반환하여 원래의 범주 인덱스를 복원합니다.
# 다시 reshape(-1, 1)하여 2차원 배열 형태로 만듭니다.
Y_class_recovery = np.argmax(Y_class_onehot, axis=1).reshape(-1, 1)
print(Y_class_recovery[:10])
# 출력 예시:
# 복원된 인덱스 (처음 10개 행):
# [[1]
#  [0]
#  [0]
#  [0]
#  [1]
#  [0]
#  [0]
#  [0]
#  [0]
#  [0]]
```

### 7.4. 원핫 인코딩 과정 및 복원

1.  **`reshape(-1, 1)`**: `OneHotEncoder`는 입력으로 2차원 배열을 기대합니다. Pandas Series나 1차원 NumPy 배열을 `reshape(-1, 1)`을 사용하여 `(데이터 개수, 1)` 형태의 2차원 배열로 변환해야 합니다.
2.  **`OneHotEncoder()`**: Scikit-learn의 `OneHotEncoder` 클래스의 객체를 생성합니다.
3.  **`fit()`**: `enc.fit(Y_class)`를 호출하여 인코더를 훈련합니다. 이 단계에서 인코더는 입력 데이터(`Y_class`)에 존재하는 모든 고유한 범주(예: 'A', 'B', 'C', 'D')를 식별하고, 각 범주에 고유한 인덱스를 할당하는 내부 매핑을 생성합니다.
4.  **`transform()`**: `enc.transform(Y_class)`를 호출하여 실제 원핫 인코딩을 수행합니다. 이 메서드는 기본적으로 메모리 효율적인 희소 행렬(sparse matrix)을 반환합니다.
5.  **`toarray()`**: `Y_class_onehot.toarray()`를 호출하여 `transform()`에서 반환된 희소 행렬을 일반적인 밀집 행렬(dense array)인 NumPy 배열로 변환합니다. 대부분의 머신러닝 모델은 밀집 행렬을 입력으로 받습니다.

**원핫 인코딩 복원**: 원핫 인코딩된 배열을 원래의 범주형 인덱스로 복원하려면 `numpy.argmax()` 함수를 사용합니다. `np.argmax(Y_class_onehot, axis=1)`는 각 행에서 값이 1인(가장 큰 값) 요소의 인덱스를 반환합니다. 이 인덱스는 원래 범주에 매핑된 숫자 값과 일치합니다.

## 8. 실전 예제: Iris 데이터셋 종합 처리

Iris 데이터셋은 머신러닝 및 통계학에서 분류(Classification) 문제의 예시로 자주 사용되는 유명한 데이터셋입니다. 붓꽃의 세 가지 종(Setosa, Versicolor, Virginica)에 대한 꽃잎(petal)과 꽃받침(sepal)의 길이 및 너비 정보를 포함하고 있습니다. 이 예제를 통해 Pandas와 NumPy, Scikit-learn의 기능을 종합적으로 활용하여 실제 데이터셋을 전처리하는 방법을 익혀보겠습니다.

**`data/iris.csv` 파일 구조 (예시)**:
```csv
sepal.length,sepal.width,petal.length,petal.width,variety
5.1,3.5,1.4,0.2,Setosa
4.9,3.0,1.4,0.2,Setosa
...,,,,
6.3,3.3,6.0,2.5,Virginica
```

**문제**: `data/iris.csv` 파일을 사용하여 다음 작업을 수행하세요.

1.  누락된 데이터 확인 및 모든 숫자형 필드의 누락 데이터를 평균값으로 대체
2.  `sepal.length`, `sepal.width`, `petal.width` 필드를 Min-Max 정규화
3.  `petal.length` 필드를 3개 구간으로 나누어 'A', 'B', 'C' 등급 부여
4.  생성된 등급(`petal_grade`)을 원핫 인코딩으로 변환

**해답**:

```python
# 파일명: homework11.py
import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder # OneHotEncoder 임포트

# 1. 데이터 로드 및 누락 데이터 확인
data = pd.read_csv('./data/iris.csv')
print("=== 1. 초기 누락 데이터 확인 ===")
print(data.isnull().sum()) # 각 컬럼별 NaN 개수 출력

# 2. 누락 데이터 평균값으로 대체
print("\n=== 2. 누락 데이터 평균값으로 대체 ===")
# 숫자형 컬럼만 선택하여 평균 계산 및 대체
numeric_cols = data.select_dtypes(include=np.number).columns
for col in numeric_cols:
    if data[col].isnull().any(): # 해당 컬럼에 NaN이 하나라도 있다면
        mean_value = data[col].mean()
        data[col].fillna(mean_value, inplace=True)

print("대체 후 누락 데이터 확인:")
print(data.isnull().sum()) # 모든 NaN이 채워졌는지 확인

# 3. Min-Max 정규화 함수 정의 및 적용
# Min-Max 정규화: (x - min) / (max - min)
def min_max_normalize(series):
    min_val = series.min()
    max_val = series.max()
    return (series - min_val) / (max_val - min_val)

print("\n=== 3. 'sepal.length', 'sepal.width', 'petal.width' 필드 정규화 적용 ===")
data['sepal.length'] = min_max_normalize(data['sepal.length'])
data['sepal.width'] = min_max_normalize(data['sepal.width'])
data['petal.width'] = min_max_normalize(data['petal.width'])
print("정규화 완료. 정규화된 데이터의 head() 확인:")
print(data.head())

# 4. 'petal.length' 필드 3개 구간으로 나누기
print("\n=== 4. 'petal.length' 필드 3개 구간으로 나누기 ===")
# np.histogram을 사용하여 petal.length의 분포를 기반으로 3개 구간의 경계값 계산
count, bin_dividers = np.histogram(data['petal.length'], bins=3)
print("구간 경계값:", bin_dividers)

# 각 구간에 부여할 라벨명 정의
bin_names = ["A", "B", "C"] # A: 가장 작은 구간, C: 가장 큰 구간

# pd.cut()을 사용하여 구간 나누기 적용
data["petal_grade"] = pd.cut(x=data['petal.length'], 
                             bins=bin_dividers,
                             labels=bin_names, 
                             include_lowest=True) # 최솟값 포함

print("구간 나누기 결과 (petal.length와 petal_grade 컬럼 head()):")
print(data[['petal.length', 'petal_grade']].head())
print("\n각 등급별 데이터 개수:")
print(data['petal_grade'].value_counts().sort_index()) # 등급별 개수 확인

# 5. 등급('petal_grade')을 원핫 인코딩으로 변환
print("\n=== 5. 'petal_grade' 원핫 인코딩 ===")
# OneHotEncoder는 2차원 배열을 입력으로 받으므로 reshape(-1, 1) 필요
Y_class = np.array(data['petal_grade']).reshape(-1, 1)

# OneHotEncoder 객체 생성 및 훈련/변환
enc = OneHotEncoder()
enc.fit(Y_class) # 인코더 훈련
Y_class_onehot = enc.transform(Y_class).toarray() # 원핫 인코딩 수행 및 밀집 행렬로 변환

print("원핫 인코딩 결과 (처음 10개 행):")
print(Y_class_onehot[:10])

print("\n복원된 인덱스 (처음 10개 행):")
# np.argmax를 사용하여 원핫 인코딩된 배열을 원래의 범주 인덱스로 복원
Y_class_recovery = np.argmax(Y_class_onehot, axis=1).reshape(-1, 1)
print(Y_class_recovery[:10])
```

## 9. 핵심 요약

### 9.1. 데이터 전처리 절차 요약

1.  **누락 데이터 처리**:
    *   **확인**: `df.isnull().sum()`을 사용하여 각 컬럼별 누락 데이터 개수를 파악합니다.
    *   **선택**: 누락 건수가 적을 경우 `df.dropna()`로 해당 행/열을 삭제하고, 많을 경우 `df.fillna()`로 평균, 중간값, 최빈값 등으로 대체합니다.
2.  **중복 데이터 처리**:
    *   **확인**: `df.duplicated()`로 중복 여부를 확인합니다.
    *   **제거**: `df.drop_duplicates()`를 사용하여 중복 행을 제거합니다. `subset` 매개변수로 특정 컬럼을 기준으로 중복을 제거할 수 있습니다.

### 9.2. 데이터 변환 기법 요약

1.  **정규화 (Normalization/Scaling)**: 데이터의 스케일을 통일하여 모델 학습의 안정성과 성능을 향상시킵니다.
    *   **Min-Max 정규화**: `(X - X_min) / (X_max - X_min)` 공식을 사용하여 데이터를 0~1 범위로 변환합니다.
    *   **Z-score 정규화**: `(X - μ) / σ` 공식을 사용하여 데이터를 평균 0, 표준편차 1인 분포로 변환합니다.
    *   **Robust 정규화**: 중간값과 IQR을 사용하여 이상치에 강건하게 스케일을 조정합니다.
2.  **타입 변환**: `df.astype()` 함수를 사용하여 컬럼의 데이터 타입을 변경합니다 (예: `'int'`, `'float'`, `'category'`). `pd.to_numeric(errors='coerce')`는 숫자 변환 중 오류 발생 시 `NaN`으로 처리하여 유용합니다.
3.  **구간 나누기 (Binning)**: 연속형 데이터를 이산적인 범주형 데이터로 변환합니다.
    *   `np.histogram()`: 데이터 분포를 기반으로 구간 경계값을 계산합니다.
    *   `pd.cut()`: 계산된 경계값을 기준으로 데이터를 구간으로 나눕니다.
    *   `pd.qcut()`: 분위수를 기준으로 각 구간에 동일한 개수의 데이터가 포함되도록 나눕니다.
4.  **원핫 인코딩 (One-Hot Encoding)**: 범주형 데이터를 머신러닝 모델이 이해할 수 있는 수치형 벡터(이진 벡터)로 변환합니다.
    *   `sklearn.preprocessing.OneHotEncoder`: Scikit-learn 라이브러리에서 제공하는 주요 도구입니다.
    *   입력 데이터는 2차원 배열(`reshape(-1, 1)`)이어야 합니다.

### 9.3. 데이터 타입 분류 및 주요 주의사항

*   **데이터 타입 분류**:
    *   **연속형 데이터**: 측정 가능한 수치 데이터 (예: 키, 몸무게, 온도). 평균, 표준편차 등 통계적 연산이 중요하며, 정규화가 필요할 수 있습니다.
    *   **범주형 (불연속형) 데이터**: 제한된 수의 범주로 나눌 수 있는 데이터 (예: 성별, 혈액형, 등급). 각 범주의 빈도수가 중요하며, 머신러닝 모델에 입력하기 전에 원핫 인코딩과 같은 수치형 변환이 필수적입니다.
*   **주요 주의사항**:
    *   **문자열 데이터 처리**: 숫자형으로 인식되어야 할 문자열 데이터(예: '123')는 반드시 숫자형으로 변환해야 합니다. 범주형 문자열 데이터는 `category` 타입으로 변환하거나 원핫 인코딩을 적용해야 합니다.
    *   **원핫 인코딩 시 차원**: `OneHotEncoder`와 같은 Scikit-learn 변환기는 2차원 배열을 입력으로 받으므로, Series를 변환할 때 `reshape(-1, 1)`을 잊지 않도록 주의합니다.
    *   **정규화 적용**: 모든 데이터에 정규화가 필요한 것은 아닙니다. 사용하는 머신러닝 알고리즘의 특성(예: 거리 기반 알고리즘)에 따라 선택적으로 적용해야 합니다.
    *   **누락 데이터 처리 시 데이터 분포 고려**: 누락된 데이터를 대체할 때, 해당 컬럼의 데이터 분포(정규 분포, 편향 분포 등)와 이상치 여부를 고려하여 평균, 중간값, 최빈값 중 적절한 대푯값을 선택해야 합니다.

이러한 데이터 전처리 과정은 머신러닝과 딥러닝 모델의 성능을 크게 좌우하는 핵심 단계입니다. 데이터의 품질을 높이고 모델이 데이터를 올바르게 해석하도록 돕는 중요한 과정입니다.

---

[⏮️ 이전 문서](./0702_ML정리.md) | [다음 문서 ⏭️](./0704_ML정리.md)