<h2>머신러닝 기초: Pandas 데이터 입출력 및 활용 심화</h2>
<h4>|&nbsp;&nbsp;작성자: Alpine_Dolce&nbsp;&nbsp;|&nbsp;&nbsp;날짜: 2025-07-02&nbsp;&nbsp;|</h4>

<h2>문서 목표</h2>
이 문서는 부트캠프에서 학습한 Pandas 라이브러리의 데이터 입출력 및 활용 심화 내용을 정리한 자료입니다. DataFrame의 다양한 기능과 외부 파일(CSV, Excel 등) 처리 방법, 그리고 조건부 데이터 검색 및 통계 함수 활용법을 상세히 다룹니다. 본 문서를 통해 Pandas를 활용한 데이터 분석 및 머신러닝 전처리 역량을 강화하는 데 도움이 되기를 바랍니다.

<h2>목차</h2>

- [1. DataFrame 심화](#1-dataframe-심화)
  - [1.1. DataFrame의 주요 특징](#11-dataframe의-주요-특징)
  - [1.2. Pandas가 지원하는 파일 형식](#12-pandas가-지원하는-파일-형식)
- [2. 외부 파일 읽기/쓰기](#2-외부-파일-읽기쓰기)
  - [2.1. 외부 파일 처리의 중요성](#21-외부-파일-처리의-중요성)
- [3. 파일 경로 처리](#3-파일-경로-처리)
  - [3.1. 경로 표현 방식](#31-경로-표현-방식)
  - [3.2. 파이썬에서의 경로 처리 주의사항](#32-파이썬에서의-경로-처리-주의사항)
- [4. CSV 파일 처리](#4-csv-파일-처리)
  - [4.1. CSV 파일의 특징](#41-csv-파일의-특징)
  - [4.2. CSV 파일 읽기](#42-csv-파일-읽기)
  - [4.3. CSV 파일 읽기 예제](#43-csv-파일-읽기-예제)
    - [1. 기본 CSV 파일 읽기](#1-기본-csv-파일-읽기)
    - [2. 제목 줄이 없는 CSV 파일 처리](#2-제목-줄이-없는-csv-파일-처리)
    - [3. 제목 줄이 특정 위치에 있는 경우](#3-제목-줄이-특정-위치에-있는-경우)
  - [4.4. CSV 파일 저장](#44-csv-파일-저장)
- [5. Excel 파일 처리](#5-excel-파일-처리)
  - [5.1. Excel 파일의 장점](#51-excel-파일의-장점)
  - [5.2. Excel 파일 읽기/쓰기 예제](#52-excel-파일-읽기쓰기-예제)
- [6. DataFrame API 활용](#6-dataframe-api-활용)
  - [6.1. 기본 정보 확인 API](#61-기본-정보-확인-api)
- [7. 조건부 데이터 검색](#7-조건부-데이터-검색)
  - [7.1. 기본 조건 검색](#71-기본-조건-검색)
  - [7.2. 복합 조건 검색](#72-복합-조건-검색)
- [8. 통계 함수 활용](#8-통계-함수-활용)
  - [8.1. DataFrame의 통계 함수들](#81-dataframe의-통계-함수들)
  - [8.2. 통계 함수 활용 예제](#82-통계-함수-활용-예제)
  - [8.3. 주요 통계 개념 설명](#83-주요-통계-개념-설명)
- [9. 실전 예제: Iris 데이터셋 분석](#9-실전-예제-iris-데이터셋-분석)
- [10. 핵심 요약](#10-핵심-요약)
  - [10.1. DataFrame 요약](#101-dataframe-요약)
  - [10.2. 주요 DataFrame API](#102-주요-dataframe-api)
    - [데이터 탐색 API](#데이터-탐색-api)
    - [파일 입출력 API](#파일-입출력-api)
    - [통계 함수 API](#통계-함수-api)
  - [10.3. 주요 주의사항](#103-주요-주의사항)

---

## 1. DataFrame 심화

### 1.1. DataFrame의 주요 특징

1.  **2차원 배열 형태**: Pandas가 제공하는 2차원 배열 형태의 데이터 타입으로, 행(row)과 열(column)으로 구성된 테이블 구조를 가집니다.
2.  **다양한 데이터 타입 지원**: 하나의 타입으로만 구성되지 않고, 각 열(컬럼)이 각기 다른 데이터 타입(예: 정수, 실수, 문자열, 불리언 등)으로 구성될 수 있습니다. 이는 실제 데이터의 복잡성을 잘 반영합니다.
3.  **객체 지향적 데이터 처리**: 별도의 구조체나 클래스를 만들지 않고도 사용자 데이터를 취급하기 쉬운 객체입니다. 직관적인 메서드와 속성을 통해 데이터를 쉽게 조작할 수 있습니다.
4.  **풍부한 통계 함수 제공**: 데이터의 요약 통계, 분포 분석 등을 위한 다양한 통계 관련 함수들을 내장하고 있어 데이터 탐색 및 분석에 매우 유용합니다.
5.  **자유로운 접근**: 각 열과 행에 대해 인덱스(위치 기반) 또는 레이블(이름 기반)을 사용하여 자유롭게 접근하고 데이터를 추출하거나 수정할 수 있습니다.

### 1.2. Pandas가 지원하는 파일 형식

Pandas는 다양한 외부 파일 형식을 `DataFrame`으로 읽어오거나 `DataFrame`을 외부 파일로 저장하는 기능을 강력하게 지원합니다. 이는 데이터 분석 워크플로우에서 필수적인 요소입니다.

| 파일 형식 | 설명 | Pandas 함수 (읽기) | Pandas 함수 (쓰기) |
| :-------- | :---------------------------------------------------------------- | :----------------- | :----------------- |
| **CSV**   | 쉼표(Comma)로 구분된 값 (Comma-Separated Values)을 가진 텍스트 파일. 가장 널리 사용되는 데이터 교환 형식. | `pd.read_csv()`    | `df.to_csv()`      |
| **Excel** | Microsoft Excel 스프레드시트 파일 (.xlsx, .xls). 여러 시트 지원. | `pd.read_excel()`  | `df.to_excel()`    |
| **JSON**  | JavaScript Object Notation. 웹 기반 데이터 교환에 주로 사용.   | `pd.read_json()`   | `df.to_json()`     |
| **HTML**  | 웹 페이지의 테이블 데이터를 읽어올 때 사용.                      | `pd.read_html()`   | `df.to_html()`     |
| **SQL**   | 관계형 데이터베이스에서 데이터를 직접 읽어오거나 저장.           | `pd.read_sql()`    | `df.to_sql()`      |
| **XML**   | eXtensible Markup Language. 구조화된 데이터 표현.               | `pd.read_xml()`    | `df.to_xml()`      |
| **Parquet** | 컬럼 기반의 효율적인 이진 파일 형식. 빅데이터 환경에서 성능 우수. | `pd.read_parquet()`| `df.to_parquet()`  |
| **Pickle** | 파이썬 객체를 직렬화하여 저장하는 형식.                           | `pd.read_pickle()` | `df.to_pickle()`   |
| **기타**  | TSV (Tab-Separated Values), 고정폭 파일 등 다양한 형식 지원.   | `pd.read_fwf()`, `pd.read_table()` | - |

## 2. 외부 파일 읽기/쓰기

### 2.1. 외부 파일 처리의 중요성

1.  **데이터 활용**: 실제 데이터 분석 프로젝트에서는 대부분 외부에서 제공되는 데이터를 활용하게 됩니다. 이는 데이터베이스, 웹 스크래핑, API 연동, 또는 파일 형태로 제공될 수 있습니다.
2.  **효율적인 데이터 처리**: Pandas는 다양한 소스에서 수집된 데이터를 일관된 `DataFrame` 형태로 효율적으로 불러오고 처리할 수 있는 기능을 제공하여 데이터 전처리 과정을 간소화합니다.
3.  **분석 결과 공유**: 데이터 분석을 통해 얻은 인사이트나 가공된 데이터를 다른 시스템이나 사용자에게 전달하기 위해, 분석 결과를 다시 외부 파일(CSV, Excel 등)로 저장하는 기능이 필수적입니다.

## 3. 파일 경로 처리

데이터 파일을 읽거나 쓸 때 파일의 위치를 정확하게 지정하는 것은 매우 중요합니다. 파일 경로는 크게 절대 경로와 상대 경로로 나뉩니다.

### 3.1. 경로 표현 방식

1.  **절대 경로 (Absolute Path)**:
    *   파일 시스템의 루트(최상위) 디렉토리부터 파일의 전체 위치를 기술하는 방식입니다.
    *   운영체제에 따라 시작점이 다릅니다. (예: Windows는 `C:\`, Linux/macOS는 `/`)
    *   **예시**: `C:/pandas_workspace/uni_10/data/score.csv` 또는 `/home/user/data/score.csv`
    *   **장점**: 파일의 위치가 명확하여 혼동의 여지가 적습니다.
    *   **단점**: 파일이나 프로젝트의 위치가 변경되면 경로를 수정해야 하는 번거로움이 있습니다.

2.  **상대 경로 (Relative Path)**:
    *   현재 애플리케이션(스크립트)이 실행 중인 폴더를 기준으로 파일의 위치를 기술하는 방식입니다.
    *   `.` (도트): 현재 디렉토리를 의미합니다.
    *   `..` (도트 두 개): 현재 디렉토리의 상위(부모) 디렉토리를 의미합니다.
    *   **예시**: `./data/score.csv` (현재 폴더 아래 `data` 폴더의 `score.csv`)
    *   **장점**: 프로젝트의 위치가 변경되어도 경로를 수정할 필요가 없어 프로젝트 이식성이 높습니다.
    *   **단점**: 현재 작업 디렉토리를 정확히 알아야 합니다.

### 3.2. 파이썬에서의 경로 처리 주의사항

Windows 운영체제에서는 파일 경로를 나타낼 때 역슬래시(`\`)를 사용하지만, 파이썬 문자열에서 역슬래시는 이스케이프 문자(`\n`, `\t` 등)로 해석될 수 있어 문제가 발생할 수 있습니다. 이를 해결하는 방법은 다음과 같습니다.

1.  **이스케이프 문자 사용**: 역슬래시를 두 번 사용하여 이스케이프 처리합니다.
    *   **예시**: `"c:\\pandas_workspace\\data\\score.csv"`
2.  **원시 문자열 (Raw String) 사용**: 문자열 앞에 `r`을 붙여 해당 문자열을 있는 그대로 해석하도록 합니다.
    *   **예시**: `r"c:\pandas_workspace\data\score.csv"`
3.  **슬래시(`/`) 사용 (권장)**: Windows에서도 슬래시(`/`)를 경로 구분자로 사용할 수 있으며, 이는 운영체제에 독립적이므로 가장 권장되는 방법입니다.
    *   **예시**: `"c:/pandas_workspace/data/score.csv"`

**경로 선택 권장사항**: 일반적으로 **절대 경로보다는 상대 경로 사용을 권장**합니다. 이는 폴더 이동 시 경로 수정 필요가 없어 프로젝트의 이식성을 크게 향상시키기 때문입니다.

## 4. CSV 파일 처리

CSV (Comma-Separated Values) 파일은 데이터를 쉼표(`,`)로 구분하여 저장하는 텍스트 파일 형식입니다. 가장 보편적으로 사용되는 데이터 교환 형식 중 하나입니다.

### 4.1. CSV 파일의 특징

1.  **텍스트 기반**: 데이터를 쉼표(`,`)로 구분하는 일반 텍스트 파일입니다. 특정 프로그램 없이 메모장과 같은 일반 텍스트 에디터로도 내용을 확인하고 작성할 수 있습니다.
2.  **간편한 편집**: Excel과 같은 스프레드시트 프로그램에서도 쉽게 열고 편집할 수 있습니다.
3.  **널리 사용**: 빅데이터 환경에서 데이터를 저장하고 교환하는 데 가장 많이 사용되는 형태 중 하나입니다. 다양한 시스템 간의 데이터 연동에 용이합니다.

### 4.2. CSV 파일 읽기

Pandas의 `read_csv()` 함수를 사용하여 CSV 파일을 `DataFrame`으로 불러올 수 있습니다. 이 함수는 다양한 옵션을 제공하여 복잡한 CSV 파일도 유연하게 처리할 수 있습니다.

**기본 읽기 구문**:
```python
import pandas as pd

# 현재 스크립트가 있는 디렉토리의 data 폴더 안에 score.csv 파일이 있다고 가정
data = pd.read_csv("./data/score.csv")
```

**주요 `read_csv()` 옵션**:

*   `header`: 제목 줄(컬럼명)의 위치를 지정합니다. 기본값은 `0` (첫 번째 줄)입니다.
    *   `header=None`: 파일에 제목 줄이 없음을 나타냅니다. 이 경우 Pandas가 0부터 시작하는 정수 인덱스를 컬럼명으로 자동 부여합니다.
    *   `header=N`: N+1번째 줄을 제목 줄로 사용합니다 (0부터 시작하는 인덱스).
*   `encoding`: 파일의 문자 인코딩 방식을 지정합니다. (예: `'utf-8'`, `'cp949'`, `'euc-kr'`)
*   `sep` 또는 `delimiter`: 데이터를 구분하는 구분자(separator)를 지정합니다. 기본값은 쉼표(`,`)입니다. 탭으로 구분된 파일(`TSV`)의 경우 `sep='\t'`로 지정할 수 있습니다.
*   `index_col`: 특정 컬럼을 DataFrame의 인덱스로 사용할 때 지정합니다.
*   `names`: `header=None`일 때 사용할 컬럼명 리스트를 직접 지정합니다.

### 4.3. CSV 파일 읽기 예제

#### 1. 기본 CSV 파일 읽기

`score.csv` 파일이 다음과 같다고 가정합니다:
```csv
name,kor,eng,mat
홍길동,90,99,90
임꺽정,80,98,70
장길산,70,97,70
홍경래,70,46,60
```

```python
import pandas as pd

data = pd.read_csv("./data/score.csv")
print("--- 기본 CSV 파일 읽기 결과 ---")
print("컬럼명:", data.columns) # DataFrame의 컬럼명 출력
print("인덱스:", data.index)   # DataFrame의 인덱스 정보 출력

# 총점, 평균 구하기: 기존 컬럼을 활용하여 새로운 파생 컬럼 생성
data["total"] = data["kor"] + data["eng"] + data["mat"]
data["avg"] = data["total"] / 3
print("\n--- 총점 및 평균 추가 후 DataFrame ---")
print(data)
# 출력 예시:
# --- 기본 CSV 파일 읽기 결과 ---
# 컬럼명: Index(["name", "kor", "eng", "mat"], dtype="object")
# 인덱스: RangeIndex(start=0, stop=4, step=1)
#
# --- 총점 및 평균 추가 후 DataFrame ---
#   name  kor  eng  mat  total        avg
# 0  홍길동   90   99   90    279  93.000000
# 1  임꺽정   80   98   70    248  82.666667
# 2  장길산   70   97   70    237  79.000000
# 3  홍경래   70   46   60    176  58.666667
```

#### 2. 제목 줄이 없는 CSV 파일 처리

`score_noheader.csv` 파일이 다음과 같다고 가정합니다:
```csv
홍길동,90,99,90
임꺽정,80,98,70
장길산,70,97,70
홍경래,70,46,60
```

```python
import pandas as pd

# 제목 줄이 없을 경우 header=None 옵션 사용
data = pd.read_csv("./data/score_noheader.csv", header=None)
print("--- 제목 줄 없이 읽은 CSV 파일 (자동 컬럼명) ---")
print("컬럼명:", data.columns) # Pandas가 0부터 시작하는 정수 컬럼명을 자동 부여

# 직접 컬럼명 부여: read_csv 후 data.columns 속성을 통해 컬럼명 변경
data.columns = ["name", "kor", "eng", "mat"]
print("\n--- 컬럼명 부여 후 DataFrame ---")
print("컬럼 부여 후:", data.columns)
print(data)

# 총점, 평균 구하기
data["total"] = data["kor"] + data["eng"] + data["mat"]
data["avg"] = data["total"] / 3
print("\n--- 총점 및 평균 추가 후 DataFrame ---")
print(data)
# 출력 예시:
# --- 제목 줄 없이 읽은 CSV 파일 (자동 컬럼명) ---
# 컬럼명: Int64Index([0, 1, 2, 3], dtype="int64")
#
# --- 컬럼명 부여 후 DataFrame ---
# 컬럼 부여 후: Index(["name", "kor", "eng", "mat"], dtype="object")
#   name  kor  eng  mat
# 0  홍길동   90   99   90
# 1  임꺽정   80   98   70
# 2  장길산   70   97   70
# 3  홍경래   70   46   60
```

#### 3. 제목 줄이 특정 위치에 있는 경우

`score_header.csv` 파일이 다음과 같다고 가정합니다: 
```csv
# 이 파일은 학생들의 성적 데이터입니다.
# 데이터 출처: 2025년 1학기
# 컬럼 설명: name(이름), kor(국어), eng(영어), mat(수학)
name,kor,eng,mat
홍길동,90,99,90
임꺽정,80,98,70
```

```python
import pandas as pd

# header가 4번째 줄에 있음 (0부터 시작하는 인덱스로 3)
data = pd.read_csv("./data/score_header.csv", header=3)
print("--- 특정 위치에 제목 줄이 있는 CSV 파일 읽기 결과 ---")
print("컬럼명:", data.columns)
print("인덱스:", data.index)

# 총점, 평균 구하기
data["total"] = data["kor"] + data["eng"] + data["mat"]
data["avg"] = data["total"] / 3
print("\n--- 총점 및 평균 추가 후 DataFrame ---")
print(data)
# 출력 예시:
# --- 특정 위치에 제목 줄이 있는 CSV 파일 읽기 결과 ---
# 컬럼명: Index(["name", "kor", "eng", "mat"], dtype="object")
# 인덱스: RangeIndex(start=0, stop=2, step=1)
#
# --- 총점 및 평균 추가 후 DataFrame ---
#   name  kor  eng  mat  total        avg
# 0  홍길동   90   99   90    279  93.000000
# 1  임꺽정   80   98   70    248  82.666667
```

### 4.4. CSV 파일 저장

`DataFrame` 객체를 CSV 파일로 저장할 때는 `to_csv()` 메서드를 사용합니다. 저장 시 다양한 옵션을 통해 파일 형식을 제어할 수 있습니다.

**기본 저장 구문**:
```python
# DataFrame을 CSV 파일로 저장
data.to_csv("output_file.csv")
```

**주요 `to_csv()` 옵션**:

*   `path_or_buf`: 저장할 파일 경로 및 이름.
*   `sep`: 구분자. 기본값은 쉼표(`,`).
*   `na_rep`: `NaN` (결측치) 값을 대체할 문자열. 기본값은 빈 문자열.
*   `float_format`: 부동 소수점 숫자의 출력 형식 지정.
*   `columns`: 저장할 컬럼의 리스트. 지정하지 않으면 모든 컬럼 저장.
*   `header`: 컬럼명(헤더)을 파일에 쓸지 여부. `True` (기본값) 또는 `False`.
*   `index`: DataFrame의 인덱스를 파일에 쓸지 여부. `True` (기본값) 또는 `False`.
*   `mode`: 파일 쓰기 모드. `'w'` (쓰기, 기본값), `'a'` (추가).
*   `encoding`: 파일의 문자 인코딩 방식. (예: `'utf-8'`, `'cp949'`).
**Excel에서 CSV 파일을 열 때 한글 깨짐 현상이 발생한다면 `encoding="cp949"`를 시도해 볼 수 있습니다.**

**예시**: `score_result.csv` 파일로 저장 (인덱스 제외, cp949 인코딩)
```python
# CSV 파일로 저장
# Excel에서 열어보려면 cp949 인코딩 필요
# index=False로 인덱스 저장 안 함
data.to_csv("score_result.csv", mode='w', encoding="cp949", index=False)
```

## 5. Excel 파일 처리

Pandas는 Microsoft Excel 파일(.xlsx, .xls)을 직접 읽고 쓰는 기능을 제공합니다. 이는 Excel을 주로 사용하는 환경에서 데이터 분석 결과를 공유하거나 데이터를 불러올 때 매우 편리합니다.

### 5.1. Excel 파일의 장점

1.  **별도 라이브러리 불필요**: `openpyxl` (xlsx), `xlrd` (xls)와 같은 백엔드 엔진이 필요하지만, Pandas 설치 시 대부분 함께 설치되므로 사용자가 별도로 COM 라이브러리나 다른 복잡한 라이브러리를 설치할 필요가 없습니다.
2.  **Pandas 직접 지원**: Pandas 내부에 `read_excel()` 및 `to_excel()` 함수가 내장되어 있어 파이썬 코드 내에서 Excel 파일을 쉽게 다룰 수 있습니다.
3.  **복잡한 데이터 구조 처리**: 여러 시트(sheet)를 가진 Excel 파일이나 특정 범위의 데이터도 유연하게 처리할 수 있습니다.

### 5.2. Excel 파일 읽기/쓰기 예제

`score.xlsx` 파일이 다음과 같다고 가정합니다:

| name | kor | eng | mat |
| :--- | :-- | :-- | :-- |
| 홍길동 | 90  | 99  | 90  |
| 임꺽정 | 80  | 98  | 70  |

```python
import pandas as pd

# Excel 파일 읽기: score.xlsx 파일을 DataFrame으로 불러오기
data = pd.read_excel("./data/score.xlsx")

# 총점 및 평균 컬럼 추가
data["total"] = data["kor"] + data["eng"] + data["mat"]
data["avg"] = data["total"] / 3
print("--- Excel 파일 읽기 및 계산 결과 ---")
print(data)

# Excel 파일로 저장
# score_result1.xlsx: DataFrame의 인덱스도 함께 저장 (기본값)
data.to_excel("score_result1.xlsx")
print("\nscore_result1.xlsx 파일이 생성되었습니다 (인덱스 포함).")

# score_result2.xlsx: DataFrame의 인덱스 제외하고 저장
data.to_excel("score_result2.xlsx", index=False)
print("score_result2.xlsx 파일이 생성되었습니다 (인덱스 제외).")

# 출력 예시:
# --- Excel 파일 읽기 및 계산 결과 ---
#   name  kor  eng  mat  total        avg
# 0  홍길동   90   99   90    279  93.000000
# 1  임꺽정   80   98   70    248  82.666667
#
# score_result1.xlsx 파일이 생성되었습니다 (인덱스 포함).
# score_result2.xlsx 파일이 생성되었습니다 (인덱스 제외).
```

**`to_excel()` 저장 시 주요 옵션**:

*   `excel_writer`: 저장할 파일 경로 및 이름.
*   `sheet_name`: 저장할 시트의 이름. 기본값은 `'Sheet1'`.
*   `na_rep`: `NaN` (결측치) 값을 대체할 문자열.
*   `header`: 컬럼명(헤더)을 파일에 쓸지 여부. `True` (기본값) 또는 `False`.
*   `index`: DataFrame의 인덱스를 파일에 쓸지 여부. `True` (기본값) 또는 `False`.

## 6. DataFrame API 활용

Pandas DataFrame은 데이터의 구조를 파악하고 기본적인 통계 정보를 얻는 데 유용한 다양한 API(메서드)를 제공합니다. 이는 데이터 탐색(EDA)의 첫 단계에서 매우 중요합니다.

### 6.1. 기본 정보 확인 API

1.  **`head()` / `tail()`**: DataFrame의 상위 또는 하위 n개의 행을 출력하여 데이터의 전체적인 모습을 빠르게 파악할 수 있습니다. 기본값은 5개 행입니다.

    ```python
    import pandas as pd

    # auto-mpg.csv 파일 로드 (예시 데이터셋)
    data = pd.read_csv("./data/auto-mpg.csv")

    print("--- 앞에서부터 5개 미리 보기 (data.head()) ---")
    print(data.head())

    print("\n--- 뒤에서부터 5개 미리 보기 (data.tail()) ---")
    print(data.tail())

    print("\n--- 앞에서부터 10개 미리 보기 (data.head(10)) ---")
    print(data.head(10))
    ```

2.  **`shape`**: DataFrame의 차원(dimensions)을 튜플 형태로 반환합니다. `(행의 개수, 열의 개수)`로 구성됩니다.

    ```python
    # data DataFrame이 로드되어 있다고 가정
    print("--- DataFrame의 차원 (shape) ---")
    print(data.shape)  # 예: (398, 9) -> 398행, 9열

    # 행과 열의 개수를 개별 변수에 할당
    row, col = data.shape
    print(f"행의 개수: {row}")
    print(f"열의 개수: {col}")
    ```

3.  **`info()`**: DataFrame의 간략한 정보를 출력합니다. 각 컬럼의 데이터 타입, Non-null 값의 개수, 메모리 사용량, 인덱스 정보 등을 포함하여 데이터의 누락 여부와 타입을 빠르게 확인할 수 있습니다.

    ```python
    import pandas as pd

    data = pd.read_csv("./data/auto-mpg.csv")
    print("--- 데이터의 기본 구조 (data.info()) ---")
    data.info()
    ```

    **`info()` 함수 제공 정보 요약**:
    *   **데이터 타입 (Dtype)**: 각 컬럼이 어떤 데이터 타입(int64, float64, object 등)을 가지는지 보여줍니다.
    *   **Non-Null Count**: 각 컬럼에 결측치(NaN)가 아닌 유효한 데이터가 몇 개 있는지 보여줍니다. 이를 통해 결측치 여부를 쉽게 파악할 수 있습니다.
    *   **메모리 사용량 (Memory Usage)**: DataFrame이 사용하는 총 메모리 양을 나타냅니다.
    *   **인덱스 정보 (RangeIndex)**: DataFrame의 행 인덱스 범위와 스텝을 보여줍니다.

4.  **`describe()`**: 숫자형 컬럼에 대한 기술 통계(Descriptive Statistics)를 계산하여 출력합니다. 데이터의 분포와 중심 경향성, 퍼짐 정도 등을 파악하는 데 유용합니다.

    ```python
    # data DataFrame이 로드되어 있다고 가정
    print("--- 데이터의 요약 통계 정보 (data.describe()) ---")
    print(data.describe())
    ```

    **`describe()` 함수 제공 정보 요약**:
    *   `count`: 해당 컬럼의 유효한(Non-null) 데이터 개수.
    *   `mean`: 평균값.
    *   `std`: 표준편차 (Standard Deviation).
    *   `min`: 최솟값.
    *   `25%` (1사분위수): 데이터를 오름차순으로 정렬했을 때 하위 25% 지점의 값.
    *   `50%` (중앙값/2사분위수): 데이터를 오름차순으로 정렬했을 때 중간 지점의 값. 중앙값은 극단적인 값(이상치)에 덜 민감하여 데이터의 중심을 나타내는 데 평균보다 더 견고할 수 있습니다.
    *   `75%` (3사분위수): 데이터를 오름차순으로 정렬했을 때 상위 25% 지점의 값.
    *   `max`: 최댓값.

## 7. 조건부 데이터 검색

DataFrame에서 특정 조건을 만족하는 데이터를 선택하는 것은 데이터 분석의 핵심 기능 중 하나입니다. Pandas는 불리언 인덱싱(Boolean Indexing)을 통해 강력한 조건부 검색 기능을 제공합니다.

### 7.1. 기본 조건 검색

단일 조건을 사용하여 DataFrame의 행을 필터링할 수 있습니다. 조건식은 각 행에 대해 `True` 또는 `False`를 반환하는 불리언 Series를 생성하며, 이 Series를 사용하여 `True`인 행만 선택합니다.

```python
import pandas as pd

data = pd.read_csv("./data/auto-mpg.csv")

print("--- 실린더 개수가 4개인 데이터 (단일 조건) ---")
# data.cylinders == 4는 각 행의 cylinders 값이 4인지 여부를 True/False로 반환하는 Series를 생성
print(data[data.cylinders == 4])

print("\n--- 연비(mpg)가 27 이상인 데이터 (단일 조건) ---")
print(data[data.mpg >= 27])

print("\n--- 모델 연도(model-year)가 70년인 데이터 (단일 조건) ---")
print(data[data["model-year"] == 70])
```

### 7.2. 복합 조건 검색

여러 조건을 동시에 만족하거나(AND) 둘 중 하나라도 만족하는(OR) 데이터를 검색할 때는 복합 조건식을 사용합니다. 이때 파이썬의 기본 논리 연산자(`and`, `or`) 대신 비트wise 논리 연산자(`&`, `|`)를 사용해야 하며, 각 조건식은 반드시 괄호로 묶어야 합니다.

**주의사항**:
*   파이썬의 `and`, `or` 연산자는 Series 전체에 대해 불리언 값을 평가할 수 없으므로 사용 불가합니다.
*   NumPy 기반의 Pandas에서는 요소별(element-wise) 논리 연산을 위해 비트wise 연산자(`&` for AND, `|` for OR, `~` for NOT)를 사용해야 합니다.
*   각 개별 조건식은 반드시 괄호 `()`로 묶어야 합니다. 이는 연산자 우선순위 문제로 인해 발생할 수 있는 오류를 방지합니다.

**NumPy 논리 함수 사용 (대안)**:
`numpy.logical_and()`, `numpy.logical_or()` 함수를 사용하여 복합 조건을 구성할 수도 있습니다. 이는 가독성을 높이는 데 도움이 될 수 있습니다.

```python
import numpy as np
import pandas as pd

data = pd.read_csv("./data/auto-mpg.csv")

print("--- 모델 연도가 70년이고 연비가 25 이상인 데이터 (AND 조건) ---")
# (data["model-year"] == 70) & (data["mpg"] >= 25) 와 동일
print(data[np.logical_and(data["model-year"] == 70, data["mpg"] >= 25)])

print("\n--- 모델 연도가 70년이거나 연비가 30 이상인 데이터 (OR 조건) ---")
# (data["model-year"] == 70) | (data["mpg"] >= 30) 와 동일
print(data[np.logical_or(data["model-year"] == 70, data["mpg"] >= 30)])
```

**오류 예시**: 파이썬의 `and`, `or` 연산자를 사용했을 때 발생하는 `ValueError`
```python
# 이렇게 사용하면 에러 발생 (ValueError: The truth value of a Series is ambiguous.)
# data["model-year"]==70 and data["mpg"]>=25]
```
이 오류는 Pandas Series가 단일 True/False 값으로 명확하게 평가될 수 없기 때문에 발생합니다. 각 요소를 개별적으로 평가하려면 비트wise 연산자를 사용해야 합니다.

## 8. 통계 함수 활용

Pandas DataFrame과 Series는 데이터의 특성을 이해하고 요약하는 데 필수적인 다양한 통계 함수를 제공합니다. 이 함수들은 데이터의 중심 경향성, 분산, 분포 등을 파악하는 데 사용됩니다.

### 8.1. DataFrame의 통계 함수들

DataFrame의 각 열은 Series 타입이므로, Series에 적용 가능한 모든 통계 함수를 개별 컬럼에 직접 적용할 수 있습니다. 또한, DataFrame 전체에 적용하여 각 컬럼별 통계량을 얻을 수도 있습니다.

**기본 통계 함수 목록**:

*   `count()`: 유효한(Non-null) 값의 개수
*   `sum()`: 값들의 합계
*   `mean()`: 산술 평균
*   `median()`: 중앙값 (데이터를 정렬했을 때 중간에 위치한 값)
*   `std()`: 표준편차 (Standard Deviation)
*   `var()`: 분산 (Variance)
*   `min()`: 최솟값
*   `max()`: 최댓값
*   `quantile(q)`: 사분위수 (q는 0과 1 사이의 값, 예: `0.25`는 1사분위수)
*   `mode()`: 최빈값 (가장 자주 나타나는 값)
*   `value_counts()`: Series에서 고유한 값들의 빈도수를 Series 형태로 반환
*   `corr()`: DataFrame의 컬럼 간 상관계수 행렬 계산
*   `cov()`: DataFrame의 컬럼 간 공분산 행렬 계산

### 8.2. 통계 함수 활용 예제

`auto-mpg.csv` 데이터셋을 사용하여 다양한 통계 함수를 적용하는 예제입니다.

```python
import pandas as pd

data = pd.read_csv("./data/auto-mpg.csv")

print("--- 모델 연도별 고유 개수 (value_counts()) ---")
# "model-year" 컬럼의 각 연도별 차량 개수 (빈도수)를 계산
print(data["model-year"].value_counts())

print("\n--- \"mpg\" (연비) 컬럼의 기본 통계량 ---")
print(f"연비 평균: {data["mpg"].mean():.2f}") # 소수점 둘째 자리까지 포맷팅
print(f"연비 최댓값: {data["mpg"].max():.2f}")
print(f"연비 최솟값: {data["mpg"].min():.2f}")
print(f"연비 중간값: {data["mpg"].median():.2f}")
print(f"연비 분산: {data["mpg"].var():.2f}")
print(f"연비 표준편차: {data["mpg"].std():.2f}")

print("\n--- \"mpg\" (연비) 컬럼의 사분위수 (quantile()) ---")
print(f"1사분위수 (Q1): {data["mpg"].quantile(0.25):.2f}")
print(f"2사분위수 (Q2, 중앙값): {data["mpg"].quantile(0.5):.2f}")
print(f"3사분위수 (Q3): {data["mpg"].quantile(0.75):.2f}")
```

### 8.3. 주요 통계 개념 설명

데이터 분석에서 자주 사용되는 통계 개념들을 이해하는 것은 Pandas 함수를 효과적으로 활용하는 데 도움이 됩니다.

1.  **표준편차 (Standard Deviation)**:
    *   데이터가 평균으로부터 얼마나 떨어져 분포하는지를 나타내는 척도입니다.
    *   값이 클수록 데이터가 평균에서 넓게 흩어져 분포하고, 값이 작을수록 데이터가 평균 주변에 밀집하여 모여 있음을 의미합니다.
    *   데이터의 변동성(variability)을 측정하는 데 사용됩니다.

2.  **분산 (Variance)**:
    *   데이터가 평균으로부터 얼마나 흩어져 있는지를 나타내는 척도로, 표준편차의 제곱 값입니다.
    *   표준편차와 유사하게 데이터의 퍼짐 정도를 나타내지만, 단위가 제곱이므로 해석이 직관적이지 않을 수 있습니다.

3.  **사분위수 (Quantile)**:
    *   데이터를 크기 순으로 정렬했을 때, 전체 데이터를 4등분하는 위치에 있는 값들을 의미합니다.
    *   **1사분위수 (Q1, 25%)**: 전체 데이터의 하위 25% 지점에 해당하는 값.
    *   **2사분위수 (Q2, 50%, 중앙값)**: 전체 데이터의 중간 지점에 해당하는 값. 중앙값은 극단적인 값(이상치)에 덜 민감하여 데이터의 중심을 나타내는 데 평균보다 더 견고할 수 있습니다.
    *   **3사분위수 (Q3, 75%)**: 전체 데이터의 상위 25% 지점에 해당하는 값.
    *   사분위수를 통해 데이터의 분포 형태와 이상치 여부를 파악하는 데 도움을 받을 수 있습니다. (예: IQR = Q3 - Q1)

4.  **중간값 (Median)**:
    *   데이터를 크기 순으로 정렬했을 때 정확히 중간에 위치한 값입니다.
    *   평균값과 달리 극단적인 값(이상치)의 영향을 덜 받으므로, 데이터 분포가 비대칭이거나 이상치가 존재할 때 데이터의 중심을 나타내는 데 더 적합할 수 있습니다.

## 9. 실전 예제: Iris 데이터셋 분석

Iris 데이터셋은 머신러닝 및 통계학에서 분류(Classification) 문제의 예시로 자주 사용되는 유명한 데이터셋입니다. 붓꽃의 세 가지 종(Setosa, Versicolor, Virginica)에 대한 꽃잎(petal)과 꽃받침(sepal)의 길이 및 너비 정보를 포함하고 있습니다. 이 예제를 통해 Pandas의 다양한 기능을 활용하여 실제 데이터셋을 탐색하고 분석하는 방법을 익혀보겠습니다.

**`data/iris.csv` 파일 구조 (예시)**:
```csv
sepal.length,sepal.width,petal.length,petal.width,variety
5.1,3.5,1.4,0.2,Setosa
4.9,3.0,1.4,0.2,Setosa
...
6.3,3.3,6.0,2.5,Virginica
```

**문제**: `data/iris.csv` 파일을 읽어서 다음 작업을 수행하세요.

1.  Iris 데이터셋의 필드(컬럼) 개수와 각 필드의 타입 확인
2.  맨 앞의 데이터 7개 출력
3.  Iris 데이터셋의 통계량 요약 정보 확인
4.  `variety`가 'Setosa'인 데이터의 통계량 출력
5.  각 `variety`별 `sepal.length` 값의 평균값 출력
6.  꽃의 종류가 'Setosa'이면서 `sepal.length`가 5cm 이상인 데이터 개수 출력

**해답**:

```python
import pandas as pd
import numpy as np

# 데이터 로드: data/iris.csv 파일을 DataFrame으로 불러오기
data = pd.read_csv("./data/iris.csv")

# 1) 필드 정보 확인: data.info()를 사용하여 컬럼 정보, Non-null 개수, 데이터 타입 확인
print("=== 1. 데이터셋 필드 정보 (data.info()) ===")
data.info()

# 2) 앞의 7개 데이터 출력: data.head(7)을 사용하여 상위 7개 행 출력
print("\n=== 2. 앞의 7개 데이터 (data.head(7)) ===")
print(data.head(7))

# 3) 통계량 요약 정보: data.describe()를 사용하여 숫자형 컬럼의 기술 통계 확인
print("\n=== 3. 통계량 요약 (data.describe()) ===")
print(data.describe())

# 4) variety가 'Setosa'인 데이터의 통계량 출력: 조건부 필터링 후 describe() 적용
print("\n=== 4. 'Setosa' 데이터 통계량 ===")
setosa_data = data[data["variety"] == 'Setosa']
print(setosa_data.describe())

# 5) 각 variety별 sepal.length 평균: groupby()와 mean()을 활용하여 그룹별 평균 계산
print("\n=== 5. 각 variety별 sepal.length 평균 ===")
# 방법 1: 각 종별로 필터링하여 평균 계산
setosa_avg = data[data["variety"] == 'Setosa']["sepal.length"].mean()
print(f"Setosa 평균 sepal.length: {setosa_avg:.2f}")

versicolor_avg = data[data["variety"] == 'Versicolor']["sepal.length"].mean()
print(f"Versicolor 평균 sepal.length: {versicolor_avg:.2f}")

virginica_avg = data[data["variety"] == 'Virginica']["sepal.length"].mean()
print(f"Virginica 평균 sepal.length: {virginica_avg:.2f}")

# 방법 2 (권장): groupby()를 사용하여 더 효율적으로 계산
print("\n--- groupby()를 이용한 각 variety별 sepal.length 평균 ---")
print(data.groupby('variety')["sepal.length"].mean())

# 6) Setosa이면서 sepal.length >= 5인 데이터 개수: 복합 조건 필터링 및 len() 사용
print("\n=== 6. 'Setosa'이면서 sepal.length >= 5인 데이터 ===")
condition_data = data[np.logical_and(data["variety"] == 'Setosa', 
                                   data["sepal.length"] >= 5)]
# 또는 비트wise 연산자 사용: data[(data["variety"] == 'Setosa') & (data["sepal.length"] >= 5)]
print(f"조건을 만족하는 데이터 개수: {len(condition_data)}개")
print(condition_data)
```

## 10. 핵심 요약

### 10.1. DataFrame 요약

*   **가장 많이 사용되는 데이터 구조**: 정형 데이터 분석의 대부분은 DataFrame을 중심으로 이루어집니다.
*   **다양한 파일 형식 지원**: CSV, Excel, JSON 등 다양한 외부 파일 형식을 DataFrame 객체로 직접 변환하여 읽고 쓸 수 있습니다.
*   **강력한 데이터 조작**: `for`문을 사용하지 않고도 조건식(불리언 인덱싱)으로 데이터에 접근하고 필터링할 수 있습니다.
*   **복합 조건 처리**: 여러 조건을 결합하여 데이터를 검색할 때는 파이썬의 논리 연산자(`and`, `or`) 대신 NumPy의 논리 함수(`np.logical_and`, `np.logical_or`) 또는 비트wise 연산자(`&`, `|`)를 사용해야 합니다.

### 10.2. 주요 DataFrame API

#### 데이터 탐색 API

*   `head(n)`: DataFrame의 앞에서부터 `n`개(기본값 5개)의 데이터를 출력합니다.
*   `tail(n)`: DataFrame의 뒤에서부터 `n`개(기본값 5개)의 데이터를 출력합니다.
*   `info()`: DataFrame의 각 필드(컬럼)의 종류, Non-null 데이터 개수, 데이터 타입, 메모리 사용량 등의 정보를 출력합니다.
*   `describe()`: 숫자형 컬럼에 대한 기본 통계값(최댓값, 최솟값, 평균, 사분위수, 표준편차 등)을 출력합니다.
*   `shape`: DataFrame의 차원(행, 열)을 튜플 형태로 반환합니다.

#### 파일 입출력 API

*   `pd.read_csv(filepath, ...)`: CSV 파일을 읽어 DataFrame으로 변환합니다.
*   `pd.read_excel(filepath, ...)`: Excel 파일을 읽어 DataFrame으로 변환합니다.
*   `df.to_csv(filepath, ...)`: DataFrame을 CSV 파일로 저장합니다.
*   `df.to_excel(filepath, ...)`: DataFrame을 Excel 파일로 저장합니다.

#### 통계 함수 API

*   `mean()`: 평균값 계산.
*   `max()`: 최댓값 계산.
*   `min()`: 최솟값 계산.
*   `median()`: 중앙값 계산.
*   `std()`: 표준편차 계산.
*   `var()`: 분산 계산.
*   `quantile(q)`: 사분위수 계산.
*   `mode()`: 최빈값 (가장 자주 나타나는 값).
*   `value_counts()`: Series에서 고유한 값들의 빈도수를 계산합니다.
*   `corr()`: DataFrame의 컬럼 간 상관계수 행렬 계산.
*   `cov()`: DataFrame의 컬럼 간 공분산 행렬 계산.

### 10.3. 주요 주의사항

*   **파일 경로 설정**: Windows 환경에서 파일 경로 설정 시 역슬래시(`\`) 대신 슬래시(`/`)를 사용하거나 원시 문자열(`r"..."`)을 사용하는 것이 좋습니다.
*   **복합 조건 검색**: 여러 조건을 결합하여 데이터를 필터링할 때는 반드시 비트wise 논리 연산자(`&`, `|`)를 사용하고 각 조건을 괄호로 묶어야 합니다.
*   **파일 저장 시 인코딩**: CSV나 텍스트 파일을 저장할 때 한글 깨짐 현상이 발생한다면 `encoding` 옵션을 `cp949` 또는 `euc-kr` 등으로 명시적으로 지정해야 할 수 있습니다.
*   **상대 경로 사용 권장**: 프로젝트의 이식성을 높이기 위해 가능한 한 상대 경로를 사용하는 것이 좋습니다.

---

[⏮️ 이전 문서](./0701_ML정리.md) | [다음 문서 ⏭️](./0703_ML정리.md)
