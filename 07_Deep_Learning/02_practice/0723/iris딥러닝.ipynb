{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris (붓꽃) 품종 분류 (기본 신경망)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북은 Scikit-learn의 Iris 데이터셋을 사용하여 붓꽃의 품종을 분류하는 간단한 심층 신경망(DNN)을 구축, 훈련, 평가합니다.\n",
    "\n",
    "**프로세스:**\n",
    "1. **데이터 로딩 및 분할**: Iris 데이터셋을 로드하고 훈련 세트와 테스트 세트로 분할합니다.\n",
    "2. **데이터 전처리**: 특성(Feature)은 `StandardScaler`를 사용하여 스케일링하고, 레이블(Label)은 `to_categorical`을 사용하여 원-핫 인코딩합니다.\n",
    "3. **모델 구축**: `Dense` 레이어로 구성된 순차 모델을 정의합니다.\n",
    "4. **훈련 및 검증**: 모델을 학습시키고, 매 에포크마다 검증 세트로 성능을 모니터링합니다.\n",
    "5. **결과 시각화**: 훈련 과정의 정확도와 손실을 그래프로 시각화하여 과적합 여부를 확인합니다.\n",
    "6. **최종 평가**: 학습된 모델을 테스트 데이터셋으로 최종 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 재현성을 위한 랜덤 시드 설정\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩 및 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris['data']\n",
    "y = iris[\"target\"]\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 7:3 비율로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "\n",
    "print(f\"훈련 데이터 형태: {X_train.shape}\")\n",
    "print(f\"테스트 데이터 형태: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 스케일링 (평균 0, 분산 1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # 테스트 데이터는 훈련 데이터의 스케일러로 변환\n",
    "\n",
    "# 레이블 원-핫 인코딩 (e.g., 2 -> [0, 0, 1])\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "print(\"스케일링된 첫 번째 훈련 데이터:\\\n\", X_train_scaled[0])\n",
    "print(\"원-핫 인코딩된 첫 번째 훈련 레이블:\", y_train_one_hot[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 구축 및 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax') # 3개의 클래스에 대한 확률을 출력\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy', # 다중 클래스 분류를 위한 손실 함수\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_scaled, y_train_one_hot,\n",
    "                    epochs=50, # 에포크 50으로 증가\n",
    "                    batch_size=16, # 배치 사이즈 조정\n",
    "                    validation_data=(X_test_scaled, y_test_one_hot), # 검증 데이터 추가\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 테스트 데이터셋 최종 평가 ---\")\n",
    "test_loss, test_acc = model.evaluate(X_test_scaled, y_test_one_hot)\n",
    "print(f\"테스트 손실: {test_loss:.4f}\")\n",
    "print(f\"테스트 정확도: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}