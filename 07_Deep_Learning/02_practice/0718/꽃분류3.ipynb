{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 꽃 이미지 분류 (이진 분류 방식)\n",
    "\n",
    "이 노트북은 데이지(daisy)와 민들레(dandelion) 이미지를 분류하는 딥러닝 모델을 **이진 분류(Binary Classification)** 방식으로 구축합니다. 이전의 다중 분류 접근법(`softmax`, `sparse_categorical_crossentropy`)과 달리, 여기서는 `sigmoid` 활성화 함수와 `binary_crossentropy` 손실 함수를 사용합니다.\n",
    "\n",
    "**주요 차이점:**\n",
    "- **출력층**: `Dense(1, activation='sigmoid')`를 사용하여 하나의 뉴런이 '클래스 1일 확률'을 직접 출력하도록 합니다.\n",
    "- **손실 함수**: `binary_crossentropy`를 사용하여 이진 분류 문제에 최적화된 손실을 계산합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 임포트 및 기본 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image as pilimg\n",
    "import imghdr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "# 기본 경로 설정\n",
    "base_path = \"./data/flowers2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 준비: 이미지 파일을 NumPy 배열로 변환\n",
    "\n",
    "모든 이미지를 일정한 크기(80x80)로 조정하고 NumPy 배열로 변환하여 `.npz` 파일로 저장합니다. 이 과정은 한 번만 실행하면 되며, 이 노트북의 다른 버전(`꽃분류2.ipynb`)에서 이미 실행했다면 건너뛸 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_from_images(flower_name, label, is_train=True):\n",
    "    \"\"\"특정 꽃 이미지를 읽어 npz 파일로 저장합니다.\"\"\"\n",
    "    path_type = \"train\" if is_train else \"test\"\n",
    "    path = os.path.join(base_path, path_type, flower_name)\n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    print(f\"'{flower_name}' ({path_type}) 카테고리 처리 시작...\")\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"  경로를 찾을 수 없습니다: {path}\")\n",
    "        return\n",
    "        \n",
    "    filenames = os.listdir(path)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"  {i + 1} / {len(filenames)} 번째 파일 처리 중...\")\n",
    "        \n",
    "        file_path = os.path.join(path, filename)\n",
    "        try:\n",
    "            if imghdr.what(file_path) in [\"gif\", \"png\", \"jpeg\", \"jpg\"]:\n",
    "                img = pilimg.open(file_path)\n",
    "                resize_img = img.resize((80, 80))\n",
    "                pixel = np.array(resize_img)\n",
    "                if pixel.shape == (80, 80, 3):\n",
    "                    data.append(pixel)\n",
    "                    labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"  파일 처리 오류: {filename}, 오류: {e}\")\n",
    "\n",
    "    save_filename = f\"imagedata_{label}_{path_type}.npz\"\n",
    "    np.savez(save_filename, data=data, targets=labels)\n",
    "    print(f\"'{flower_name}' ({path_type}) 데이터 저장 완료 -> {save_filename}\")\n",
    "\n",
    "def initialize_dataset():\n",
    "    \"\"\"모든 카테고리에 대해 데이터 생성을 수행합니다.\"\"\"\n",
    "    if len(glob.glob('imagedata_*.npz')) >= 4:\n",
    "        print(\"이미 전처리된 .npz 파일들이 존재합니다. 데이터 생성을 건너뜁니다.\")\n",
    "        return\n",
    "\n",
    "    flowers = [\"daisy\", \"dandelion\"] # 0: daisy, 1: dandelion\n",
    "    for i, f in enumerate(flowers):\n",
    "        make_data_from_images(f, i, is_train=True)\n",
    "        make_data_from_images(f, i, is_train=False)\n",
    "    print(\"\\n모든 데이터 저장 완료.\")\n",
    "\n",
    "# 데이터 준비 함수 실행\n",
    "initialize_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 로딩 및 전처리\n",
    "생성된 `.npz` 파일들을 불러와 훈련 및 테스트 데이터셋을 구성하고, DNN 모델에 맞게 전처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"npz 파일들을 로드하고 전처리하여 최종 데이터셋을 반환합니다.\"\"\"\n",
    "    with np.load(\"imagedata_0_train.npz\") as f1, np.load(\"imagedata_1_train.npz\") as f2:\n",
    "        X_train = np.concatenate((f1[\"data\"], f2[\"data\"]), axis=0)\n",
    "        y_train = np.concatenate((f1[\"targets\"], f2[\"targets\"]), axis=0)\n",
    "\n",
    "    with np.load(\"imagedata_0_test.npz\") as f1, np.load(\"imagedata_1_test.npz\") as f2:\n",
    "        X_test = np.concatenate((f1[\"data\"], f2[\"data\"]), axis=0)\n",
    "        y_test = np.concatenate((f1[\"targets\"], f2[\"targets\"]), axis=0)\n",
    "        \n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], 80 * 80 * 3)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], 80 * 80 * 3)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "    X_test_scaled = scaler.transform(X_test_flat)\n",
    "    \n",
    "    return X_train_scaled, y_train, X_test_scaled, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "print(\"--- 최종 데이터 형태 ---\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 구축 (이진 분류 방식)\n",
    "이진 분류를 위한 DNN 모델을 정의합니다. 출력층에 `Dense(1, activation='sigmoid')`를 사용하여 '민들레(클래스 1)일 확률'을 0과 1 사이의 값으로 출력합니다. 손실 함수로는 `binary_crossentropy`를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_binary_model():\n",
    "    network = models.Sequential([\n",
    "        layers.Input(shape=(80 * 80 * 3,)),\n",
    "        layers.Dense(128, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(32, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\") # 이진 분류를 위한 출력층\n",
    "    ])\n",
    "\n",
    "    network.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy', # 이진 분류를 위한 손실 함수\n",
    "                    metrics=['accuracy'])\n",
    "    return network\n",
    "\n",
    "model = create_binary_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델 훈련\n",
    "전처리된 데이터를 사용하여 모델을 학습시킵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 평가 및 결과 시각화\n",
    "훈련된 모델의 최종 성능을 평가하고, 훈련 과정 동안의 손실과 정확도 변화를 그래프로 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 최종 모델 평가 ---\")\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"훈련셋   => 손실값: {train_loss:.4f}, 정확도: {train_acc:.4f}\")\n",
    "print(f\"테스트셋 => 손실값: {test_loss:.4f}, 정확도: {test_acc:.4f}\")\n",
    "\n",
    "# 훈련 과정 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}