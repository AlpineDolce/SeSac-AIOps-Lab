{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 컬러 이미지 분류: DNN vs CNN\n",
    "\n",
    "이 노트북은 CIFAR-10 컬러 이미지 데이터셋을 사용하여 두 가지 다른 종류의 신경망 모델을 구축하고 성능을 비교합니다.\n",
    "\n",
    "1. **완전 연결 신경망 (DNN)**: 이미지를 1차원 벡터로 펼쳐서 학습하는 기본적인 딥러닝 모델입니다.\n",
    "2. **합성곱 신경망 (CNN)**: 이미지의 공간적 특징을 효과적으로 학습하기 위해 설계된 모델입니다.\n",
    "\n",
    "두 모델의 성능 차이를 통해 CNN이 이미지 처리에 왜 더 강력한지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 임포트 및 데이터 로드\n",
    "필요한 라이브러리를 임포트하고 Keras에 내장된 CIFAR-10 데이터셋을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras import models, layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = cifar10.load_data()\n",
    "\n",
    "print(\"--- Original Data Shapes ---\")\n",
    "print(\"X_train shape:\", X_train_orig.shape)\n",
    "print(\"y_train shape:\", y_train_orig.shape)\n",
    "print(\"X_test shape:\", X_test_orig.shape)\n",
    "print(\"y_test shape:\", y_test_orig.shape)\n",
    "print(\"Number of unique labels:\", len(np.unique(y_train_orig)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 탐색 및 시각화\n",
    "데이터셋에 포함된 이미지가 어떤 모습인지 확인하기 위해 몇 개의 이미지를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imageShow(images, index):\n",
    "    \"\"\"지정된 인덱스의 이미지를 하나 보여줍니다.\"\"\"\n",
    "    plt.imshow(images[index], cmap=plt.cm.binary)\n",
    "    plt.title(f\"Image at index {index}\")\n",
    "    plt.show()\n",
    "\n",
    "def imageShowGrid(images, rows, cols):\n",
    "    \"\"\"이미지를 그리드 형태로 여러 개 보여줍니다.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(rows * cols):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(images[i], cmap=plt.cm.binary)\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# 단일 이미지 출력\n",
    "print(\"Showing a single image:\")\n",
    "imageShow(X_train_orig, 0)\n",
    "\n",
    "# 여러 이미지 출력\n",
    "print(\"\\nShowing a grid of images:\")\n",
    "imageShowGrid(X_train_orig, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 1: 완전 연결 신경망 (DNN)\n",
    "첫 번째로, 이미지를 1차원 배열로 펼친 후 완전 연결층(Dense layer)으로만 구성된 모델을 만들어 학습시킵니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 DNN을 위한 데이터 전처리\n",
    "- **Reshape**: 3차원 이미지 데이터(32x32x3)를 1차원 벡터(3072)로 변환합니다.\n",
    "- **Scaling**: 픽셀 값을 0-1 범위로 정규화합니다.\n",
    "- **One-Hot Encoding**: 정수 형태의 레이블을 원-핫 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN용 데이터 복사 및 전처리\n",
    "X_train_dnn = X_train_orig.reshape(50000, 32 * 32 * 3).astype('float32') / 255\n",
    "X_test_dnn = X_test_orig.reshape(10000, 32 * 32 * 3).astype('float32') / 255\n",
    "\n",
    "y_train_cat = to_categorical(y_train_orig)\n",
    "y_test_cat = to_categorical(y_test_orig)\n",
    "\n",
    "print(\"X_train_dnn shape:\", X_train_dnn.shape)\n",
    "print(\"y_train_cat shape:\", y_train_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 DNN 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = models.Sequential([\n",
    "    layers.Input(shape=(32 * 32 * 3,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "dnn_model.compile(optimizer='sgd',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "dnn_model.summary()\n",
    "\n",
    "print(\"\\nDNN 모델 학습 시작\")\n",
    "history_dnn = dnn_model.fit(X_train_dnn, y_train_cat, epochs=100, batch_size=100, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 DNN 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- DNN Model Evaluation ---\")\n",
    "train_loss, train_acc = dnn_model.evaluate(X_train_dnn, y_train_cat)\n",
    "print(f\"훈련셋 손실: {train_loss:.4f}, 정확도: {train_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = dnn_model.evaluate(X_test_dnn, y_test_cat)\n",
    "print(f\"테스트셋 손실: {test_loss:.4f}, 정확도: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델 2: 합성곱 신경망 (CNN)\n",
    "두 번째로, 이미지의 2D 구조를 그대로 입력받아 처리하는 합성곱 신경망(CNN)을 구성하고 학습시킵니다.\n",
    "\n",
    "**주요 레이어 설명:**\n",
    "- **Conv2D**: 이미지의 특징을 추출하는 필터를 학습합니다.\n",
    "- **MaxPooling2D**: 중요한 특징만 남기고 데이터 크기를 줄여 과대적합을 방지합니다.\n",
    "- **Flatten**: CNN의 다차원 출력을 완전 연결망에 전달하기 위해 1차원 벡터로 변환합니다.\n",
    "- **Dropout**: 훈련 중 일부 뉴런을 비활성화하여 과대적합을 방지합니다 (이 코드에서는 사용되지 않았지만 일반적인 기법)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 CNN을 위한 데이터 전처리\n",
    "CNN은 3차원 이미지 데이터를 직접 입력으로 받으므로 Reshape 과정이 필요 없습니다. 스케일링과 원-핫 인코딩만 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN용 데이터는 원본 형태를 유지하고 스케일링만 진행합니다.\n",
    "# 레이블은 이미 원-핫 인코딩 되었습니다.\n",
    "X_train_cnn = X_train_orig.astype('float32')\n",
    "X_test_cnn = X_test_orig.astype('float32')\n",
    "\n",
    "print(\"X_train_cnn shape:\", X_train_cnn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 CNN 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.Sequential([\n",
    "    layers.Input(shape=(32, 32, 3)),\n",
    "    layers.Rescaling(1./255), # 입력 스케일링\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "cnn_model.compile(optimizer='rmsprop',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "print(\"\\nCNN 모델 학습 시작\")\n",
    "# 에포크 수를 줄여서 실행 시간을 단축합니다 (원본: 140)\n",
    "history_cnn = cnn_model.fit(X_train_cnn, y_train_cat, epochs=20, batch_size=100, validation_data=(X_test_cnn, y_test_cat), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 CNN 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- CNN Model Evaluation ---\")\n",
    "train_loss, train_acc = cnn_model.evaluate(X_train_cnn, y_train_cat)\n",
    "print(f\"훈련셋 손실: {train_loss:.4f}, 정확도: {train_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = cnn_model.evaluate(X_test_cnn, y_test_cat)\n",
    "print(f\"테스트셋 손실: {test_loss:.4f}, 정확도: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 결론\n",
    "두 모델의 최종 테스트 정확도를 비교해보면, CNN 모델이 DNN 모델보다 훨씬 높은 성능을 보이는 것을 확인할 수 있습니다. 이는 CNN이 이미지의 공간적 계층 구조(spatial hierarchy)를 보존하고 지역적 패턴(local pattern)을 효과적으로 학습하기 때문입니다. 반면, DNN은 이미지를 1차원 벡터로 펼치는 과정에서 이러한 중요한 공간 정보가 손실됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}