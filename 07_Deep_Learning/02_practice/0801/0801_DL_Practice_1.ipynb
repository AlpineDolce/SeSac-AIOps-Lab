{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac276b0c",
   "metadata": {},
   "source": [
    "## 자연어 임베딩\n",
    "\n",
    "### 임베딩층 - 내부적으로는 연산을 해서 단어와 단어 사이의 관계를 계산해서 밀집벡터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601d40d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n",
      "압축풀기 완료\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os, pathlib, shutil, random\n",
    "\n",
    "#데이터 다운로드 \n",
    "def download():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "\n",
    "    response = requests.get(url, stream=True)  # 스트리밍 방식으로 다운로드\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=8192):  # 8KB씩 다운로드\n",
    "            file.write(chunk)\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "\n",
    "\n",
    "#압축풀기 : 프로그램 호출 -> 프로세스 , tar 라이브러리가 있어야 한다 \n",
    "def release():\n",
    "    subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True) #tar 프로그램 가동하기 \n",
    "    #tar.gz => linux에서는 파일을 여러개를 한번에 압축을 못함 tar라는 형식으로 압축할 모든 파일을 하나로 묶어서 패키지로 만든다음에 \n",
    "    #          압축을 한다.  tar , gz가동  그래서 압축풀고 다시 패키지도 풀어야 한다. \n",
    "    #          tar  -xvzf 파일명   형태임         \n",
    "    print(\"압축풀기 완료\")\n",
    "\n",
    "\n",
    "#train => train과 validation으로 나눠야 한다. , train 폴더에 있는 unsup 폴더는 직접 지워냐 한다. \n",
    "#라벨이 2개 여야 한다. \n",
    "\n",
    "#라벨링 \n",
    "def labeling(): \n",
    "    base_dir = pathlib.Path(\"aclImdb\") \n",
    "    val_dir = base_dir/\"val\"   # pathlib 객체에  / \"디렉토리\" => 결과가 문자열이 아니다 \n",
    "    train_dir = base_dir/\"train\"\n",
    "\n",
    "    for category in (\"neg\", \"pos\"):\n",
    "        os.makedirs(val_dir/category)  #디렉토리를 만들고 \n",
    "        files = os.listdir(train_dir/category) #해당 카테고리의 파일 목록을 모두 가져온다 \n",
    "        random.Random(1337).shuffle(files) #파일을 랜덤하게 섞어서 복사하려고 파일 목록을 모두 섞는다 \n",
    "        num_val_samples = int(0.2 * len(files)) \n",
    "        val_files = files[-num_val_samples:] #20%만 val폴더로 이동한다 \n",
    "        for fname in val_files:\n",
    "            shutil.move(train_dir/category/fname, val_dir/category/fname )    \n",
    "\n",
    "download() #파일 다운받기 = 용량이 너무 커서 8192만큼씩 잘라서 저장하는 코드임 \n",
    "release()\n",
    "labeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a813c48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 3 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "inputs.shape (32,)\n",
      "inputs.dtype <dtype: 'string'>\n",
      "targets.shape (32,)\n",
      "targets.dtype <dtype: 'int32'>\n",
      "inputs[0] tf.Tensor(\n",
      "[b'I stumbled on this little gem a few months ago on IFC, and just watched it again today. If there was an Academy Award category for \"Best Modernistic Recreation of Classic Hollywood Styles\" this film would have won hands down. Gibson Frazier is amazing, channeling the spirit of Buster Keaton and Harold Lloyd with a script that reads like a Robert Heinlein short story. (Or perhaps Harlan Ellison...) Hang your belief at the door and flow with the fantasy. You are not going to learn any deep truths here. Except perhaps how to make a really snappy film on small change. The plot (what there is of it, and be fair, not many reporter/gangster film from the 20\\'s and 30\\'s did either) moves Frazier from the clean black and white world of New York to its darker underbelly without once getting dirty or losing the starch on his Herbert Hoover collar. Worth you 80 minutes. More so if you love old \\'B\\' movies.'\n",
      " b'This version of \"Moby Dick\" insults the audience by claiming it is based on Melville\\'s novel-even going so far as to show a phony first chapter sentence rather than the famous \"Call me Ishmael\". In addition to having atrocious acting, even from John Barrymore,this is perhaps the greatest example of how far Hollywood (especially early Hollywood) would go to revise and change a famous novel just to beef up its chances at the box office.All of the novel\\'s beautiful,poetic language has been absolutely eradicated, and Ahab has been changed from a brooding,blasphemous,obsessive madman to a dashing,misunderstood hero who only wants to kill Moby Dick after his fiance(!) turns away from him after seeing his wooden leg. To this is added the standard evil brother who wants the fiance for himself, and a different ending!'\n",
      " b'A few earlier comments express the concern that the film does not \"let the GAY man be gay\". Some viewers feel that \"Stage Beauty\" sends a message that only hethero sex is normal and if there was sexual ambiguity at the beginning, all ends \"well\". Some of the negative critics\\' reviews on rotten tomatoes also have a problem with the representation of androgyny and sexuality in the movie. My perception is that the movie did not try to send a message or convey a judgment. Throughout the movie Ned Kynaston did not self-identify as either gay or straight, or as a man or a woman. This constant confusion was made clear in several scenes including the end of the movie. Today we understand that sexuality is better described on a continuum (Kinsey) and therefore it is almost absurd to classify anyone as gay or straight. In the case of Ned Kynaston the lack of self-identification was both in terms of sexual orientation as well as gender. Although the two identity crises were clearly related. A critic on rotten tomatoes begins his review with: \"Stage Beauty\" is \"the ultimate fantasy for women who tend to fall in love with gay men\". This statement is based on at least two wrong assumptions: 1. Ned Kynaston is gay 2. Ned Kynaston stops being gay. I think one of the many strengths of this movie is exactly this non judgmental, factual representation of the complexity of sexual identity. We seem to be making some progress in understanding these issues better as they are being addressed by more and more movies, like \"Alexander\", \"Beautiful Boxer\", \"Brokeback Mountain\".'], shape=(3,), dtype=string)\n",
      "targets[0] tf.Tensor([2 0 2], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#데이터셋을 활용해서 디렉토리로부터 파일을 불러와서 벡터화를 진행한다 \n",
    "import keras \n",
    "batch_size = 32 #한번에 읽어올 양 \n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", #디렉토리명 \n",
    "    batch_size=batch_size\n",
    ")\n",
    "#데이터셋은 알아서  inputs, targets 을 반복해서 갖고 온다. 우리한테 필요한거는 inputs만이다\n",
    "for inputs, targets in train_ds: #실제 읽어오는 데이터 확인 \n",
    "    print(\"inputs.shape\", inputs.shape)\n",
    "    print(\"inputs.dtype\", inputs.dtype)\n",
    "    print(\"targets.shape\", targets.shape)\n",
    "    print(\"targets.dtype\", targets.dtype)\n",
    "    print(\"inputs[0]\", inputs[:3])\n",
    "    print(\"targets[0]\", targets[:3])\n",
    "    break #하나만 출력해보자 \n",
    "#0이 부정 1이 긍정 -> 폴더명을 정렬해서 0,1,2 이런식으로 라벨링을 한다 neg -0 pos-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded14015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32, 600), dtype=int64, numpy=\n",
      "array([[ 1164,    10,     7, ...,     0,     0,     0],\n",
      "       [   10,   216,     5, ...,     0,     0,     0],\n",
      "       [  593, 13747,  2108, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [   11,   197,     1, ...,     0,     0,     0],\n",
      "       [   15,  1398,     8, ...,     0,     0,     0],\n",
      "       [   38,    73,    69, ...,     0,     0,     0]])>, <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
      "array([0, 2, 2, 1, 2, 2, 0, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "#시퀀스를 만들어야 한다 \n",
    "max_length = 600  #한 평론에서 사용하는 단어는 최대 길이를 600개라고 보자  \n",
    "max_tokens = 20000 #자주 사용하는 단어 20000 개만 쓰겠다 \n",
    "\n",
    "text_vectorization = TextVectorization( \n",
    "    max_tokens = max_tokens,\n",
    "    output_mode = \"int\", #임베딩 층을 사요하려면 반드시 int여야 한다\n",
    "    output_sequence_length = max_length  \n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds) #어휘사전을 만들어야 한다 \n",
    "\n",
    "int_train_ds = train_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=1 )\n",
    "int_val_ds = val_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=1 )\n",
    "int_test_ds = test_ds.map( lambda x,y:(text_vectorization(x), y), num_parallel_calls=1 )\n",
    "\n",
    "#내부구조 살짝 보기 \n",
    "for item in int_train_ds:\n",
    "    print(item)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96834aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, 256)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │     \u001b[38;5;34m5,120,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m73,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,194,049</span> (19.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,194,049\u001b[0m (19.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,194,049</span> (19.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,194,049\u001b[0m (19.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# 임베딩층 - 내부적으로는 연산을 해서 단어와 단어 사이의 관계를 계산해서 밀집벡터를 만든다.\n",
    "# 원핫인코딩 - 메모리를 너무 많이 차지함 최대한 한문장을 표현하는데 만일 최대 20000 단어까지 처리한다면\n",
    "# 한문장당 20000개가 필요, 희소행렬 요소가 거의 다 0인데 그 중 몇개가 값이 있을 때, 학습 시 속도가 엄청 느리다.\n",
    "# 임베딩층을 사용한다. => 단어와 단어사이의 거리를 재는 방식인데, 비슷비슷한 단어가 같은 문장에 나타난다고 전제하고 문제를 해결한다.\n",
    "# 틀릴 수 있다.\n",
    "# 케라스가 Embedding 레이어를 제공한다. 이 레이어는 반드시 정수 인덱스를 받아야 한다. 시퀀스를 받아서 밀집벡터를 만든다.\n",
    "# 알고리즘 공개 안함. 파이썬은 소스 공개 안할 방법이 없다.  ProgramData/anaconda3/envs/가상환경/libs/site-packages\n",
    "\n",
    "from keras import models, layers\n",
    "import tensorflow as tf \n",
    "embedding_dim = 600 \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs) # 미리 학습된 임베딩층으로 바꿀 수 있다.\n",
    "# 입력벡터크기는 20000, 출력벡터는 256(특별한 의미 없이 마음대로)의 크기를 갖는다.\n",
    "print(embedded.shape)\n",
    "#양방향 RNN 을 가동시킴 \n",
    "x = layers.Bidirectional( layers.LSTM(32))(embedded) \n",
    "x = layers.Dropout(0.5)(x) \n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs, outputs) \n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10)\n",
    "print(\"테스트셋 \", model.evaluate(int_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbf7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de482492",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47129487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b304d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ed191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5a161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
