{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 영화 리뷰 감성 분석 (기본 신경망)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 노트북은 Keras에 내장된 IMDB 데이터셋을 사용하여 영화 리뷰가 긍정적인지 부정적인지를 분류하는 신경망을 구축, 훈련, 평가합니다.\n",
    "\n",
    "**프로세스:**\n",
    "1. **데이터 로딩**: IMDB 데이터셋을 로드합니다. 리뷰는 전처리되어 단어 시퀀스를 나타내는 정수 배열로 제공됩니다.\n",
    "2. **데이터 탐색**: 정수 시퀀스를 다시 텍스트로 디코딩하여 원본 리뷰 내용을 확인합니다.\n",
    "3. **데이터 전처리**: 신경망에 입력할 수 있도록 정수 시퀀스를 원-핫 인코딩(multi-hot encoding) 방식의 벡터로 변환합니다.\n",
    "4. **모델 구축**: `Dense` 레이어로 구성된 간단한 순차 모델을 정의합니다.\n",
    "5. **훈련 및 검증**: 훈련 데이터를 사용하여 모델을 학습시키고, 검증 데이터를 사용하여 성능을 모니터링합니다.\n",
    "6. **결과 시각화 및 평가**: 훈련 과정의 정확도와 손실을 그래프로 시각화하고, 테스트 데이터셋으로 최종 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 빈도가 높은 10,000개의 단어만 사용하도록 데이터 로드\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "print(f\"훈련 데이터 수: {len(train_data)}\")\n",
    "print(f\"테스트 데이터 수: {len(test_data)}\")\n",
    "print(\"첫 번째 훈련 데이터 (정수 시퀀스):\\n\", train_data[0])\n",
    "print(\"첫 번째 훈련 레이블:\", train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 탐색 (정수 시퀀스를 텍스트로 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 정수 인덱스를 매핑하는 딕셔너리 로드\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# 정수 인덱스와 단어를 매핑하도록 딕셔너리 반전\n",
    "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
    "\n",
    "def decode_review(text_sequence):\n",
    "    # 0, 1, 2는 '패딩', '문서 시작', '사전 외 단어'를 위한 특수 인덱스이므로 3을 뺌\n",
    "    return \" \".join([reverse_word_index.get(i - 3, \"?\") for i in text_sequence])\n",
    "\n",
    "print(\"첫 번째 리뷰 디코딩 결과:\\n\", decode_review(train_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 전처리 (벡터화)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    # (len(sequences), dimension) 크기의 0으로 채워진 행렬 생성\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        # sequence의 각 인덱스 위치를 1로 설정\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "# 훈련 데이터와 테스트 데이터를 벡터로 변환\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# 레이블을 float32 타입의 벡터로 변환\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "print(\"벡터화된 첫 번째 훈련 데이터 샘플:\\n\", x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 검증 세트 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 중 10,000개를 검증 세트로 분리\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "print(f\"훈련 데이터 수: {len(partial_x_train)}\")\n",
    "print(f\"검증 데이터 수: {len(x_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 구축 및 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\", input_shape=(10000,)),\n",
    "    layers.Dense(16, activation=\"relu\"),\n",
    "    layers.Dense(1, activation=\"sigmoid\") # 이진 분류이므로 sigmoid 활성화 함수 사용\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\", # 이진 분류를 위한 손실 함수\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 훈련 결과 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 손실 그래프\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 모델 평가 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 테스트 데이터셋 평가 ---\")\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(f\"테스트 손실: {results[0]:.4f}\")\n",
    "print(f\"테스트 정확도: {results[1]:.4f}\")\n",
    "\n",
    "print(\"\\n--- 예측 결과 샘플 ---\")\n",
    "predictions = model.predict(x_test)\n",
    "for i in range(10):\n",
    "    pred_label = \"긍정\" if predictions[i][0] > 0.5 else \"부정\"\n",
    "    true_label = \"긍정\" if y_test[i] == 1 else \"부정\"\n",
    "    print(f\"리뷰 {i+1}: 예측={pred_label} (값: {predictions[i][0]:.3f}), 실제={true_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}