{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f0357c",
   "metadata": {},
   "source": [
    "# MNIST 손글씨 숫자 분류 딥러닝 모델\n",
    "\n",
    "## 🎯 프로젝트 개요\n",
    "이 노트북에서는 MNIST 데이터셋을 사용하여 손글씨 숫자(0-9)를 분류하는 다층 퍼셉트론(MLP) 딥러닝 모델을 구현합니다. 딥러닝의 가장 기본적인 'Hello, World!' 예제로서, 모델 개발의 전체 워크플로우를 경험하는 것을 목표로 합니다.\n",
    "\n",
    "## 📚 학습 목표\n",
    "- TensorFlow/Keras를 사용한 딥러닝 모델 구현 워크플로우 이해\n",
    "- 대용량 이미지 데이터셋(MNIST)의 로드 및 전처리 방법 습득\n",
    "- `Sequential` 모델을 이용한 다층 퍼셉트론(MLP) 신경망 구조 설계\n",
    "- 다중 분류 문제에 대한 손실 함수(`sparse_categorical_crossentropy`) 및 활성화 함수(`softmax`)의 올바른 사용법 학습\n",
    "- 모델 학습(`fit`) 및 성능 평가(`evaluate`) 과정 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c0202b",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트 및 환경 설정\n",
    "\n",
    "딥러닝 모델 구현에 필요한 라이브러리를 임포트하고, 실험의 재현성을 보장하기 위해 랜덤 시드를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1429ac4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",",
     "output_type": "stream",
     "text": [
      "TensorFlow 버전: 2.19.0\n",
      "라이브러리 임포트 완료!\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "from tensorflow.keras.datasets import mnist  # MNIST 데이터셋\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "# 재현 가능한 결과를 위한 랜덤 시드 설정\n",
    "tf.random.set_seed(1234)\n",
    "\n",
    "print(\"TensorFlow 버전:\", tf.__version__)\n",
    "print(\"라이브러리 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd093c",
   "metadata": {},
   "source": [
    "## 2. MNIST 데이터셋 로드 및 탐색\n",
    "\n",
    "Keras에 내장된 `mnist.load_data()` 함수를 사용하여 MNIST 손글씨 숫자 데이터셋을 로드하고, 데이터의 구조와 형태를 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f769a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "데이터 타입:\n",
      "train_images: <class 'numpy.ndarray'>\n",
      "train_labels: <class 'numpy.ndarray'>\n",
      "\n",
      "데이터 형태:\n",
      "훈련 이미지: (60000, 28, 28)\n",
      "훈련 라벨: (60000,)\n",
      "테스트 이미지: (10000, 28, 28)\n",
      "테스트 라벨: (10000,)\n",
      "\n",
      "총 데이터 개수: 70,000개\n",
      "✅ 데이터 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋 로드\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 데이터 타입 확인\n",
    "print(\"데이터 타입:\")\n",
    "print(f\"train_images: {type(train_images)}\")\n",
    "print(f\"train_labels: {type(train_labels)}\")\n",
    "\n",
    "# 데이터 형태(shape) 확인\n",
    "print(\"\n데이터 형태:\")\n",
    "print(f\"훈련 이미지: {train_images.shape}\")  # (60000, 28, 28)\n",
    "print(f\"훈련 라벨: {train_labels.shape}\")    # (60000,)\n",
    "print(f\"테스트 이미지: {test_images.shape}\") # (10000, 28, 28)\n",
    "print(f\"테스트 라벨: {test_labels.shape}\")   # (10000,)\n",
    "\n",
    "print(f\"\n총 데이터 개수: {len(train_images) + len(test_images):,}개\")\n",
    "print(\"✅ 데이터 로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1100c466",
   "metadata": {},
   "source": [
    "## 3. 신경망 모델 구조 설계\n",
    "\n",
    "`Sequential` 모델을 사용하여 층을 순서대로 쌓아 다층 퍼셉트론(MLP) 구조를 설계합니다.\n",
    "\n",
    "### 모델 구조:\n",
    "- **입력층**: 784개 뉴런 (28x28 픽셀을 1차원 벡터로 평탄화하여 입력)\n",
    "- **은닉층 1**: 256개 뉴런, `ReLU` 활성화 함수\n",
    "- **은닉층 2**: 256개 뉴런, `ReLU` 활성화 함수\n",
    "- **은닉층 3**: 128개 뉴런, `ReLU` 활성화 함수\n",
    "- **은닉층 4**: 64개 뉴런, `ReLU` 활성화 함수\n",
    "- **출력층**: 10개 뉴런, `Softmax` 활성화 함수 (10개 클래스에 대한 확률 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fe3a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 신경망 모델 구조 설계 완료!\n",
      "\n",
      "참고: 출력층 설계 패턴\n",
      "- 회귀 문제: layers.Dense(1)\n",
      "- 이진 분류: layers.Dense(1, activation='sigmoid')\n",
      "- 다중 분류: layers.Dense(클래스 수, activation='softmax')\n"
     ]
    }
   ],
   "source": [
    "# Sequential 모델을 사용하여 신경망 구조 설계\n",
    "model = keras.Sequential([\n",
    "    # 은닉층 1: 256개 뉴런, ReLU 활성화 함수\n",
    "    # Dense 레이어의 매개변수: (출력 뉴런 수, 활성화 함수)\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    # 은닉층 2: 256개 뉴런, ReLU 활성화 함수\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    \n",
    "    # 은닉층 3: 128개 뉴런, ReLU 활성화 함수\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    \n",
    "    # 은닉층 4: 64개 뉴런, ReLU 활성화 함수\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # 출력층: 10개 뉴런 (0~9 숫자), Softmax 활성화 함수\n",
    "    # 다중분류에서는 출력층에 반드시 softmax 함수 사용\n",
    "    # softmax: 각 클래스에 대한 확률을 출력 (합이 1이 됨)\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "print(\"✅ 신경망 모델 구조 설계 완료!\")\n",
    "print(\"\n참고: 출력층 설계 패턴\")\n",
    "print(\"- 회귀 문제: layers.Dense(1)\")\n",
    "print(\"- 이진 분류: layers.Dense(1, activation='sigmoid')\")\n",
    "print(\"- 다중 분류: layers.Dense(클래스 수, activation='softmax')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac2c52",
   "metadata": {},
   "source": [
    "## 4. 모델 컴파일 설정\n",
    "\n",
    "모델을 학습할 수 있도록 옵티마이저, 손실 함수, 평가지표를 설정합니다.\n",
    "\n",
    "### 설정 요소:\n",
    "- **옵티마이저 (Optimizer)**: `rmsprop`. 손실 함수를 최소화하기 위해 가중치를 업데이트하는 알고리즘입니다.\n",
    "- **손실 함수 (Loss Function)**: `sparse_categorical_crossentropy`. 다중 분류 문제에서 라벨이 정수 형태일 때 사용합니다.\n",
    "- **평가지표 (Metrics)**: `accuracy`. 모델의 성능을 직관적으로 파악하기 위한 지표입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bacd8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 모델 컴파일 완료!\n",
      "\n",
      "📝 손실함수 비교:\n",
      "- sparse_categorical_crossentropy: 라벨이 정수 형태 (0, 1, 2, ...)\n",
      "- categorical_crossentropy: 라벨이 원-핫 인코딩 형태 ([1,0,0], [0,1,0], ...)\n",
      "  → 현재는 라벨이 정수 형태이므로 sparse_categorical_crossentropy 사용\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일 설정\n",
    "model.compile(\n",
    "    optimizer='rmsprop',                      # 옵티마이저: RMSprop\n",
    "    loss='sparse_categorical_crossentropy',   # 손실함수: 다중분류용\n",
    "    metrics=['accuracy']                      # 평가지표: 정확도\n",
    ")\n",
    "\n",
    "print(\"✅ 모델 컴파일 완료!\")\n",
    "print(\"\n📝 손실함수 비교:\")\n",
    "print(\"- sparse_categorical_crossentropy: 라벨이 정수 형태 (0, 1, 2, ...)\")\n",
    "print(\"- categorical_crossentropy: 라벨이 원-핫 인코딩 형태 ([1,0,0], [0,1,0], ...)\")\n",
    "print(\"  → 현재는 라벨이 정수 형태이므로 sparse_categorical_crossentropy 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e97ba",
   "metadata": {},
   "source": [
    "## 5. 데이터 전처리 (차원 변환 및 정규화)\n",
    "\n",
    "딥러닝 모델이 처리할 수 있는 형태로 데이터를 가공합니다.\n",
    "\n",
    "### 전처리 과정:\n",
    "1. **차원 변환**: 2차원 이미지(28×28)를 1차원 벡터(784)로 평탄화합니다.\n",
    "2. **정규화**: 픽셀 값을 0~255 범위에서 0~1 범위로 스케일링하여 학습 안정성을 높입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be833cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 전 데이터 형태:\n",
      "훈련 이미지: (60000, 28, 28)\n",
      "테스트 이미지: (10000, 28, 28)\n",
      "픽셀 값 범위: 0 ~ 255\n",
      "\n",
      "전처리 후 데이터 형태:\n",
      "훈련 이미지: (60000, 784)\n",
      "테스트 이미지: (10000, 784)\n",
      "정규화된 픽셀 값 범위: 0.0 ~ 1.0\n",
      "\n",
      "✅ 데이터 전처리 완료!\n",
      "📝 정규화가 중요한 이유: 딥러닝에서는 입력값의 스케일이 학습에 큰 영향을 미치며, 0~1 사이의 값으로 정규화하면 학습이 더 안정적이고 빠르게 진행됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 전처리 전 데이터 형태 확인\n",
    "print(\"전처리 전 데이터 형태:\")\n",
    "print(f\"훈련 이미지: {train_images.shape}\")\n",
    "print(f\"테스트 이미지: {test_images.shape}\")\n",
    "print(f\"픽셀 값 범위: {train_images.min()} ~ {train_images.max()}\")\n",
    "\n",
    "# 1. 차원 변환: 3차원 → 2차원 (28×28 → 784)\n",
    "train_images = train_images.reshape(train_images.shape[0], 28 * 28)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28 * 28)\n",
    "\n",
    "# 2. 정규화: 픽셀 값을 0~1 범위로 스케일링 (딥러닝에서는 필수!)\n",
    "train_images = train_images.astype(float) / 255\n",
    "test_images = test_images.astype(float) / 255\n",
    "\n",
    "# 전처리 후 데이터 형태 확인\n",
    "print(\"\n전처리 후 데이터 형태:\")\n",
    "print(f\"훈련 이미지: {train_images.shape}\")\n",
    "print(f\"테스트 이미지: {test_images.shape}\")\n",
    "print(f\"정규화된 픽셀 값 범위: {train_images.min():.1f} ~ {train_images.max():.1f}\")\n",
    "\n",
    "print(\"\n✅ 데이터 전처리 완료!\")\n",
    "print(\"📝 정규화가 중요한 이유: 딥러닝에서는 입력값의 스케일이 학습에 큰 영향을 미치며, 0~1 사이의 값으로 정규화하면 학습이 더 안정적이고 빠르게 진행됩니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40cbff4",
   "metadata": {},
   "source": [
    "## 6. 모델 학습 실행\n",
    "\n",
    "`fit()` 함수를 사용하여 모델을 학습시킵니다.\n",
    "\n",
    "### 학습 매개변수:\n",
    "- **epochs**: 100 (전체 데이터셋을 100번 반복 학습)\n",
    "- **batch_size**: 128 (한 번에 128개 샘플씩 처리하여 가중치를 업데이트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31068c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 모델 학습을 시작합니다...\n",
      "⏰ 학습 시간: 약 5-10분 소요 예상\n",
      "Epoch 1/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8366 - loss: 0.5156\n",
      "Epoch 2/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9640 - loss: 0.1171\n",
      "Epoch 3/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9773 - loss: 0.0732\n",
      "Epoch 4/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9840 - loss: 0.0516\n",
      "Epoch 5/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9884 - loss: 0.0378\n",
      "Epoch 6/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9911 - loss: 0.0292\n",
      "Epoch 7/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9928 - loss: 0.0223\n",
      "Epoch 8/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9936 - loss: 0.0202\n",
      "Epoch 9/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9951 - loss: 0.0173\n",
      "Epoch 10/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9951 - loss: 0.0156\n",
      "Epoch 11/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9956 - loss: 0.0139\n",
      "Epoch 12/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9962 - loss: 0.0124\n",
      "Epoch 13/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9957 - loss: 0.0155\n",
      "Epoch 14/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9970 - loss: 0.0108\n",
      "Epoch 15/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9971 - loss: 0.0092\n",
      "Epoch 16/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9975 - loss: 0.0081\n",
      "Epoch 17/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9971 - loss: 0.0105\n",
      "Epoch 18/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0080\n",
      "Epoch 19/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9981 - loss: 0.0078\n",
      "Epoch 20/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9977 - loss: 0.0085\n",
      "Epoch 21/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9980 - loss: 0.0060\n",
      "Epoch 22/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9978 - loss: 0.0088\n",
      "Epoch 23/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0063\n",
      "Epoch 24/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0051\n",
      "Epoch 25/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0050\n",
      "Epoch 26/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9984 - loss: 0.0062\n",
      "Epoch 27/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9984 - loss: 0.0047\n",
      "Epoch 28/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0054\n",
      "Epoch 29/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9985 - loss: 0.0048\n",
      "Epoch 30/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9987 - loss: 0.0042\n",
      "Epoch 31/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9986 - loss: 0.0055\n",
      "Epoch 32/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.0048\n",
      "Epoch 33/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9984 - loss: 0.0061\n",
      "Epoch 34/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0039\n",
      "Epoch 35/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0028\n",
      "Epoch 36/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9990 - loss: 0.0036\n",
      "Epoch 37/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0023\n",
      "Epoch 38/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0021\n",
      "Epoch 39/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0013\n",
      "Epoch 40/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0036\n",
      "Epoch 41/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0023\n",
      "Epoch 42/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0032\n",
      "Epoch 43/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0020\n",
      "Epoch 44/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0032\n",
      "Epoch 45/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0014\n",
      "Epoch 46/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 7.4950e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 8.3530e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0020\n",
      "Epoch 49/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0029\n",
      "Epoch 50/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0013\n",
      "Epoch 51/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0015\n",
      "Epoch 52/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0014\n",
      "Epoch 53/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9993 - loss: 0.0025\n",
      "Epoch 54/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0019\n",
      "Epoch 55/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9994 - loss: 0.0022\n",
      "Epoch 56/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9992 - loss: 0.0038\n",
      "Epoch 57/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0015\n",
      "Epoch 58/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 8.7001e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 9.8383e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 5.8939e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0015\n",
      "Epoch 62/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 3.0743e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9997 - loss: 0.0011\n",
      "Epoch 64/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0016\n",
      "Epoch 65/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0010    \n",
      "Epoch 66/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9995 - loss: 0.0018\n",
      "Epoch 67/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0017\n",
      "Epoch 68/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0013\n",
      "Epoch 69/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 7.9794e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 1.7576e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 2.3871e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 7.8351e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 6.6011e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9998 - loss: 0.0014\n",
      "Epoch 75/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 4.8495e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9996 - loss: 0.0029\n",
      "Epoch 77/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9999 - loss: 5.0457e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 2.6942e-06\n",
      "Epoch 79/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.6730e-07\n",
      "Epoch 80/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.8328e-07\n",
      "Epoch 81/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.5924e-07\n",
      "Epoch 82/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.4336e-07\n",
      "Epoch 83/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.3147e-07\n",
      "Epoch 84/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.2216e-07\n",
      "Epoch 85/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.1439e-07\n",
      "Epoch 86/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0786e-07\n",
      "Epoch 87/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.0224e-07\n",
      "Epoch 88/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.7377e-08\n",
      "Epoch 89/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.3105e-08\n",
      "Epoch 90/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.9269e-08\n",
      "Epoch 91/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.5844e-08\n",
      "Epoch 92/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.2698e-08\n",
      "Epoch 93/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.9829e-08\n",
      "Epoch 94/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.7245e-08\n",
      "Epoch 95/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 7.4852e-08\n",
      "Epoch 96/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.2574e-08\n",
      "Epoch 97/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.0533e-08\n",
      "Epoch 98/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.8662e-08\n",
      "Epoch 99/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.6842e-08\n",
      "Epoch 100/100\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.5163e-08\n",
      "\n",
      "✅ 모델 학습 완료!\n",
      "\n",
      "📝 배치 크기(batch_size) 설명:\n",
      "- 너무 크면: 메모리 부족 발생 가능\n",
      "- 너무 작으면: 학습 속도가 느려짐\n",
      "- 적절한 크기: 32, 64, 128, 256 등의 2의 거듭제곱 값 사용\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행\n",
    "print(\"🚀 모델 학습을 시작합니다...\")\n",
    "print(\"⏰ 학습 시간: 약 5-10분 소요 예상\")\n",
    "\n",
    "hist = model.fit(\n",
    "    train_images,    # X: 독립변수, 입력 데이터\n",
    "    train_labels,    # y: 종속변수, 타겟 라벨\n",
    "    epochs=100,      # 학습 회수 (전체 데이터셋을 100번 반복)\n",
    "    batch_size=128   # 배치 크기 (메모리 효율성을 위해 128개씩 처리)\n",
    ")\n",
    "\n",
    "print(\"\n✅ 모델 학습 완료!\")\n",
    "print(\"\n📝 배치 크기(batch_size) 설명:\")\n",
    "print(\"- 너무 크면: 메모리 부족 발생 가능\")\n",
    "print(\"- 너무 작으면: 학습 속도가 느려짐\")\n",
    "print(\"- 적절한 크기: 32, 64, 128, 256 등의 2의 거듭제곱 값 사용\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f41edb",
   "metadata": {},
   "source": [
    "## 7. 모델 성능 평가\n",
    "\n",
    "`evaluate()` 함수를 사용하여 훈련셋과 테스트셋에서의 모델 성능을 평가하고, 과적합 여부를 진단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196ab956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 훈련셋 성능:\n",
      "   손실(Loss): 0.0000\n",
      "   정확도(Accuracy): 1.0000 (100.00%)\n",
      "\n",
      "📊 테스트셋 성능:\n",
      "   손실(Loss): 0.2185\n",
      "   정확도(Accuracy): 0.9836 (98.36%)\n",
      "\n",
      "📈 성능 분석:\n",
      "   정확도 차이: 0.0164 (1.64%p)\n",
      "   ✅ 적절한 일반화 성능을 보입니다.\n",
      "\n",
      "🎉 MNIST 손글씨 분류 모델 구현 완료!\n"
     ]
    }
   ],
   "source": [
    "# 훈련셋 성능 평가\n",
    "train_loss, train_acc = model.evaluate(train_images, train_labels, verbose=0)\n",
    "print(\"📊 훈련셋 성능:\")\n",
    "print(f\"   손실(Loss): {train_loss:.4f}\")\n",
    "print(f\"   정확도(Accuracy): {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "\n",
    "# 테스트셋 성능 평가\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(\"\n📊 테스트셋 성능:\")\n",
    "print(f\"   손실(Loss): {test_loss:.4f}\")\n",
    "print(f\"   정확도(Accuracy): {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "\n",
    "# 성능 차이 분석\n",
    "accuracy_diff = train_acc - test_acc\n",
    "print(f\"\n📈 성능 분석:\")\n",
    "print(f\"   정확도 차이: {accuracy_diff:.4f} ({accuracy_diff*100:.2f}%p)\")\n",
    "\n",
    "if accuracy_diff > 0.05:  # 5% 이상 차이\n",
    "    print(\"   ⚠️  과적합(Overfitting) 가능성이 있습니다.\")\n",
    "else:\n",
    "    print(\"   ✅ 적절한 일반화 성능을 보입니다.\")\n",
    "\n",
    "print(\"\n🎉 MNIST 손글씨 분류 모델 구현 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5e670",
   "metadata": {},
   "source": [
    "## 8. 학습 요약 및 추가 개선 방안\n",
    "\n",
    "### ✅ 구현한 내용:\n",
    "1. **MNIST 데이터셋 로드 및 탐색**: Keras에 내장된 데이터셋을 활용하여 대용량 이미지 데이터를 로드했습니다.\n",
    "2. **다층 퍼셉트론(MLP) 모델 설계**: `Sequential` API를 사용하여 여러 개의 `Dense` 층으로 구성된 신경망을 구축했습니다.\n",
    "3. **데이터 전처리**: 모델 학습에 적합하도록 이미지 데이터를 1차원 벡터로 변환하고, 0~1 사이의 값으로 정규화했습니다.\n",
    "4. **모델 학습 및 성능 평가**: `compile`, `fit`, `evaluate` 메서드를 사용하여 모델을 학습하고, 훈련 및 테스트 데이터에 대한 성능을 객관적으로 평가했습니다.\n",
    "\n",
    "### 🔧 모델 개선 방안 (심화):\n",
    "1. **CNN(Convolutional Neural Network) 사용**: 이미지의 공간적 특징(spatial features)을 효과적으로 학습하기 위해, MLP 대신 CNN을 사용하면 성능을 크게 향상시킬 수 있습니다.\n",
    "2. **Dropout 추가**: 은닉층 사이에 `layers.Dropout(rate)`을 추가하여 훈련 시 일부 뉴런을 무작위로 비활성화함으로써 과적합을 방지할 수 있습니다.\n",
    "3. **Batch Normalization**: 각 층의 입력 분포를 정규화하여 학습을 안정시키고 수렴 속도를 높일 수 있습니다.\n",
    "4. **Learning Rate Scheduling**: 학습률을 고정하지 않고, 학습 진행에 따라 동적으로 조정하여 최적점에 더 잘 수렴하도록 도울 수 있습니다.\n",
    "5. **Data Augmentation**: 훈련 이미지에 회전, 이동, 확대/축소 등 인위적인 변형을 가하여 데이터의 양과 다양성을 늘려 모델의 일반화 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "### 🎯 다음 단계:\n",
    "- **Scikit-learn의 Iris 데이터셋으로 딥러닝 구현**: 다른 종류의 데이터(수치형 데이터)에 딥러닝을 적용하는 방법을 학습합니다.\n",
    "- **CNN을 사용한 이미지 분류**: CIFAR-10과 같은 더 복잡한 이미지 데이터셋에 CNN 모델을 적용해봅니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}