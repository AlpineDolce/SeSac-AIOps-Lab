{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 벡터화 직접 구현하기\n",
    "\n",
    "이 노트북은 텍스트 데이터를 벡터로 변환하는 과정을 직접 구현하는 `MyVectorize` 클래스를 다룹니다. 텍스트 표준화, 토큰화, 어휘 사전 구축, 인코딩 및 디코딩 기능을 포함합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 라이브러리 임포트\n",
    "필요한 라이브러리를 임포트합니다. 여기서는 구두점 처리를 위해 `string` 모듈을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MyVectorize 클래스 정의\n",
    "텍스트 처리를 위한 모든 메서드를 포함하는 클래스입니다.\n",
    "- `standardize`: 텍스트를 소문자로 변환하고 구두점을 제거합니다.\n",
    "- `tokenize`: 텍스트를 공백 기준으로 단어 토큰으로 분리합니다.\n",
    "- `make_vocabulary`: 데이터셋으로부터 단어 사전을 구축합니다.\n",
    "- `encode`: 텍스트를 숫자 시퀀스로 변환합니다.\n",
    "- `decode`: 숫자 시퀀스를 다시 텍스트로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVectorize:\n",
    "    def standardize(self, text): #표준화\n",
    "        text = text.lower() #1.전부 소문자로 만든다 \n",
    "        return \"\".join(c for c in text  if c not in string.punctuation)\n",
    "        #구두점 제거한 문장을 만들어서 반환함 \n",
    "    \n",
    "    def tokenize(self, text): #토큰화 \n",
    "        return text.split()  #잘라서 보낸다. \n",
    "\n",
    "    #어휘사전 만드는 함수 \n",
    "    def make_vocabulary(self, dataset):\n",
    "        #1.전체 데이터셋을 순회하녀 단어 사전을 만든다. \n",
    "        #2.기본저긍로 빈문장\"\" 과 UNK- 자주 사용하는 단어만, 자주 사용하는 단어가 아니면 UNK로 표현한다.(unknown)\n",
    "        #3.새로운 단어가 발견되면 어휘사전에 추가하고 해당 단어에 고유한 숫자 인데스를 부여한다 \n",
    "        self.vocabulary = {\"\":0, \"[UNK]\":1}  #0하고 1은 특수목적으로 사용한다 \n",
    "        for text in dataset: #한문장씩 처리한다\n",
    "            text = self.standardize(text) #표준화 \n",
    "            tokens = self.tokenize(text)  #토큰화 \n",
    "            for token in tokens:\n",
    "                if token not in self.vocabulary: #아직 어휘사전에 없는 단어이면 추가한다 \n",
    "                    self.vocabulary[token] = len(self.vocabulary)\n",
    "\n",
    "        #역순서  단어:숫자 => 숫자:단어로 바꾼다\n",
    "        self.inverse_vocabulary = dict( (v,k) for k,v in self.vocabulary.items())  \n",
    "\n",
    "    def encode(self, text): #문장을 받아서 문장 벡터화를 한다 \n",
    "        text = self.standardize(text)\n",
    "        tokens = self.tokenize(text)\n",
    "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
    "\n",
    "    def decode(self, int_sequence):\n",
    "        return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 준비 및 클래스 인스턴스화\n",
    "실습에 사용할 간단한 데이터셋을 정의하고 `MyVectorize` 클래스의 인스턴스를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv = MyVectorize()\n",
    "dataset=[\n",
    "    \"I write, erase, reqrite\",\n",
    "    \"Erase again, and then\",\n",
    "    \"A poppy blooms\",\n",
    "    \"Dog is pretty\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 표준화 및 토큰화 테스트\n",
    "`standardize`와 `tokenize` 메서드가 잘 동작하는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = dataset[1]\n",
    "print(f\"Original: {test_sentence}\")\n",
    "\n",
    "standardized_test = mv.standardize(test_sentence)\n",
    "print(f\"Standardized: {standardized_test}\")\n",
    "\n",
    "tokenized_test = mv.tokenize(standardized_test)\n",
    "print(f\"Tokenized: {tokenized_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 어휘 사전 생성\n",
    "`make_vocabulary` 메서드를 호출하여 전체 데이터셋에 대한 어휘 사전을 생성합니다. 생성된 `vocabulary` (단어 -> 인덱스)와 `inverse_vocabulary` (인덱스 -> 단어)를 출력하여 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv.make_vocabulary(dataset)\n",
    "\n",
    "print(\"Vocabulary (word to index):\")\n",
    "print(mv.vocabulary)\n",
    "\n",
    "print(\"\\nInverse Vocabulary (index to word):\")\n",
    "print(mv.inverse_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 인코딩 및 디코딩 테스트\n",
    "임의의 문장을 `encode` 하여 숫자 시퀀스로 변환하고, 그 숫자 시퀀스를 다시 `decode` 하여 원본과 유사한 문장으로 복원되는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = mv.encode(\"I write erase\")\n",
    "print(f\"Encoded 'I write erase': {encoded_text}\")\n",
    "\n",
    "# [UNK]를 포함한 디코딩 예시 (어휘 사전에 없는 23번 인덱스)\n",
    "decoded_text = mv.decode([2, 3, 4, 23])\n",
    "print(f\"Decoded [2, 3, 4, 23]: {decoded_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}