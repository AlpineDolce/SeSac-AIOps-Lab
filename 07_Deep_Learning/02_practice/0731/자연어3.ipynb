{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB 영화 리뷰 감성 분석 (Multi-hot Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import subprocess\n",
    "import re\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import os, pathlib, shutil, random\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 다운로드 및 준비\n",
    "\n",
    "IMDB 영화 리뷰 데이터셋을 다운로드하고, 압축을 해제한 후, 훈련 데이터의 일부를 검증 데이터로 분리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 다운로드 함수\n",
    "def download():\n",
    "    url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "    file_name = \"aclImdb_v1.tar.gz\"\n",
    "    if not os.path.exists(file_name):\n",
    "        print(\"Downloading data...\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(file_name, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(\"Download complete!\")\n",
    "    else:\n",
    "        print(\"Data already downloaded.\")\n",
    "\n",
    "# 압축 해제 함수\n",
    "def release():\n",
    "    if not os.path.exists(\"aclImdb\"):\n",
    "        print(\"Extracting files...\")\n",
    "        subprocess.run([\"tar\", \"-xvzf\", \"aclImdb_v1.tar.gz\"], shell=True)\n",
    "        # unsup 폴더 제거 (라벨이 없으므로 사용 안함)\n",
    "        if os.path.exists(\"aclImdb/train/unsup\"):\n",
    "            shutil.rmtree(\"aclImdb/train/unsup\")\n",
    "        print(\"Extraction complete.\")\n",
    "    else:\n",
    "        print(\"Data already extracted.\")\n",
    "\n",
    "# 훈련/검증 데이터 분리 함수\n",
    "def labeling(): \n",
    "    base_dir = pathlib.Path(\"aclImdb\")\n",
    "    val_dir = base_dir / \"val\"\n",
    "    train_dir = base_dir / \"train\"\n",
    "    if not val_dir.exists():\n",
    "        print(\"Creating validation set...\")\n",
    "        for category in (\"neg\", \"pos\"):\n",
    "            os.makedirs(val_dir / category)\n",
    "            files = os.listdir(train_dir / category)\n",
    "            random.Random(1337).shuffle(files)\n",
    "            num_val_samples = int(0.2 * len(files))\n",
    "            val_files = files[-num_val_samples:]\n",
    "            for fname in val_files:\n",
    "                shutil.move(train_dir / category / fname, val_dir / category / fname)\n",
    "        print(\"Validation set created.\")\n",
    "    else:\n",
    "        print(\"Validation set already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최초 실행 시에만 주석 해제\n",
    "# download()\n",
    "# release()\n",
    "# labeling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 로드\n",
    "\n",
    "`text_dataset_from_directory`를 사용하여 디렉토리 구조를 기반으로 훈련, 검증, 테스트 데이터셋을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\"aclImdb/train\", batch_size=batch_size)\n",
    "val_ds = keras.utils.text_dataset_from_directory(\"aclImdb/val\", batch_size=batch_size)\n",
    "test_ds = keras.utils.text_dataset_from_directory(\"aclImdb/test\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 구조 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 벡터화 (Multi-hot Encoding)\n",
    "\n",
    "텍스트 데이터를 수치형 데이터로 변환합니다. 여기서는 **Multi-hot Encoding** 또는 **Bag-of-Words(단어 가방)** 방식을 사용합니다. 이 방식은 문장에서 단어의 순서는 무시하고, 어휘 사전에 있는 단어의 등장 유무만 벡터에 표시합니다.\n",
    "\n",
    "- `max_tokens=20000`: 어휘 사전에 포함할 단어의 최대 개수를 20,000개로 제한합니다. (빈도수 기준)\n",
    "- `output_mode='multi_hot'`: 각 텍스트를 20,000차원의 벡터로 변환합니다. 텍스트에 어휘 사전에 있는 단어가 포함되어 있으면 해당 인덱스의 값을 1로, 그렇지 않으면 0으로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어휘 사전 생성 및 데이터셋 변환\n",
    "\n",
    "훈련 데이터셋의 텍스트만을 사용하여 어휘 사전을 생성(`adapt`)하고, 이 사전을 기준으로 모든 데이터셋(훈련, 검증, 테스트)을 Multi-hot 벡터로 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터에서 텍스트만 추출하여 어휘 사전을 생성합니다.\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# 모든 데이터셋에 벡터화 레이어를 적용합니다.\n",
    "# num_parallel_calls를 사용하여 전처리 속도를 높입니다.\n",
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y), num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 벡터화된 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- 벡터화 후 ---\")\n",
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape) # (batch_size, max_tokens)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0][:20]:\", inputs[0][:20]) # 첫 번째 리뷰 벡터의 앞 20개 값\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 구축 및 컴파일\n",
    "\n",
    "간단한 완전 연결 신경망(Dense) 모델을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers, models\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 훈련\n",
    "\n",
    "- `ModelCheckpoint`: 검증 세트에서 가장 좋은 성능을 내는 모델을 파일로 저장합니다.\n",
    "- `.cache()`: 데이터셋을 메모리에 캐시하여 다음 에포크부터는 전처리 과정을 생략하고 훈련 속도를 높입니다. (메모리에 올릴 수 있을 만큼 데이터셋이 작을 때 유용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    binary_1gram_train_ds.cache(),\n",
    "    validation_data=binary_1gram_val_ds.cache(),\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 모델 평가\n",
    "\n",
    "훈련 과정에서 저장된 최적의 모델을 불러와 테스트 데이터셋으로 최종 성능을 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"테스트셋 정확도: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}